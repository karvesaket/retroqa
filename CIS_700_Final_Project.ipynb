{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CIS 700 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karvesaket/retroqa/blob/master/CIS_700_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ZewQTZVAIe",
        "colab_type": "text"
      },
      "source": [
        "# Question Answering - Microsoft NewsQA Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS1OS3VVVgPF",
        "colab_type": "text"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reXO23btWXe3",
        "colab_type": "text"
      },
      "source": [
        "## Connect Google Drive (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myB_3g1VWKla",
        "colab_type": "code",
        "outputId": "cf28881e-ecdd-4b73-ec8b-7c8bbde6535b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47iejPa-Wci2",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboad setup (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhX6U3ChU8Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSUUoMrYVmjM",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading and Processing  (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wip8AvHAVmqu",
        "colab_type": "code",
        "outputId": "7efbdaa9-5b47-4266-e4b5-52a6edff314c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUWDywcHSnUM",
        "colab_type": "text"
      },
      "source": [
        " (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15L4xXtacoZ",
        "colab_type": "code",
        "outputId": "1800ec8c-b19a-46ca-df44-f2b6c989ca63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def populateDatafields(somedf, col_dict):\n",
        "  datafields = []\n",
        "  for col in somedf.columns:\n",
        "    if col in col_dict.keys():\n",
        "      datafields.append((col, col_dict[col]))\n",
        "    # if(col == \"story_text\"):\n",
        "    #   datafields.append((col, col_dict[col]))\n",
        "    # elif col == \"question\":\n",
        "    #   datafields.append((col, TEXT))\n",
        "    else:\n",
        "      datafields.append((col, None))\n",
        "  return datafields\n",
        "\n",
        "newsqa_df = pd.read_csv('/content/drive/Shared drives/CIS 700-1 Final Project/Data/combined-newsqa-data-json-spacy-LATEST-train.csv')\n",
        "newsqa_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story_id</th>\n",
              "      <th>question</th>\n",
              "      <th>is_answer_absent</th>\n",
              "      <th>is_question_bad</th>\n",
              "      <th>validated_answers</th>\n",
              "      <th>story_text</th>\n",
              "      <th>char_start_index</th>\n",
              "      <th>char_end_index</th>\n",
              "      <th>char_text</th>\n",
              "      <th>word_start_index</th>\n",
              "      <th>word_end_index</th>\n",
              "      <th>word_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./cnn/stories/b94ba34137ea0eea08e20b7e1580ad59...</td>\n",
              "      <td>What does the VP say?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Former Vice President Dick Cheney said Sunday ...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./cnn/stories/e65cfdce945df6624f2516a9ad2f9292...</td>\n",
              "      <td>What show was Susan on?</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Scottish woman who became an Internet sing...</td>\n",
              "      <td>299</td>\n",
              "      <td>322</td>\n",
              "      <td>\"Britain's Got Talent.\"\\n</td>\n",
              "      <td>59</td>\n",
              "      <td>66</td>\n",
              "      <td>\" Britain 's Got Talent . \" \\n\\n\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./cnn/stories/8fbcaf3abc124b7baaa278d382411f43...</td>\n",
              "      <td>what about the cubans</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hurricane Gustav churned into the Gulf of Mexi...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./cnn/stories/ce137f38b5e2f793d25cee8b458a8add...</td>\n",
              "      <td>What does the book chroncile?</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"Sesame Street\" may not be a real place, but t...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./cnn/stories/1d8bc084e34d2f35349c773bfcde3422...</td>\n",
              "      <td>Is there a trust fund established for her?</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A California woman who turned up alive 18 year...</td>\n",
              "      <td>2747</td>\n",
              "      <td>2755</td>\n",
              "      <td>has been</td>\n",
              "      <td>571</td>\n",
              "      <td>572</td>\n",
              "      <td>has been</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82342</th>\n",
              "      <td>./cnn/stories/4776859089a777c567f574bc2145e8af...</td>\n",
              "      <td>What is a key stumbling block?</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Israeli government ministers Wednesday overwhe...</td>\n",
              "      <td>4742</td>\n",
              "      <td>4764</td>\n",
              "      <td>Continued construction</td>\n",
              "      <td>888</td>\n",
              "      <td>889</td>\n",
              "      <td>Continued construction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82343</th>\n",
              "      <td>./cnn/stories/94b37c8f266c3fe90e30a565ef0e4edf...</td>\n",
              "      <td>What were the protests about?</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>France is sending four state police units to i...</td>\n",
              "      <td>753</td>\n",
              "      <td>808</td>\n",
              "      <td>low wages and living conditions in the Caribbe...</td>\n",
              "      <td>135</td>\n",
              "      <td>143</td>\n",
              "      <td>low wages and living conditions in the Caribbe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82344</th>\n",
              "      <td>./cnn/stories/e16f0e93099b1b467bfbf8ec58426ca8...</td>\n",
              "      <td>What was the storm's name?</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"10:27\": 2}</td>\n",
              "      <td>Hurricane Paloma continued to intensify Friday...</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>Hurricane Paloma</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Hurricane Paloma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82345</th>\n",
              "      <td>./cnn/stories/7b40a01e9a18baf4506e6c06a13f3ca9...</td>\n",
              "      <td>Who wanted to bring girlfriend</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Walking into school Wednesday morning was not ...</td>\n",
              "      <td>55</td>\n",
              "      <td>74</td>\n",
              "      <td>Constance McMillen.</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>Constance McMillen .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82346</th>\n",
              "      <td>./cnn/stories/28e251ea722829df08d63e83824110fe...</td>\n",
              "      <td>Which fire grew to 25,000 acres?</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{\"294:307\": 2, \"247:256\": 1}</td>\n",
              "      <td>One of the larger fires in Southern California...</td>\n",
              "      <td>259</td>\n",
              "      <td>271</td>\n",
              "      <td>The Santiago</td>\n",
              "      <td>45</td>\n",
              "      <td>46</td>\n",
              "      <td>The Santiago</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>82347 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                story_id  ...                                          word_text\n",
              "0      ./cnn/stories/b94ba34137ea0eea08e20b7e1580ad59...  ...                                               None\n",
              "1      ./cnn/stories/e65cfdce945df6624f2516a9ad2f9292...  ...              \" Britain 's Got Talent . \" \\n\\n\\n\\n \n",
              "2      ./cnn/stories/8fbcaf3abc124b7baaa278d382411f43...  ...                                               None\n",
              "3      ./cnn/stories/ce137f38b5e2f793d25cee8b458a8add...  ...                                               None\n",
              "4      ./cnn/stories/1d8bc084e34d2f35349c773bfcde3422...  ...                                          has been \n",
              "...                                                  ...  ...                                                ...\n",
              "82342  ./cnn/stories/4776859089a777c567f574bc2145e8af...  ...                            Continued construction \n",
              "82343  ./cnn/stories/94b37c8f266c3fe90e30a565ef0e4edf...  ...  low wages and living conditions in the Caribbe...\n",
              "82344  ./cnn/stories/e16f0e93099b1b467bfbf8ec58426ca8...  ...                                  Hurricane Paloma \n",
              "82345  ./cnn/stories/7b40a01e9a18baf4506e6c06a13f3ca9...  ...                              Constance McMillen . \n",
              "82346  ./cnn/stories/28e251ea722829df08d63e83824110fe...  ...                                      The Santiago \n",
              "\n",
              "[82347 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8pe6TQrM4Op",
        "colab_type": "text"
      },
      "source": [
        "## Split data to train and test\n",
        "**Do not run this everytime**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5DCweheMkvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# create train and validation set \n",
        "train, val = train_test_split(newsqa_df, test_size=0.2)\n",
        "train.to_csv('/content/drive/Shared drives/CIS 700-1 Final Project/Data/combined-newsqa-data-json-spacy-answerable-train.csv', index=False)\n",
        "val.to_csv('/content/drive/Shared drives/CIS 700-1 Final Project/Data/combined-newsqa-data-json-spacy-answerable-val.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHGM9bXcM7AB",
        "colab_type": "text"
      },
      "source": [
        "## Load data and create iterators (R)\n",
        "\n",
        "NOTE - Change mode to 'bert' here when running for bert\n",
        "\n",
        "Also remember to change the train and val files to correct files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaPZ57L0Ltpi",
        "colab_type": "code",
        "outputId": "356b90a7-ea81-4abf-cca7-dee9af612325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "VERBOSE = True\n",
        "RUN_MODE = 'word' # ['word', 'sentence', 'bert']\n",
        "\n",
        "def split_start(x, y):\n",
        "  idx = x[0].split(\",\")[0]\n",
        "  if idx == 'None':\n",
        "    return int(-1)\n",
        "  else:\n",
        "    return int(idx)\n",
        "\n",
        "process_start = data.Pipeline(split_start)\n",
        "\n",
        "def split_end(x, y):\n",
        "  if len(x[0].split(\",\")) > 1:\n",
        "    idx = x[0].split(\",\")[len(x[0].split(\",\")) - 1]\n",
        "    if idx == 'None':\n",
        "      return int(-1)\n",
        "    else:\n",
        "      return int(idx)\n",
        "  else:\n",
        "    idx = x[0].split(\",\")[0]\n",
        "    if idx == 'None':\n",
        "      return int(-1)\n",
        "    else:\n",
        "      return int(idx)\n",
        "\n",
        "process_end = data.Pipeline(split_end)\n",
        "\n",
        "import math\n",
        "def floor_label(x, y):\n",
        "  return math.floor(float(x[0]))\n",
        "\n",
        "process_answerable = data.Pipeline(floor_label)\n",
        "\n",
        "if RUN_MODE == 'bert':\n",
        "  !pip install transformers\n",
        "  from transformers import BertTokenizer, AlbertTokenizer\n",
        "\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "  # tokenizer = AlbertTokenizer.from_pretrained('albert-large-v2', do_lower_case=True)\n",
        "\n",
        "  # print(\"Original: \", newsqa_df['story_text'][0])\n",
        "  print(\"Tokenized: \", tokenizer.tokenize(newsqa_df['story_text'][0]))\n",
        "  # print(len(tokenizer.vocab))\n",
        "\n",
        "\n",
        "  init_token_idx = tokenizer.cls_token_id\n",
        "  eos_token_idx = tokenizer.sep_token_id\n",
        "  pad_token_idx = tokenizer.pad_token_id\n",
        "  unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "  max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "  # max_input_length = tokenizer.max_model_input_sizes['albert-base-v2']\n",
        "\n",
        "  def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    # tokens = tokens[:max_input_length-2]\n",
        "    return tokens\n",
        "\n",
        "  BERT_FIELD = data.Field(batch_first = True,\n",
        "              use_vocab = False,\n",
        "              tokenize = tokenize_and_cut,\n",
        "              preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "              init_token = init_token_idx,\n",
        "              eos_token = eos_token_idx,\n",
        "              pad_token = pad_token_idx,\n",
        "              unk_token = unk_token_idx)\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "WORD_MODE_FIELD = data.Field(sequential=True, lower=True, tokenize='spacy', include_lengths=True, batch_first=True)\n",
        "WORD_FIELD = data.Field(sequential=True, lower=True, tokenize='spacy')\n",
        "SENTENCE_FIELD = data.NestedField(WORD_FIELD, tokenize=sent_tokenize, include_lengths=True)\n",
        "START_INDEX = data.Field(sequential=True, postprocessing=process_start, use_vocab=False)\n",
        "END_INDEX = data.Field(sequential=True, postprocessing=process_end, use_vocab=False)\n",
        "ANSWERABLE = data.Field(sequential=True, postprocessing=process_answerable, use_vocab=False)\n",
        "\n",
        "\n",
        "if RUN_MODE == 'word':\n",
        "  col_dict = {'story_text': WORD_MODE_FIELD, 'question': WORD_MODE_FIELD, 'word_start_index': START_INDEX, 'word_end_index': END_INDEX, 'is_answer_absent': ANSWERABLE}\n",
        "elif RUN_MODE == 'bert':\n",
        "  col_dict = {'story_text': BERT_FIELD, 'question': BERT_FIELD, 'word_start_index_1': START_INDEX, 'word_end_index_1': END_INDEX, 'is_answer_absent': ANSWERABLE}\n",
        "elif RUN_MODE == 'sentence':\n",
        "  col_dict = {'story_text': SENTENCE_FIELD, 'question': WORD_FIELD, 'word_start_index_1': START_INDEX, 'word_end_index_1': END_INDEX, 'is_answer_absent': ANSWERABLE}\n",
        "\n",
        "datafields = populateDatafields(newsqa_df, col_dict)\n",
        "if VERBOSE:\n",
        "  print(datafields)\n",
        "\n",
        "print(\"Building Dataset...\")\n",
        "training_data=data.TabularDataset(path = '/content/drive/Shared drives/CIS 700-1 Final Project/Data/combined-newsqa-data-json-spacy-LATEST-train.csv',\\\n",
        "                                  format = 'csv',\\\n",
        "                                  fields = datafields,\\\n",
        "                                  skip_header = True)\n",
        "\n",
        "validation_data=data.TabularDataset(path = '/content/drive/Shared drives/CIS 700-1 Final Project/Data/combined-newsqa-data-json-spacy-LATEST-val.csv',\\\n",
        "                                  format = 'csv',\\\n",
        "                                  fields = datafields,\\\n",
        "                                  skip_header = True)\n",
        "\n",
        "if VERBOSE:\n",
        "  count = 0\n",
        "  for t in training_data:\n",
        "    print(\"*******************************\")\n",
        "    print(\"Story Text: \", len(t.story_text), t.story_text)\n",
        "    print(\"Question: \", t.question)\n",
        "    print(\"Start Index: \", t.word_start_index)\n",
        "    print(\"End Index: \", t.word_end_index)\n",
        "    print(\"Unanswerable: \", t.is_answer_absent)\n",
        "    # print(\"Answer: \", t.story_text[int(t.word_start_index_1):int(t.word_end_index_1)])\n",
        "\n",
        "    if count > 5:\n",
        "      break\n",
        "    count += 1\n",
        "\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "print(\"Building Vocab...\")\n",
        "\n",
        "name = '6B'\n",
        "dim = 300\n",
        "if RUN_MODE == 'word':\n",
        "  WORD_MODE_FIELD.build_vocab(training_data, validation_data, min_freq = 3, vectors=GloVe(name = name, dim = dim))\n",
        "  if VERBOSE:\n",
        "    print(\"Length of Vocab: \", len(WORD_MODE_FIELD.vocab))\n",
        "elif RUN_MODE == 'sentence':\n",
        "  SENTENCE_FIELD.build_vocab(training_data, validation_data, min_freq = 3, vectors=GloVe(name = name, dim = dim))\n",
        "  if VERBOSE:\n",
        "    print(\"Length of Vocab: \", len(SENTENCE_FIELD.vocab))\n",
        "\n",
        "if RUN_MODE == 'word':\n",
        "  FIELD = WORD_MODE_FIELD\n",
        "elif RUN_MODE == 'sentence':\n",
        "  FIELD = SENTENCE_FIELD\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(\"Initializing the iterator...\")\n",
        "# Define the train iterator\n",
        "train_iterator = data.BucketIterator(\n",
        "    training_data, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.story_text),\n",
        "    sort_within_batch = True,\n",
        "    repeat=False, \n",
        "    shuffle=True,\n",
        "    device=device)\n",
        "\n",
        "val_iterator = data.BucketIterator(\n",
        "    validation_data, \n",
        "    batch_size = 1,\n",
        "    sort_key = lambda x: len(x.story_text),\n",
        "    sort_within_batch = False,\n",
        "    sort=False,\n",
        "    repeat=False,\n",
        "    shuffle=False,\n",
        "    device=device)\n",
        "\n",
        "if VERBOSE:\n",
        "  for batch in train_iterator:\n",
        "    print(\"Story: \", batch.story_text[0].shape, batch.story_text[1].shape)\n",
        "    print(\"Start/End: \", batch.word_start_index, batch.word_end_index, batch.is_answer_absent)\n",
        "    break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('story_id', None), ('question', <torchtext.data.field.Field object at 0x7fc42fa352e8>), ('is_answer_absent', <torchtext.data.field.Field object at 0x7fc3d6f6e4a8>), ('is_question_bad', None), ('validated_answers', None), ('story_text', <torchtext.data.field.Field object at 0x7fc42fa352e8>), ('char_start_index', None), ('char_end_index', None), ('char_text', None), ('word_start_index', <torchtext.data.field.Field object at 0x7fc3d6f6e710>), ('word_end_index', <torchtext.data.field.Field object at 0x7fc3d6f6e7b8>), ('word_text', None)]\n",
            "Building Dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r.vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*******************************\n",
            "Story Text:  386 ['former', 'vice', 'president', 'dick', 'cheney', 'said', 'sunday', 'he', 'supports', 'the', 'obama', 'administration', \"'s\", 'decision', 'to', 'repeal', 'the', '\"', 'do', \"n't\", 'ask', ',', 'do', \"n't\", 'tell', '\"', 'policy', 'banning', 'gays', 'and', 'lesbians', 'from', 'serving', 'openly', 'in', 'the', 'military', '--', 'a', 'move', 'that', 'was', 'staunchly', 'opposed', 'by', 'most', 'top', 'republicans', '.', '\\n\\n\\n\\n\\n\\n', '\"', 'i', 'think', 'the', 'decision', 'that', \"'s\", 'been', 'made', 'with', 'respect', 'to', 'allowing', 'gays', 'to', 'serve', 'openly', 'in', 'the', 'military', 'is', 'a', 'good', 'one', '\"', 'cheney', 'told', 'cnn', \"'s\", 'candy', 'crowley', 'on', '\"', 'state', 'of', 'the', 'union', '.', '\"', '\"', 'it', \"'s\", 'the', 'right', 'thing', 'to', 'do', '.', '\"', '\\n\\n\\n\\n\\n\\n', 'the', 'policy', ',', 'first', 'enacted', 'during', 'the', 'clinton', 'administration', ',', 'was', 'officially', 'repealed', 'on', 'september', '20', '.', 'over', '14,000', 'people', 'were', 'kicked', 'out', 'of', 'the', 'military', 'due', 'to', '\"', 'do', \"n't\", 'ask', ',', 'do', \"n't\", 'tell', '.', '\"', '\\n\\n\\n\\n\\n\\n', 'the', 'controversial', 'repeal', 'of', 'the', 'policy', 'became', 'a', 'focal', 'point', 'of', 'a', 'gop', 'presidential', 'debate', 'last', 'month', 'when', 'members', 'of', 'the', 'audience', 'booed', 'a', 'gay', 'soldier', 'who', 'asked', 'about', 'the', 'decision', '.', 'president', 'barack', 'obama', 'criticized', 'the', 'republican', 'presidential', 'contenders', 'on', 'saturday', 'night', 'for', 'failing', 'to', 'rebuke', 'the', 'audience', '.', '\\n\\n\\n\\n\\n\\n', '\"', 'we', 'do', \"n't\", 'believe', 'in', 'standing', 'silent', 'when', 'that', 'happens', ',', '\"', 'obama', 'told', 'attendees', 'at', 'the', 'annual', 'national', 'dinner', 'of', 'the', 'human', 'rights', 'campaign', ',', 'an', 'organization', 'that', 'promotes', 'equality', 'for', 'gays', 'and', 'lesbians', '.', '\\n\\n\\n\\n\\n\\n', 'if', '\"', 'you', 'want', 'to', 'be', 'commander', '-', 'in', '-', 'chief', ',', 'you', 'can', 'start', 'by', 'standing', 'up', 'for', 'the', 'men', 'and', 'women', 'who', 'wear', 'the', 'uniform', 'of', 'the', 'united', 'states', 'even', 'when', 'it', 'is', 'not', 'politically', 'convenient', ',', '\"', 'obama', 'said', '.', '\\n\\n\\n\\n\\n\\n', 'cheney', 'responded', 'to', 'obama', 'by', 'noting', 'that', 'he', \"'s\", '\"', 'a', 'little', 'bit', 'leery', 'of', 'the', 'notion', 'that', 'somehow', 'we', 'ought', 'to', 'go', 'hammer', 'the', 'republican', 'candidates', 'because', 'they', 'did', \"n't\", 'respond', 'to', 'booing', 'in', 'the', 'audience', '.', '\"', '\\n\\n\\n\\n\\n\\n', '\"', 'when', 'you', \"'re\", 'in', 'a', 'political', 'campaign', 'and', 'debates', ',', 'people', 'boo', 'a', 'lot', 'of', 'things', ',', '\"', 'cheney', 'told', 'cnn', '.', '\"', 'i', \"'m\", 'not', 'sure', 'that', 'it', 'was', 'all', 'focused', 'specifically', 'on', 'that', 'particular', 'issue', '.', '\"', '\\n\\n\\n\\n\\n\\n', 'after', 'the', 'debate', ',', 'former', 'pennsylvania', 'sen.', 'rick', 'santorum', 'said', 'he', 'had', \"n't\", 'heard', 'the', 'booing', 'when', 'the', 'question', 'was', 'asked', '.', 'santorum', 'was', 'being', 'asked', 'the', 'question', 'when', 'the', 'incident', 'occurred', '.']\n",
            "Question:  ['what', 'does', 'the', 'vp', 'say', '?']\n",
            "Start Index:  ['None']\n",
            "End Index:  ['None']\n",
            "Unanswerable:  ['1.0']\n",
            "*******************************\n",
            "Story Text:  957 ['the', 'scottish', 'woman', 'who', 'became', 'an', 'internet', 'singing', 'sensation', 'after', 'her', 'performance', 'on', 'a', 'british', 'talent', 'show', 'said', 'friday', 'she', 'does', \"n't\", 'want', 'fame', 'to', 'change', 'her', '.', '\\n\\n\\n\\n', 'susan', 'boyle', 'at', 'home', 'with', 'her', 'piano', '.', '\\n\\n\\n\\n', 'susan', 'boyle', ',', '47', ',', 'has', 'said', 'she', \"'s\", 'still', 'the', 'same', 'humble', 'girl', 'next', 'door', 'despite', 'her', 'knockout', 'singing', 'on', '\"', 'britain', \"'s\", 'got', 'talent', '.', '\"', '\\n\\n\\n\\n', 'she', 'shocked', 'and', 'inspired', 'the', 'audience', ',', 'judges', ',', 'and', 'web', 'watchers', 'with', 'her', 'rendition', 'of', '\"', 'i', 'dreamed', 'a', 'dream', '\"', 'from', 'the', 'musical', '\"', 'les', 'misérables', '\"', 'in', 'the', 'first', 'round', 'of', 'the', 'show', '.', '\\n\\n\\n\\n', '\"', 'i', 'would', \"n't\", 'want', 'to', 'change', 'myself', 'too', 'much', 'because', 'that', 'would', 'really', 'make', 'things', 'a', 'bit', 'false', ',', '\"', 'she', 'told', 'cnn', \"'s\", '\"', 'american', 'morning', '\"', 'on', 'friday', '.', '\"', 'i', 'want', 'to', 'receive', 'people', 'as', 'the', 'real', 'me', ',', 'a', 'real', 'person', '.', '\"', '\\n\\n\\n\\n', 'boyle', \"'s\", 'appearance', 'belied', 'her', 'talent', ',', 'but', 'in', 'the', 'end', 'it', 'was', 'the', 'very', 'reason', 'she', 'won', 'over', 'the', 'audience', '.', 'there', 'is', 'speculation', 'tv', 'producers', 'might', 'give', 'boyle', 'a', 'makeover', 'for', 'the', 'rest', 'of', 'the', 'show', ',', 'but', 'she', 'refused', 'to', 'say', 'whether', 'she', \"'d\", 'accept', 'one', '.', 'ireport.com', ':', 'have', 'you', 'been', 'judged', 'on', 'looks', '?', '\\n\\n\\n\\n', '\"', 'i', 'ca', \"n't\", 'make', 'a', 'comment', 'on', 'that', ',', '\"', 'she', 'said', '.', '\\n\\n\\n\\n', 'watch', 'boyle', 'sing', 'a', 'new', 'song', 'for', 'cnn', \"'s\", 'larry', 'king', '»', '\\n\\n\\n\\n', 'a', 'clip', 'of', 'boyle', \"'s\", 'performance', 'on', 'the', 'talent', 'show', 'had', 'more', 'than', '15', 'million', 'views', 'on', 'youtube', 'by', 'friday', ',', 'and', 'the', 'world', \"'s\", 'media', 'have', 'beaten', 'a', 'path', 'to', 'her', 'door', 'in', 'blackburn', ',', 'scotland', '.', ' ', 'watch', 'boyle', \"'s\", 'singing', 'wow', 'the', 'world', '»', '\\n\\n\\n\\n', 'while', 'she', 'said', 'she', \"'s\", 'the', 'same', 'person', '--', 'the', 'shy', 'girl', 'who', 'has', 'never', 'been', 'kissed', '--', 'it', \"'s\", 'clear', 'that', 'boyle', \"'s\", 'life', 'already', 'is', 'changing', '.', '\\n\\n\\n\\n', 'in', 'her', 'home', 'are', 'heaps', 'of', 'fan', 'mail', 'and', 'cards', 'from', 'well', '-', 'wishers', '.', 'throngs', 'of', 'fans', 'have', 'been', 'shrieking', 'at', 'her', 'doorstep', 'begging', 'for', 'her', 'autograph', '.', '\\n\\n\\n\\n', 'boyle', 'said', 'she', \"'s\", 'still', 'in', 'shock', 'and', 'overwhelmed', 'by', 'her', 'overnight', 'stardom', '.', ' ', 'watch', 'how', 'boyle', 'sees', 'herself', '»', '\\n\\n\\n\\n', '\"', 'i', \"'m\", 'gobsmacked', ',', 'absolutely', 'gobsmacked', ',', '\"', 'she', 'told', 'cnn', 'on', 'friday', 'morning', '.', '\\n\\n\\n\\n', 'during', 'an', 'earlier', 'interview', 'with', 'cnn', \"'s\", 'atika', 'shubert', ',', 'boyle', 'expressed', 'amazement', 'at', 'people', \"'s\", 'reaction', 'to', 'her', '--', '\"', 'the', 'way', 'everyone', 'seems', 'to', 'have', 'embraced', 'me', ',', 'the', 'way', 'they', 'seem', 'to', 'have', 'apparently', 'fallen', 'in', 'love', 'with', 'me', ',', '\"', 'she', 'said', '.', ' ', 'watch', 'young', 'fans', 'flock', 'to', 'boyle', \"'s\", 'home', '»', '\\n\\n\\n\\n', 'the', 'singer', 'acknowledged', 'she', 'noticed', 'the', 'initial', 'sniggers', 'when', 'she', 'got', 'onstage', 'for', 'her', 'performance', 'on', '\"', 'britain', \"'s\", 'got', 'talent', '.', '\"', 'but', 'she', 'said', 'she', 'did', \"n't\", 'let', 'it', 'get', 'to', 'her', '.', 'if', 'nothing', 'else', ',', 'it', 'fueled', 'her', 'motivation', '.', '\\n\\n\\n\\n', '\"', 'i', 'just', 'thought', 'mentally', ',', 'i', \"'ll\", 'show', 'them', ',', 'so', 'i', 'did', ',', '\"', 'she', 'said', '.', '\"', 'if', 'people', 'are', 'cynical', ',', 'you', 'try', 'and', 'win', 'them', \"'\", 'round', 'and', 'it', 'worked', '.', 'it', 'must', 'have', 'been', 'a', 'miracle', ',', 'but', 'it', 'worked', '.', '\"', '\\n\\n\\n\\n', 'boyle', 'still', 'has', 'a', 'long', 'way', 'to', 'go', '--', 'having', 'just', 'gotten', 'through', 'to', 'the', 'second', 'round', 'of', '\"', 'britain', \"'s\", 'got', 'talent', '\"', 'after', 'judge', 'simon', 'cowell', 'described', 'her', 'first', 'performance', 'as', '\"', 'extraordinary', '.', '\"', '\\n\\n\\n\\n', 'if', 'she', 'can', 'make', 'her', 'way', 'through', 'the', 'show', \"'s\", 'final', 'rounds', ',', 'she', 'will', 'get', 'to', 'sing', 'for', 'queen', 'elizabeth', 'ii', 'at', 'the', 'royal', 'variety', 'show', '.', 'boyle', 'has', 'promised', 'to', 'be', 'on', 'her', 'best', 'behavior', 'if', 'she', 'gets', 'that', 'far', '.', '\\n\\n\\n\\n', '\"', 'whatever', 'comes', 'my', 'way', ',', 'i', 'am', 'ready', '.', 'it', 'would', 'be', 'lovely', 'to', 'sing', 'for', 'the', 'queen', '.', 'there', 'would', 'be', 'less', 'of', 'the', 'carry', '-', 'on', 'from', 'me', 'and', 'more', 'of', 'the', 'singing', '.', '\\n\\n\\n\\n', '\"', 'she', 'is', 'a', 'very', 'regal', 'lady', ',', 'very', 'nice', ',', 'so', 'i', 'would', 'be', 'nice', ',', 'too', ',', 'and', 'just', 'get', 'up', 'there', 'and', 'give', 'it', 'a', 'bit', 'of', 'wellie', '[', 'try', ']', ',', '\"', 'boyle', 'told', 'the', 'show', \"'s\", 'web', 'site', '.', '\\n\\n\\n\\n', 'boyle', 'began', 'singing', 'in', 'school', 'productions', 'at', '12', '.', 'she', 'had', 'private', 'lessons', 'and', 'won', 'local', 'competitions', ',', 'but', 'a', 'professional', 'career', 'never', 'took', 'hold', 'partly', 'because', 'of', 'circumstances', 'at', 'home', '.', '\\n\\n\\n\\n', 'boyle', 'said', 'she', 'cared', 'for', 'her', 'aging', 'parents', ',', 'both', 'of', 'whom', 'have', 'died', '.', '\\n\\n\\n\\n', 'thoughts', 'of', 'her', 'mother', 'led', 'boyle', 'to', 'apply', 'to', 'be', 'on', 'the', 'talent', 'show', ',', 'she', 'said', '.', '\\n\\n\\n\\n', '\"', 'she', 'was', 'my', 'inspiration', ',', 'and', 'she', 'was', 'the', 'driving', 'force', 'behind', 'my', 'application', ',', '\"', 'boyle', 'told', 'cnn', '.', '\"', 'i', 'felt', 'it', 'was', 'a', 'tribute', 'to', 'her', '.', 'she', 'was', 'a', 'wonderful', 'lady', '.', '\"', ' ', 'watch', 'boyle', 'say', ',', '\"', 'it', 'must', 'have', 'been', 'a', 'miracle', '\"', '»', '\\n\\n\\n\\n', 'boyle', 'said', 'she', 'was', 'trying', 'to', 'take', 'her', 'newfound', 'fame', 'in', 'stride', '.', '\\n\\n\\n\\n', '\"', 'it', \"'s\", 'a', 'challenge', '.', 'life', 'is', 'a', 'challenge', 'sometimes', ',', 'but', 'this', 'is', 'different', '.', 'and', 'i', 'like', 'to', 'test', 'myself', '.', '\\n\\n\\n\\n', '\"', 'if', 'it', 'all', 'gets', 'too', 'much', 'and', 'they', 'lock', 'me', 'up', ',', 'i', 'want', 'a', 'great', 'big', 'straitjacket', 'with', 'spots', 'on', 'it', '--', 'a', 'pink', 'one', '...', 'and', 'a', 'big', 'zip', 'on', 'the', 'back', 'so', 'i', 'can', 'escape', '.', '\"', '\\n\\n\\n\\n', 'cowell', 'reportedly', 'is', 'trying', 'to', 'piece', 'together', 'a', 'record', 'deal', 'for', 'boyle', ',', 'an', 'unemployed', 'charity', 'worker', 'who', 'lives', 'with', 'her', 'cat', ',', 'pebbles', '.', 'but', 'boyle', 'said', 'she', \"'s\", 'not', 'focusing', 'on', 'any', 'of', 'that', 'yet', '--', 'she', 'still', 'spends', 'her', 'time', 'concentrating', 'on', 'the', 'current', 'competition', '.']\n",
            "Question:  ['what', 'show', 'was', 'susan', 'on', '?']\n",
            "Start Index:  ['59']\n",
            "End Index:  ['66']\n",
            "Unanswerable:  ['0.0']\n",
            "*******************************\n",
            "Story Text:  1029 ['hurricane', 'gustav', 'churned', 'into', 'the', 'gulf', 'of', 'mexico', 'on', 'saturday', 'night', ',', 'still', 'an', '\"', 'extremely', 'dangerous', '\"', 'category', '4', 'storm', 'threatening', 'to', 'blast', 'the', 'same', 'region', 'devastated', 'by', 'hurricane', 'katrina', 'three', 'years', 'ago', '.', '\\n\\n\\n\\n', 'saturday', 'morning', ',', 'gustav', 'was', 'about', '255', 'miles', 'east', '-', 'southeast', 'of', 'the', 'western', 'tip', 'of', 'cuba', '.', '\\n\\n\\n\\n', 'gustav', ',', 'a', 'category', '4', 'storm', 'with', '135', 'mph', '(', '220', 'km', '/', 'hr', ')', 'top', 'winds', ',', 'was', 'centered', '485', 'miles', '(', '780', 'kilometers', ')', 'southeast', 'of', 'the', 'mouth', 'of', 'the', 'mississippi', 'river', ',', 'according', 'to', 'the', 'national', 'hurricane', 'center', \"'s\", ' ', '6', 'a.m.', 'gmt', '(', '2', 'a.m.', 'et', ')', 'advisory', '.', '\\n\\n\\n\\n', 'that', \"'s\", 'only', '15', 'mph', 'slower', 'than', 'when', 'the', 'storm', 'first', 'crossed', 'land', 'in', 'cuba', '.', 'the', 'u.s.', 'national', 'hurricane', 'center', 'said', 'the', 'storm', 'could', 'pick', 'up', 'even', 'more', 'strength', 'as', 'it', 'enters', 'the', 'gulf', \"'s\", 'warmer', 'waters', '.', '\\n\\n\\n\\n', 'the', 'hurricane', 'center', 'said', 'gustav', 'is', 'an', '\"', 'extremely', 'dangerous', 'category', '4', 'hurricane', '.', '\"', '\\n\\n\\n\\n', 'in', 'the', 'united', 'states', ',', 'state', 'and', 'federal', 'officials', '--', 'eager', 'to', 'prove', 'they', 'are', 'ready', '--', 'urged', 'residents', 'to', 'flee', ',', 'and', 'many', 'of', 'those', 'residents', 'obeyed', ',', 'moving', 'north', 'by', 'the', 'tens', 'of', 'thousands', ',', 'according', 'to', 'the', 'governor', 'of', 'louisiana', ',', 'whose', 'state', 'was', 'hit', 'three', 'years', 'ago', 'by', 'hurricane', 'katrina', '.', '\\n\\n\\n\\n', 'new', 'orleans', 'mayor', 'ray', 'nagin', 'called', 'gustav', '\"', 'the', 'mother', 'of', 'all', 'storms', ',', '\"', 'saying', 'its', 'destruction', 'could', 'outstrip', 'that', 'from', 'katrina', ',', 'which', 'flooded', 'much', 'of', 'his', 'city', 'and', 'killed', 'about', '3,000', 'people', '.', '\\n\\n\\n\\n', 'a', 'hurricane', 'watch', 'remained', 'in', 'effect', 'for', 'the', 'northern', 'gulf', 'coast', 'from', 'east', 'of', 'high', 'island', 'texas', 'eastward', 'to', 'the', 'alabama', '-', 'florida', 'border', ',', 'including', 'the', 'city', 'of', 'new', 'orleans', 'and', 'lake', 'pontchartrain', ',', 'the', 'national', 'hurricane', 'center', 'said', '.', 'a', 'hurricane', 'watch', 'means', 'that', 'hurricane', 'conditions', 'are', 'possible', 'within', 'the', 'watch', 'area', ',', 'generally', 'within', '36', 'hours', '.', '\\n\\n\\n\\n', 'on', 'its', 'current', 'track', ',', 'gustav', 'would', 'reach', 'the', 'northern', 'gulf', 'on', 'monday', '.', '\\n\\n\\n\\n', 'thousands', 'of', 'anxious', 'cubans', 'had', 'boarded', 'up', 'their', 'homes', 'and', 'sought', 'safety', 'from', 'gustav', 'earlier', 'as', 'it', 'slammed', 'into', 'the', 'island', 'nation', '.', 'as', 'it', 'made', 'landfall', 'in', 'western', 'cuba', 'on', 'saturday', 'night', 'with', 'sustained', 'winds', 'near', '150', 'mph', '(', '240', 'kmh', ')', ',', 'the', 'national', 'hurricane', 'center', 'said', '.', '\\n\\n\\n\\n', 'some', 'fluctuations', ',', 'with', 'an', 'overall', 'slight', 'strengthening', ',', 'is', 'forecast', 'for', 'the', 'next', '24', 'hours', ',', 'and', 'forecasters', 'said', 'gustav', 'could', 'become', 'a', 'category', '5', 'hurricane', 'within', 'that', 'period', '.', ' ', 'see', 'gustav', \"'s\", 'projected', 'path', '»', '\\n\\n\\n\\n', 'hurricanes', 'are', 'ranked', '1', 'to', '5', 'in', 'intensity', 'on', 'the', 'saffir', '-', 'simpson', 'scale', '.', 'a', 'category', '5', 'is', 'the', 'highest', 'classification', ',', 'with', 'sustained', 'winds', 'of', 'more', 'than', '155', 'mph', '.', 'a', 'category', '4', 'has', 'winds', 'of', '131', 'to', '155', 'mph', 'and', 'can', 'cause', 'extreme', 'damage', '.', '\\n\\n\\n\\n', 'anticipating', 'the', 'storm', ',', 'about', '10,000', 'cubans', 'evacuated', 'saturday', 'to', 'cuba', \"'s\", 'mainland', 'from', 'la', 'isla', 'de', 'juventud', ',', 'or', 'isle', 'of', 'youth', ',', 'a', 'province', 'south', 'of', 'the', 'nation', \"'s\", 'western', 'end', '.', 'the', 'hurricane', 'center', 'said', 'western', 'cuba', 'could', 'receive', 'as', 'much', 'as', '25', 'inches', 'of', 'rain', '.', '\\n\\n\\n\\n', 'residents', 'sought', 'refuge', 'with', 'family', 'or', 'friends', ',', 'or', 'went', 'to', 'a', 'government', 'shelter', ',', 'cnn', \"'s\", 'morgan', 'neill', 'said', '.', '\\n\\n\\n\\n', 'cubans', 'also', 'headed', 'to', 'higher', 'ground', 'on', 'the', 'mainland', ',', 'with', 'many', 'utilizing', 'horse', '-', 'drawn', 'carriages', 'and', 'vintage', 'cars', '.', '\\n\\n\\n\\n', 'president', 'raúl', 'castro', 'checked', 'with', 'the', 'island', \"'s\", 'officials', 'saturday', 'to', 'be', 'sure', 'preparations', 'were', 'on', 'track', ',', 'neill', 'said', '.', '\\n\\n\\n\\n', 'cuba', \"'s\", 'western', 'province', 'of', 'pinar', 'del', 'rio', ',', 'currently', 'under', 'a', 'hurricane', 'warning', ',', 'is', 'the', 'center', 'of', 'the', 'nation', \"'s\", 'lucrative', 'cigar', 'industry', '.', '\\n\\n\\n\\n', 'a', 'hurricane', 'warning', 'remained', 'in', 'effect', 'for', 'the', 'cuban', 'provinces', 'of', 'pinar', 'del', 'rio', ',', 'la', 'habana', ',', 'ciudad', 'de', 'la', 'habana', ',', 'isla', 'de', 'juventud', ',', 'matanzas', 'and', 'cienfuegos', '.', 'a', 'warning', 'indicates', 'that', 'hurricane', 'conditions', 'will', 'arrive', 'in', 'the', 'next', '24', 'hours', '.', '\\n\\n\\n\\n', 'gustav', 'caused', 'little', 'damage', 'on', 'grand', 'cayman', 'on', 'friday', 'night', ',', 'according', 'to', 'an', 'official', 'at', 'the', 'island', \"'s\", 'airport', '.', '\\n\\n\\n\\n', 'as', 'a', 'tropical', 'storm', ',', 'gustav', 'blasted', 'jamaica', 'with', 'high', 'winds', 'and', 'drenched', 'it', 'with', 'heavy', 'rain', 'thursday', 'night', 'into', 'friday', '.', 'four', 'people', 'were', 'killed', ',', 'the', 'national', 'emergency', 'operations', 'center', 'in', 'jamaica', 'said', ',', 'and', 'the', 'storm', 'downed', 'trees', 'and', 'damaged', 'houses', '.', '\\n\\n\\n\\n', '\"', 'it', 'is', 'total', 'devastation', 'everywhere', ',', '\"', 'the', 'councilor', 'for', 'the', 'manchioneal', 'division', ',', 'alston', 'hunter', ',', 'told', 'the', 'jamaican', 'gleaner', 'newspaper', '.', '\"', 'several', 'residents', 'are', 'now', 'crammed', 'into', 'disaster', 'shelters', 'here', 'in', 'east', 'portland', ',', 'and', 'the', 'weather', 'continues', 'to', 'make', 'the', 'situation', 'worse', '.', '\"', '\\n\\n\\n\\n', 'at', 'least', '51', 'people', 'were', 'killed', 'in', 'southwestern', 'haiti', 'and', 'eight', 'were', 'killed', 'in', 'neighboring', 'dominican', 'republic', 'as', 'gustav', 'roared', 'through', 'as', 'a', 'category', '1', 'hurricane', 'on', 'wednesday', ',', 'officials', 'there', 'said', '.', '\\n\\n\\n\\n', 'the', 'hurricane', 'center', \"'s\", 'five', '-', 'day', 'forecast', 'places', 'a', 'landfall', 'anywhere', 'from', 'galveston', ',', 'texas', ',', 'east', 'to', 'mobile', ',', 'alabama', '.', '\\n\\n\\n\\n', 'new', 'orleans', ',', 'louisiana', ',', 'is', 'at', 'the', 'center', 'of', 'the', 'projected', 'path', '.', '\\n\\n\\n\\n', 'a', 'hurricane', 'watch', 'remained', 'in', 'effect', 'for', 'the', 'northern', 'gulf', 'coast', 'from', 'east', 'of', 'high', 'island', ',', 'texas', ',', 'to', 'the', 'alabama', '-', 'florida', 'border', ',', 'including', 'the', 'city', 'of', 'new', 'orleans', 'and', 'lake', 'pontchartrain', ',', 'the', 'national', 'hurricane', 'center', 'said', '.', 'a', 'watch', 'means', 'hurricane', 'conditions', 'are', 'possible', 'within', 'the', 'specified', 'area', ',', 'generally', 'within', '36', 'hours', '.', '\\n\\n\\n\\n', 'the', 'gulf', 'coast', 'is', 'still', 'recovering', 'from', 'hurricane', 'katrina', ',', 'which', 'killed', 'more', 'than', '1,800', 'people', 'when', 'it', 'slammed', 'ashore', 'august', '29', ',', '2005', '.', '\\n\\n\\n\\n', 'mississippi', 'gov.', 'haley', 'barbour', 'said', 'hurricane', 'katrina', 'victims', 'living', 'in', 'government', '-', 'issued', 'trailers', 'or', 'mobile', 'homes', 'along', 'his', 'state', \"'s\", 'coast', 'will', 'begin', 'evacuating', 'this', 'weekend', '.', '\\n\\n\\n\\n', 'gustav', 'is', 'the', 'second', 'major', 'hurricane', 'of', 'the', '2008', 'atlantic', 'hurricane', 'season', '.', '\\n\\n\\n\\n', 'meanwhile', ',', 'tropical', 'storm', 'hanna', 'passed', 'north', 'of', 'the', 'leeward', 'islands', 'on', 'friday', 'with', 'maximum', 'sustained', 'winds', 'of', '50', 'mph', '(', '80', 'kph', ')', ',', 'the', 'hurricane', 'center', 'said', '.', '\\n\\n\\n\\n', 'the', 'center', 'predicted', 'gradual', 'strengthening', ',', 'and', 'hanna', 'could', 'be', 'near', 'hurricane', 'strength', 'sunday', '.']\n",
            "Question:  ['what', 'about', 'the', 'cubans']\n",
            "Start Index:  ['None']\n",
            "End Index:  ['None']\n",
            "Unanswerable:  ['0.333333333333']\n",
            "*******************************\n",
            "Story Text:  1110 ['\"', 'sesame', 'street', '\"', 'may', 'not', 'be', 'a', 'real', 'place', ',', 'but', 'tell', 'that', 'to', 'some', 'of', 'the', 'people', 'michael', 'davis', 'met', 'when', 'researching', 'and', 'talking', 'about', 'his', 'new', 'book', ',', '\"', 'street', 'gang', '.', '\"', '\\n\\n\\n\\n', 'bert', ',', 'left', ',', 'and', 'ernie', 'have', 'been', 'mainstays', 'of', '\"', 'sesame', 'street', '\"', 'since', 'the', 'beginning', '.', '\\n\\n\\n\\n', '\"', 'i', 'met', 'a', 'lot', 'of', 'people', 'who', 'i', 'worked', 'with', 'in', 'new', 'york', 'or', 'got', 'to', 'know', 'in', 'new', 'york', '--', 'transplants', '--', 'who', 'said', 'to', 'me', ',', \"'\", 'when', 'i', 'first', 'arrived', 'here', 'in', 'new', 'york', ',', 'i', 'had', 'this', 'strange', 'desire', 'to', 'find', 'sesame', 'street', ',', \"'\", '\"', 'he', 'said', '.', '\\n\\n\\n\\n', 'well', ',', 'to', 'paraphrase', 'the', 'famous', 'theme', 'song', ',', 'who', 'would', \"n't\", 'want', 'to', 'get', 'to', '\"', 'sesame', 'street', '\"', '?', '\\n\\n\\n\\n', 'for', 'two', 'generations', ',', 'the', 'fictional', 'block', 'of', 'brownstones', 'inhabited', 'by', 'curious', 'children', ',', 'friendly', 'adults', 'and', 'some', 'odd', '-', 'looking', 'muppets', 'has', 'helped', 'shape', 'childhood', 'education', 'by', 'offering', 'exercises', ',', 'games', 'and', 'life', 'lessons', 'all', 'wrapped', 'up', 'in', 'a', 'television', '-', 'friendly', 'format', '.', 'it', \"'s\", 'a', 'model', 'that', \"'s\", 'proved', 'durable', 'and', 'influential', ',', 'says', 'syracuse', 'university', 'pop', 'culture', 'professor', 'robert', 'thompson', '.', '\\n\\n\\n\\n', '\"', 'if', 'i', 'were', 'to', 'make', 'a', 'list', 'of', 'the', 'top', '10', 'most', 'significant', 'american', 'tv', 'shows', '...', 'i', \"'d\", 'put', \"'\", 'sesame', 'street', \"'\", 'on', 'the', 'list', '.', 'the', 'fact', 'that', 'it', \"'s\", 'still', 'on', 'the', 'air', 'attests', 'to', 'its', '[', 'significance', ']', ',', '\"', 'he', 'said', '.', ' ', 'see', '\"', 'sesame', 'street', '\"', 'in', 'pictures', '»', '\\n\\n\\n\\n', '\"', 'the', 'idea', 'they', 'came', 'up', 'with', 'was', 'kind', 'of', 'radical', ':', 'if', 'you', 'can', 'sell', 'kids', 'sugared', 'cereal', 'and', 'toys', 'using', 'madison', 'avenue', 'techniques', ',', 'why', 'could', \"n't\", 'you', 'use', 'the', 'same', 'techniques', 'for', 'teaching', 'counting', ',', 'the', 'alphabet', 'and', 'basic', 'social', 'skills', '?', 'and', 'it', 'works', '.', '\"', '\\n\\n\\n\\n', 'indeed', ',', 'as', 'davis', 'notes', 'in', '\"', 'street', 'gang', '\"', '(', 'viking', ')', ',', 'the', 'genesis', 'of', '\"', 'sesame', 'street', '\"', 'was', 'when', 'the', '3-year', '-', 'old', 'daughter', 'of', 'a', 'carnegie', 'foundation', 'executive', 'was', 'fascinated', 'by', 'television', ',', 'waking', 'up', 'to', 'watch', 'the', 'broadcast', 'day', 'begin', 'and', 'memorizing', 'commercial', 'jingles', '.', 'he', 'talked', 'about', 'his', 'daughter', 'with', 'a', 'friend', ',', 'producer', 'joan', 'ganz', 'cooney', '.', 'in', 'the', 'liberal', 'ferment', 'of', 'the', \"mid-'60s\", ',', 'both', 'wondered', 'whether', 'educational', 'tv', 'could', 'go', 'beyond', 'the', 'staid', 'classroom', 'shows', 'of', 'the', 'era', '.', '\\n\\n\\n\\n', 'cooney', 'became', 'the', 'driving', 'force', 'of', '\"', 'sesame', 'street', '.', '\"', 'she', 'put', 'together', 'the', 'plan', ',', 'helped', 'recruit', 'talent', ',', 'located', 'financing', 'and', 'oversaw', 'production', '.', '\"', 'sesame', 'street', '\"', 'became', 'the', 'foundation', 'for', 'the', 'children', \"'s\", 'television', 'workshop', '(', 'now', 'sesame', 'workshop', ')', ',', 'which', 'created', 'other', 'educational', 'shows', 'such', 'as', '\"', 'the', 'electric', 'company', '\"', 'and', '\"', '3', '-', '2', '-', '1', 'contact', '.', '\"', '\\n\\n\\n\\n', '\"', 'she', 'is', 'just', 'such', 'an', 'impressive', 'woman', ',', '\"', 'said', 'davis', ',', 'adding', 'that', 'cooney', 'gave', 'her', 'blessing', 'to', 'his', 'book', 'project', 'without', 'any', 'requirements', 'but', 'one', ':', 'that', 'he', '\"', 'get', 'it', 'right', '.', '\"', '\"', 'she', \"'s\", 'just', 'one', 'of', 'those', 'extraordinary', 'public', 'figures', '.', '\"', '\\n\\n\\n\\n', 'cooney', 'did', \"n't\", 'hold', 'much', 'back', 'in', 'telling', 'her', 'story', 'to', 'davis', ',', 'and', 'neither', 'did', 'others', '.', 'from', 'its', 'debut', 'on', 'november', '10', ',', '1969', ',', 'the', 'show', 'was', 'a', 'hit', '--', 'within', 'a', 'year', ',', 'it', 'was', 'on', 'the', 'cover', 'of', 'time', 'magazine', '--', 'but', 'it', 'was', 'not', 'without', 'its', 'personality', 'clashes', '.', '\\n\\n\\n\\n', 'the', 'original', 'gordon', ',', 'matt', 'robinson', ',', 'was', 'a', 'producer', 'uncomfortable', 'in', 'the', 'spotlight', '.', 'northern', 'calloway', ',', 'who', 'played', 'david', ',', 'struggled', 'with', 'mental', 'illness', '.', 'the', 'show', \"'s\", 'primary', 'songwriters', ',', 'joe', 'raposo', 'and', 'jeff', 'moss', ',', 'were', 'constantly', 'in', 'competition', ';', 'raposo', '\"', 'fairly', 'seethed', 'with', 'envy', '\"', 'when', 'moss', \"'\", '\"', 'rubber', 'duckie', '\"', 'hit', 'the', 'top', '20', ',', 'davis', 'writes', '.', 'the', 'book', 'provides', 'balanced', 'biographies', 'of', 'a', 'number', 'of', 'principals', ',', 'including', 'producer', 'jon', 'stone', ',', 'whom', 'davis', 'calls', '\"', 'the', 'heart', 'of', 'the', 'book', '.', '\"', '\\n\\n\\n\\n', '\"', 'i', 'wanted', 'people', 'to', 'say', ',', \"'\", 'wow', ',', 'this', 'guy', 'jon', 'stone', ',', 'he', 'really', 'was', 'the', 'orson', 'welles', 'of', '\"', 'sesame', 'street', '.', '\"', \"'\", 'without', 'him', ',', 'the', 'show', 'would', \"n't\", 'have', 'been', 'what', 'it', 'became', ',', '\"', 'davis', 'said', '.', '\\n\\n\\n\\n', 'but', 'for', 'all', 'the', 'backstage', 'machinations', 'that', 'affect', 'any', 'creative', 'enterprise', ',', '\"', 'sesame', 'street', '\"', 'stayed', 'true', 'to', 'education', ',', 'in', 'all', 'its', 'forms', '.', 'one', 'show', 'matter', '-', 'of', '-', 'factly', 'included', 'a', 'breast', '-', 'feeding', 'buffy', 'sainte', '-', 'marie', ';', 'others', 'featured', 'a', 'boy', 'with', 'down', 'syndrome', ',', 'jason', 'kingsley', '.', 'ew', ':', 'stars', 'who', 'dropped', 'by', '\"', 'sesame', 'street', '\"', '\\n\\n\\n\\n', 'jim', 'henson', ',', 'who', 'was', 'famous', 'as', 'creator', 'of', 'the', 'muppets', 'when', '\"', 'sesame', 'street', '\"', 'began', ',', 'invented', 'a', 'world', 'of', '(', 'literally', ')', 'colorful', 'characters', '--', 'oscar', ',', 'big', 'bird', ',', 'the', 'cookie', 'monster', ',', 'bert', 'and', 'ernie', '--', 'and', ',', 'with', 'his', 'puppeteering', 'crew', ',', 'gave', 'them', 'soul', '.', '\\n\\n\\n\\n', 'and', 'when', 'mr.', 'hooper', '(', 'will', 'lee', ')', 'died', ',', 'the', 'show', 'dealt', 'with', 'his', 'passing', 'honestly', '.', '\\n\\n\\n\\n', 'over', 'the', 'years', ',', 'the', 'show', 'has', 'taken', 'its', 'knocks', '.', 'critics', 'from', 'the', 'left', 'have', 'complained', 'about', 'its', 'merchandising', ';', 'critics', 'from', 'the', 'right', 'disliked', 'its', 'avowed', 'commitment', 'to', 'diversity', '.', 'in', 'the', \"'\", '90s', ',', '\"', 'barney', '\"', 'stole', 'its', 'thunder', ',', 'and', 'cable', 'drained', 'its', 'audience', '.', 'as', '\"', 'sesame', 'street', '\"', 'comes', 'up', 'on', 'its', '40th', 'birthday', ',', 'some', 'critics', 'wonder', 'whether', 'it', \"'s\", 'still', 'necessary', '.', '\\n\\n\\n\\n', 'but', 'for', 'all', 'that', ',', 'says', 'thompson', ',', 'the', 'show', 'remains', 'important', ',', 'both', 'in', 'its', 'pioneering', 'educational', 'style', 'and', 'in', 'its', 'clever', 'business', 'model', '.', 'and', 'it', 'takes', 'its', 'charges', 'seriously', ',', 'he', 'points', 'out', '.', '\\n\\n\\n\\n', '\"', 'one', 'thing', 'i', 'still', 'like', 'about', \"'\", 'sesame', 'street', \"'\", 'is', 'that', 'it', \"'s\", 'not', 'artsy', ',', '\"', 'he', 'said', '.', '\\n\\n\\n\\n', 'for', 'davis', \"'\", 'part', ',', 'doing', 'the', 'book', '--', 'which', 'succeeded', 'a', 'tv', 'guide', 'article', 'he', 'did', 'on', 'the', 'show', \"'s\", '35th', 'anniversary', '--', 'gave', 'him', 'renewed', 'respect', 'for', 'its', 'creators', \"'\", 'achievements', '.', 'and', 'he', \"'s\", 'found', 'out', 'through', 'his', 'web', 'site', ',', 'http://www.streetgangbook.com/', ',', 'that', '\"', 'sesame', 'street', '\"', 'still', 'has', 'the', 'magic', 'to', 'move', 'children', '--', 'mothers', 'of', 'autistic', 'children', 'credit', 'the', 'show', 'with', 'helping', 'the', 'kids', \"'\", 'development', '--', 'and', 'adults', '.', '\\n\\n\\n\\n', '\"', 'somebody', 'said', ',', \"'\", 'i', 'was', 'ok', 'when', 'my', 'mom', 'explained', 'to', 'me', 'there', 'was', 'no', 'santa', 'claus', ',', \"'\", '\"', 'he', 'recounts', '.', '\"', \"'\", 'but', 'i', 'cried', 'my', 'eyes', 'out', 'the', 'day', 'i', 'realized', 'kermit', 'was', 'a', 'puppet', '.', \"'\", 'is', \"n't\", 'that', 'great', '?', '\"']\n",
            "Question:  ['what', 'does', 'the', 'book', 'chroncile', '?']\n",
            "Start Index:  ['None']\n",
            "End Index:  ['None']\n",
            "Unanswerable:  ['0.5']\n",
            "*******************************\n",
            "Story Text:  685 ['a', 'california', 'woman', 'who', 'turned', 'up', 'alive', '18', 'years', 'after', 'being', 'kidnapped', 'at', 'age', '11', 'is', 'reconnecting', 'with', 'her', 'family', 'after', 'nearly', 'two', 'decades', 'apart', ',', 'her', 'aunt', 'said', 'thursday', '.', '\\n\\n\\n\\n', 'tina', 'dugard', 'speaks', 'to', 'the', 'media', 'thursday', 'about', 'how', 'her', 'niece', 'jaycee', 'is', 'reconnecting', 'with', 'her', 'family', '.', '\\n\\n\\n\\n', 'police', 'said', 'thursday', 'that', 'the', 'man', 'charged', 'with', 'abducting', 'and', 'raping', 'jaycee', 'lee', 'dugard', 'had', 'been', 'accused', 'of', 'raping', 'a', '14-year', '-', 'old', 'in', '1972', ',', 'but', 'those', 'charges', 'were', 'dropped', 'for', 'unknown', 'reasons', '.', '\\n\\n\\n\\n', '\"', 'i', 'think', 'there', \"'s\", 'a', 'good', 'chance', 'of', 'that', ',', 'yes', ',', '\"', 'antioch', 'police', 'lt', '.', 'leonard', 'orman', 'said', 'when', 'asked', 'whether', 'he', 'believed', 'that', 'other', 'victims', 'would', 'be', 'found', '.', '\\n\\n\\n\\n', 'dugard', 'is', 'spending', 'time', 'in', '\"', 'a', 'secluded', 'place', ',', 'reconnecting', '\"', 'with', 'her', 'mother', 'and', 'younger', 'sister', ',', 'said', 'jaycee', \"'s\", 'aunt', ',', 'tina', 'dugard', ',', 'who', 'spent', 'time', 'with', 'them', '.', '\\n\\n\\n\\n', 'the', 'two', 'children', 'born', 'to', 'her', 'during', 'her', 'captivity', 'are', '\"', 'clever', ',', 'articulate', ',', 'curious', 'girls', ',', '\"', 'she', 'said', '.', '\\n\\n\\n\\n', '\"', 'this', 'is', 'a', 'joyful', 'time', 'for', 'my', 'family', ',', '\"', 'she', 'said', '.', '\"', 'jaycee', 'remembers', 'all', 'of', 'us', '.', '\"', '\\n\\n\\n\\n', 'jaycee', 'lee', 'dugard', 'was', 'kidnapped', 'in', '1991', 'from', 'a', 'bus', 'stop', 'near', 'her', 'home', 'in', 'south', 'lake', 'tahoe', ',', 'california', ',', 'and', 'discovered', 'last', 'week', '.', 'authorities', 'say', 'a', 'couple', 'kidnapped', 'her', 'and', 'raised', 'her', 'in', 'a', 'compound', 'of', 'tents', 'and', 'outbuildings', 'in', 'the', 'backyard', 'of', 'their', 'antioch', ',', 'california', ',', 'home', 'for', '18', 'years', '.', '\\n\\n\\n\\n', 'nancy', 'and', 'phillip', 'garrido', 'have', 'been', 'charged', 'with', 'a', 'total', 'of', '29', 'felonies', ',', 'including', 'the', 'rape', 'and', 'kidnapping', 'of', 'dugard', ',', 'who', 'police', 'say', 'gave', 'birth', 'to', 'two', 'daughters', 'fathered', 'by', 'garrido', 'during', 'her', 'captivity', '.', 'the', 'garridos', 'have', 'pleaded', 'not', 'guilty', '.', 'philip', 'garrido', 'is', 'a', 'registered', 'sex', 'offender', '.', '\\n\\n\\n\\n', 'tina', 'dugard', 'appeared', 'in', 'los', 'angeles', 'on', 'thursday', 'to', 'read', 'a', 'statement', 'on', 'behalf', 'of', 'her', 'family', '.', ' ', 'watch', 'jaycee', \"'s\", 'aunt', 'speak', 'to', 'the', 'media', '»', '\\n\\n\\n\\n', '\"', 'jaycee', 'is', 'a', 'remarkable', 'young', 'woman', 'who', 'has', 'raised', 'two', 'beautiful', 'daughters', ',', '\"', 'she', 'said', '.', '\"', 'they', 'are', 'clever', ',', 'articulate', ',', 'curious', 'girls', 'who', 'have', 'a', 'bright', 'future', 'ahead', 'of', 'them', '.', '\"', '\\n\\n\\n\\n', 'the', 'girls', 'are', '11', 'and', '15', '.', '\\n\\n\\n\\n', '\"', 'although', 'they', 'have', 'no', 'formal', 'education', ',', 'they', 'are', 'certainly', 'educated', ',', '\"', 'she', 'said', '.', '\"', 'jaycee', 'did', 'a', 'truly', 'amazing', 'job', 'with', 'the', 'limited', 'resources', 'and', 'education', 'that', 'she', 'herself', 'had', ',', 'and', 'we', 'are', 'so', 'proud', 'of', 'her', '.', '\"', '\\n\\n\\n\\n', 'tina', 'dugard', 'said', 'jaycee', \"'s\", 'mother', \"'s\", 'smile', 'is', '\"', 'as', 'wide', 'as', 'the', 'sea', '.', '\"', '\\n\\n\\n\\n', '\"', 'her', 'oldest', 'daughter', 'is', 'finally', 'home', ',', '\"', 'she', 'said', '.', '\\n\\n\\n\\n', 'dugard', ',', 'now', '29', ',', 'is', 'enjoying', 'catching', 'up', 'on', 'the', 'years', 'missed', 'with', 'her', 'family', ',', 'tina', 'dugard', 'said', '.', '\\n\\n\\n\\n', '\"', 'she', 'is', 'especially', 'enjoying', 'getting', 'to', 'know', 'her', 'little', 'sister', ',', 'who', 'was', 'just', 'a', 'baby', 'when', 'jaycee', 'was', 'taken', ',', '\"', 'she', 'said', '.', '\"', 'not', 'only', 'have', 'we', 'laughed', 'and', 'cried', 'together', ',', 'but', 'we', \"'ve\", 'spent', 'time', 'sitting', 'quietly', ',', 'taking', 'pleasure', 'in', 'each', 'other', \"'s\", 'company', '.', '\"', '\\n\\n\\n\\n', 'the', 'dugard', 'family', 'statement', 'thanked', 'the', 'law', 'enforcement', 'and', 'social', 'agencies', 'involved', 'in', 'reconnecting', 'them', '.', '\"', 'their', 'support', 'and', 'professionalism', 'have', 'been', 'invaluable', ',', '\"', 'it', 'said', '.', '\\n\\n\\n\\n', 'a', 'trust', 'fund', 'has', 'been', 'established', 'for', 'donations', 'to', 'help', 'dugard', ',', 'the', 'aunt', 'said', '.', '\\n\\n\\n\\n', '\"', 'it', 'has', 'come', 'to', 'my', 'family', \"'s\", 'attention', 'that', 'there', 'may', 'be', 'unauthorized', 'solicitation', 'of', 'funds', 'to', 'support', 'jaycee', 'and', 'the', 'family', ',', '\"', 'she', 'said', '.', '\\n\\n\\n\\n', 'the', 'family', 'released', 'three', 'photos', 'of', 'a', 'young', 'dugard', '.', 'one', 'was', 'taken', 'at', 'her', 'grandmother', \"'s\", 'home', 'when', 'she', 'was', '3', '.', 'a', 'second', 'showed', 'her', 'dressed', 'as', 'a', 'punk', 'rocker', 'the', 'halloween', 'before', 'her', 'abduction', '.', '\\n\\n\\n\\n', 'tina', 'dugard', 'said', 'she', 'snapped', 'the', 'third', 'photo', 'at', 'the', '1991', 'rose', 'bowl', 'parade', 'when', 'she', 'asked', 'her', 'niece', 'to', '\"', 'make', 'a', 'face', 'for', 'me', ',', 'and', 'she', 'did', '.', '\"']\n",
            "Question:  ['is', 'there', 'a', 'trust', 'fund', 'established', 'for', 'her', '?']\n",
            "Start Index:  ['571']\n",
            "End Index:  ['572']\n",
            "Unanswerable:  ['0.0']\n",
            "*******************************\n",
            "Story Text:  179 ['a', 'gunman', 'holed', 'up', 'across', 'the', 'street', 'from', 'a', 'french', 'nursery', 'school', 'opened', 'fire', 'friday', 'on', 'mothers', 'and', 'nannies', 'entering', 'the', 'building', 'to', 'pick', 'up', 'children', 'for', 'lunch', ',', 'slightly', 'injuring', 'eight', 'adults', ',', 'police', 'said', '.', '\\n\\n\\n\\n', 'french', 'police', 'at', 'the', 'lyon', 'nursery', 'school', 'friday', '.', '\\n\\n\\n\\n', 'two', 'of', 'the', 'injured', 'were', 'taken', 'to', 'a', 'hospital', '.', 'no', 'children', 'were', 'harmed', ',', 'said', 'officials', 'from', 'the', 'ecole', 'maternelle', ',', 'located', 'in', 'lyon', 'in', 'southeastern', 'france', '.', '\\n\\n\\n\\n', 'police', 'said', 'the', 'shooter', ',', 'who', 'remained', 'at', 'large', ',', 'used', 'an', 'air', 'rifle', '.', '\\n\\n\\n\\n', 'the', 'school', 'was', 'closed', 'and', 'a', 'security', 'cordon', 'set', 'up', 'around', 'it', 'and', 'other', 'schools', 'in', 'the', 'district', ',', 'police', 'said', '.', '\\n\\n\\n\\n', 'in', 'an', 'interview', 'with', 'the', 'french', 'radio', 'network', 'rtl', ',', 'lyon', 'mayor', 'thierre', 'philip', 'expressed', 'doubt', 'that', 'the', 'school', 'was', 'the', 'gunman', \"'s\", 'real', 'target', '.', '\\n\\n\\n\\n', '\"', 'it', 'was', 'pedestrians', ',', 'especially', 'mothers', 'or', 'nannies', 'who', 'came', 'to', 'pick', 'up', 'the', 'children', ',', 'who', 'were', 'hurt', ',', '\"', 'he', 'said', '.', '\\n\\n\\n\\n', 'cnn', \"'s\", 'sujatha', 'samy', 'contributed', 'to', 'this', 'report', '.']\n",
            "Question:  ['what', 'type', 'of', 'gun', 'was', 'used', '?']\n",
            "Start Index:  ['90']\n",
            "End Index:  ['93']\n",
            "Unanswerable:  ['0.0']\n",
            "*******************************\n",
            "Story Text:  356 ['egyptians', 'and', 'activists', 'around', 'the', 'world', 'took', 'to', 'the', 'streets', 'in', 'various', 'cities', 'saturday', 'to', 'show', 'their', 'support', 'for', 'the', 'protests', 'currently', 'underway', 'in', 'egypt', 'and', 'to', 'join', 'the', 'call', 'for', 'president', 'hosni', 'mubarak', 'to', 'step', 'down', '.', '\\n\\n\\n\\n\\n\\n', 'outside', 'the', 'egyptian', 'embassy', 'in', 'london', ',', 'people', 'rallied', ',', 'chanted', ',', 'hoisted', 'banners', 'and', 'demanded', 'change', '.', '\\n\\n\\n\\n\\n\\n', 'one', 'sign', ',', 'written', 'in', 'red', 'ink', ',', 'read', ':', '\"', 'from', 'the', 'nile', ',', 'to', 'the', 'sea', '--', 'egypt', 'soon', 'will', 'be', 'free', '!', 'freedom', 'for', 'egypt', '!', '\"', '\\n\\n\\n\\n\\n\\n', '\"', 'people', 'were', 'clearly', 'voicing', 'their', 'anger', 'at', 'president', 'mubarak', \"'s\", 'regime', 'but', 'similarly', 'they', 'were', 'also', 'very', 'passionate', 'about', 'their', 'demands', 'for', 'democracy', 'and', 'political', 'reforms', ',', '\"', 'said', 'amedeo', \"d'amore\", ',', 'who', 'attended', 'the', 'london', 'protest', '.', '\\n\\n\\n\\n\\n\\n', 'demonstrators', 'chanted', ':', '\"', 'one', ',', 'two', ',', 'three', ',', 'four', ',', 'we', 'do', \"n't\", 'want', 'mubarak', 'anymore', '!', '\"', \"d'amore\", 'said', 'in', 'an', 'e', '-', 'mail', 'to', 'cnn', '.', '\\n\\n\\n\\n\\n\\n', 'elsewhere', 'in', 'europe', ',', 'protesters', 'in', 'geneva', 'shouted', 'slogans', 'in', 'arabic', ',', 'french', 'and', 'english', ',', 'chanting', ',', '\"', 'get', 'out', ',', 'mubarak', '!', '\"', 'according', 'to', 'courtney', 'radsch', ',', 'who', 'attended', 'the', 'demonstration', 'there', '.', '\\n\\n\\n\\n\\n\\n', 'radsch', 'sent', 'cnn', 'a', 'video', 'of', 'the', 'protest', 'that', 'showed', 'a', 'crowd', 'of', 'people', 'carrying', 'signs', 'that', 'read', ',', '\"', 'freedom', '.', 'social', 'justice', '.', 'democracy', '\"', 'and', '\"', 'free', 'egypt', '.', '\"', '\\n\\n\\n\\n\\n\\n', 'protests', 'also', 'took', 'place', 'in', 'cities', 'throughout', 'canada', '.', '\\n\\n\\n\\n\\n\\n', 'about', '150', 'protesters', 'gathered', 'outside', 'the', 'egyptian', 'consulate', 'in', 'montreal', ',', 'cnn', 'affiliate', 'ctv', 'reported', '.', 'in', 'toronto', ',', 'a', 'downtown', 'rally', 'drew', 'more', 'than', '500', 'people', ',', 'it', 'said', '.', '\\n\\n\\n\\n\\n\\n', 'ahmed', 'khalifa', ',', 'who', 'helped', 'organize', 'the', 'toronto', 'protest', ',', 'said', 'he', 'was', 'encouraged', 'by', 'the', 'protests', 'in', 'egypt', 'and', 'stressed', 'their', 'significance', '.', '\\n\\n\\n\\n\\n\\n', '\"', 'we', 'are', 'witnessing', 'a', 'great', 'change', 'in', 'history', ',', '\"', 'he', 'told', 'ctv', '.', '\"', 'it', \"'s\", 'like', 'the', 'fall', 'of', 'the', 'berlin', 'wall', '.', 'we', 'are', 'watching', 'egyptian', 'people', 'and', 'middle', 'eastern', 'people', 'telling', 'us', 'that', \"'\", 'we', 'want', 'freedom', '.', \"'\", '\"', '\\n\\n\\n\\n\\n\\n', 'anti', '-', 'mubarak', 'demonstrations', 'were', 'also', 'held', 'across', 'the', 'united', 'states', '.']\n",
            "Question:  ['protests', 'are', 'held', 'in', 'which', 'three', 'places', '?']\n",
            "Start Index:  ['None']\n",
            "End Index:  ['None']\n",
            "Unanswerable:  ['0.0']\n",
            "Building Vocab...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                          \n",
            "100%|█████████▉| 399779/400000 [00:50<00:00, 8116.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of Vocab:  88494\n",
            "Initializing the iterator...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399779/400000 [01:10<00:00, 8116.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Story:  torch.Size([32, 733]) torch.Size([32])\n",
            "Start/End:  tensor([ 69, 505,  96,  65, 135,  97,  19, 565,  -1,  21, 325,   1,  81,  53,\n",
            "        123,  93,  -1,  54,   6,  -1,  -1,  25,  42, 614,  10,   0,   0,  -1,\n",
            "         -1, 278,  49,   0], device='cuda:0') tensor([ 70, 515,  97,  73, 137,  97,  19, 569,  -1,  24, 327,   1,  90,  53,\n",
            "        123, 119,  -1,  54,  11,  -1,  -1,  32,  59, 620,  11,   0,   1,  -1,\n",
            "         -1, 278,  55,   1], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JAcIppRS9TT",
        "colab_type": "text"
      },
      "source": [
        "### Testing \n",
        "\n",
        "Do not run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LIcn-G6Z3Bg",
        "colab_type": "code",
        "outputId": "b7e0ddf7-55e2-4ee2-ca81-5313319ee6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Number of batches: \", len(train_iterator))\n",
        "for batch in train_iterator:\n",
        "  seq_length = batch.story_text[0].shape[1]\n",
        "  start = batch.word_start_index\n",
        "  end = batch.word_end_index\n",
        "  smask = torch.sum((start >= seq_length).long())\n",
        "  if smask != 0:\n",
        "    print(\"Found issue in start!\")\n",
        "    print(seq_length, start)\n",
        "\n",
        "  emask = torch.sum((end >= seq_length).long())\n",
        "  if emask != 0:\n",
        "    print(\"Found issue in end!\")\n",
        "    print(seq_length, end)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches:  644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JgtO44bVPg7",
        "colab_type": "text"
      },
      "source": [
        "## BERT testing\n",
        "\n",
        "Do not run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgPTMW2fQm4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# print(\"Original: \", newsqa_df['story_text'][0])\n",
        "print(\"Tokenized: \", tokenizer.tokenize(newsqa_df['story_text'][0]))\n",
        "print(len(tokenizer.vocab))\n",
        "\n",
        "\n",
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "def tokenize_and_cut(sentence):\n",
        "  tokens = tokenizer.tokenize(sentence) \n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens\n",
        "\n",
        "BERT_FIELD = data.Field(batch_first = True,\n",
        "              use_vocab = False,\n",
        "              tokenize = tokenize_and_cut,\n",
        "              preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "              init_token = init_token_idx,\n",
        "              eos_token = eos_token_idx,\n",
        "              pad_token = pad_token_idx,\n",
        "              unk_token = unk_token_idx)\n",
        "START_INDEX = data.Field(sequential=True, postprocessing=process_start, use_vocab=False)\n",
        "END_INDEX = data.Field(sequential=True, postprocessing=process_end, use_vocab=False)\n",
        "ANSWERABLE = data.Field(sequential=True, postprocessing=process_answerable, use_vocab=False)\n",
        "\n",
        "col_dict = {'story_text': BERT_FIELD, 'question': BERT_FIELD, 'word_start_index_1': START_INDEX, 'word_end_index_1': END_INDEX, 'is_answer_absent': ANSWERABLE}\n",
        "\n",
        "datafields = populateDatafields(newsqa_df, col_dict)\n",
        "if VERBOSE:\n",
        "  print(datafields)\n",
        "\n",
        "print(\"Building Dataset...\")\n",
        "training_data=data.TabularDataset(path = '/content/drive/Shared drives/CIS 700-1 Final Project/Data/mini-combined-newsqa-data-v2.csv',\\\n",
        "                                  format = 'csv',\\\n",
        "                                  fields = datafields,\\\n",
        "                                  skip_header = True)\n",
        "if VERBOSE:\n",
        "  count = 0\n",
        "  for t in training_data:\n",
        "    print(\"*******************************\")\n",
        "    print(\"Story Text: \", len(t.story_text), t.story_text)\n",
        "    print(\"Question: \", t.question)\n",
        "    print(\"Start Index: \", t.word_start_index_1)\n",
        "    print(\"End Index: \", t.word_end_index_1)\n",
        "    print(\"Unanswerable: \", t.is_answer_absent)\n",
        "    # print(\"Answer: \", t.story_text[int(t.word_start_index_1):int(t.word_end_index_1)])\n",
        "\n",
        "    if count > 15:\n",
        "      break\n",
        "    count += 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "BATCH_SIZE = 2\n",
        "\n",
        "print(\"Initializing the iterator...\")\n",
        "# Define the train iterator\n",
        "train_iterator = data.BucketIterator(\n",
        "    training_data, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.story_text),\n",
        "    sort_within_batch = True,\n",
        "    repeat=False, \n",
        "    shuffle=True,\n",
        "    device = device)\n",
        "\n",
        "if VERBOSE:\n",
        "  for batch in train_iterator:\n",
        "    print(\"Story: \", batch.story_text[0].shape, batch.story_text[1].shape)\n",
        "    print(\"Start/End: \", batch.word_start_index_1, batch.word_end_index_1, batch.is_answer_absent)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PJF0DBigeLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert = bert.to(device)\n",
        "print(bert.config.to_dict()['hidden_size'])\n",
        "\n",
        "def bert_encoder(document, question):\n",
        "  with torch.no_grad():\n",
        "    splits = torch.split(document, max_input_length-2, dim=1)\n",
        "    embedded_document_splits = []\n",
        "    for split in splits:\n",
        "      embedded_document_splits.append(bert(split)[0])\n",
        "    embedded_document = torch.cat(embedded_document_splits, dim=1)\n",
        "    embedded_question = bert(question)\n",
        "  print(embedded_document.shape, embedded_question[0].shape)\n",
        "\n",
        "for batch in train_iterator:\n",
        "  document = batch.story_text #[B, S, W]\n",
        "  # doc_lengths = batch.story_text[1]\n",
        "  question = batch.question #[B, W]\n",
        "\n",
        "  bert_encoder(document, question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIrMbGfsVrPM",
        "colab_type": "text"
      },
      "source": [
        "# Model Definition (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCg_1rPe1l3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akn6j1W23cX6",
        "colab_type": "text"
      },
      "source": [
        "## Input Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUqTQqCs61oP",
        "colab_type": "text"
      },
      "source": [
        "### Sketchy Reading Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJcLgf6HVxO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SketchyReading(nn.Module):\n",
        "  def __init__(self, mode, vocab_size, embedding_length, word_embeddings=None, bert_encoder=None):\n",
        "    super(SketchyReading, self).__init__()\n",
        "\n",
        "    if mode not in ['avg', 'recurrent', 'word', 'bert']:\n",
        "      raise ValueError(\"Choose a mode from - avg / recurrent / word / bert\")\n",
        "    self.mode = mode\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_length = embedding_length\n",
        "    \n",
        "    # Embedding Layer\n",
        "    if self.mode == 'bert':\n",
        "      if bert_encoder is None:\n",
        "        raise ValueError(\"bert_encoder cannot be None when mode = bert\")\n",
        "      self.embeddings = bert_encoder\n",
        "    else:\n",
        "      if word_embeddings is None:\n",
        "        raise ValueError(\"word_embeddings cannot be None when mode is not bert\")\n",
        "      self.embeddings = nn.Embedding(self.vocab_size, self.embedding_length)\n",
        "      self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
        "\n",
        "    self.recurrent = nn.GRU(self.embedding_length, self.embedding_length, batch_first=True)\n",
        "    \n",
        "\n",
        "  def forward(self, document, document_lengths, question):\n",
        "    if self.mode == 'bert':\n",
        "      splits = torch.split(document, max_input_length-2, dim=1)\n",
        "      embedded_document_splits = []\n",
        "      for split in splits:\n",
        "        embedded_document_splits.append(self.embeddings(split)[0])\n",
        "      D = torch.cat(embedded_document_splits, dim=1)\n",
        "      # D = self.embeddings(document)[0]\n",
        "      Q = self.embeddings(question)[0]\n",
        "    elif self.mode == 'word':\n",
        "      batch_size = document.shape[0]\n",
        "      document = document.view(batch_size, -1)\n",
        "      D = self.embeddings(document)\n",
        "      Q = self.embeddings(question)\n",
        "    elif self.mode == 'avg':\n",
        "      embedded_document = self.embeddings(document)\n",
        "      D = torch.mean(embedded_document, dim=2)\n",
        "      Q = self.embeddings(question)\n",
        "    elif self.mode == 'recurrent':\n",
        "      embedded_document = self.embeddings(document)\n",
        "      batch_size = embedded_document.shape[0]\n",
        "      sent_length = embedded_document.shape[1]\n",
        "      word_length = embedded_document.shape[2]\n",
        "      temp_document = embedded_document.view(-1, word_length, self.embedding_length)\n",
        "      document_lengths = document_lengths.view(-1)\n",
        "      document_lengths[(document_lengths == 0)] = 1\n",
        "      # print(temp_document.shape, document_lengths)\n",
        "      packed_document = nn.utils.rnn.pack_padded_sequence(temp_document, document_lengths, batch_first=True, enforce_sorted=False)\n",
        "      _, D = self.recurrent(packed_document)\n",
        "      # print(D.shape)\n",
        "      D = D.reshape(batch_size, sent_length, self.embedding_length)\n",
        "\n",
        "      Q = self.embeddings(question)\n",
        "\n",
        "    return D, Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GPYQ-vb3fqe",
        "colab_type": "text"
      },
      "source": [
        "### Attention Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg5r4mmX3iCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, mode):\n",
        "    super(Attention, self).__init__()\n",
        "    if mode not in ['similarity', 'additive']:\n",
        "      raise ValueError(\"Choose a mode from - avg / recurrent\")\n",
        "    self.mode = mode\n",
        "\n",
        "  def forward(self, D, Q):\n",
        "    # print(D.shape, Q.shape)\n",
        "    if self.mode == 'similarity':\n",
        "      QT = Q.permute(0, 2, 1)\n",
        "      S = torch.bmm(D, QT)\n",
        "    elif self.mode == 'additive':\n",
        "      raise NotImplementedError\n",
        "    return S"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eQHt2lJvev3",
        "colab_type": "text"
      },
      "source": [
        "### Intensive Reading Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai_G39ujvijB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntensiveReading(nn.Module):\n",
        "  def __init__(self, mode):\n",
        "    super(IntensiveReading, self).__init__()\n",
        "\n",
        "    self.attention = Attention(mode)\n",
        "\n",
        "  def forward(self, D, Q):\n",
        "    S = self.attention(D, Q)\n",
        "    \n",
        "    AQ = F.softmax(S, dim=2)\n",
        "    D_q = torch.bmm(AQ, Q)\n",
        "    DPrime = torch.cat((D, D_q), dim=2)\n",
        "\n",
        "    AD = F.softmax(S, dim=1)\n",
        "    AD = AD.permute(0, 2, 1)\n",
        "    Q_d = torch.bmm(AD, D)\n",
        "    QPrime = torch.cat((Q, Q_d), dim=2)\n",
        "    \n",
        "    return DPrime, QPrime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNhcPvtY8G8n",
        "colab_type": "text"
      },
      "source": [
        "### Final Input Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi3A8ptt61bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputModule(nn.Module):\n",
        "  def __init__(self, sketchy_mode, intensive_mode, vocab_size, embedding_length, word_embeddings=None, bert_encoder=None):\n",
        "    super(InputModule, self).__init__()\n",
        "\n",
        "    self.sketchy_reader = SketchyReading(sketchy_mode, vocab_size, embedding_length, word_embeddings, bert_encoder)\n",
        "    self.sketchy_reader = self.sketchy_reader.to(device)\n",
        "\n",
        "    self.intensive_reader = IntensiveReading(intensive_mode)\n",
        "    self.intensive_reader = self.intensive_reader.to(device)\n",
        "    \n",
        "  def forward(self, document, document_lengths, question):\n",
        "    D, Q = self.sketchy_reader(document, document_lengths, question)\n",
        "    DPrime, QPrime = self.intensive_reader(D, Q)\n",
        "    return D, Q, DPrime, QPrime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJCQK3X8GFog",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb2tlHWvmEmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sketchy_mode = 'word'\n",
        "\n",
        "intensive_mode = 'similarity'\n",
        "if sketchy_mode == 'bert':\n",
        "  from transformers import BertModel\n",
        "\n",
        "  bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "  bert_encoder = bert_encoder.to(device)\n",
        "  vocab_size = len(tokenizer.vocab)\n",
        "  embedding_length = bert_encoder.config.to_dict()['hidden_size']\n",
        "  word_embeddings = None\n",
        "else:\n",
        "  vocab_size = len(FIELD.vocab)\n",
        "  embedding_length = 300\n",
        "  word_embeddings = FIELD.vocab.vectors\n",
        "  bert_encoder = None\n",
        "\n",
        "\n",
        "input_model = InputModule(sketchy_mode, intensive_mode, vocab_size, embedding_length, word_embeddings, bert_encoder)\n",
        "input_model = input_model.to(device)\n",
        "\n",
        "for batch in train_iterator:\n",
        "   #[B, S, W]\n",
        "  if RUN_MODE == 'word':\n",
        "    document = batch.story_text[0]\n",
        "    doc_lengths = batch.story_text[1]\n",
        "    question = batch.question[0]\n",
        "  elif RUN_MODE == 'bert':\n",
        "    document = batch.story_text\n",
        "    question = batch.question\n",
        "    doc_lengths = None\n",
        "  elif RUN_MODE == 'sentence':\n",
        "    document = batch.story_text[0]\n",
        "    doc_lengths = batch.story_text[2]\n",
        "    question = batch.question[0] #[B, W]\n",
        "  D, Q, DPrime, QPrime = input_model(document, doc_lengths, question)\n",
        "  print(\"DP\", DPrime.shape, \"QP\", QPrime.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kegF-Mcz3iJU",
        "colab_type": "text"
      },
      "source": [
        "## Episodic Memory Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhRMRiJmA0sd",
        "colab_type": "text"
      },
      "source": [
        "### Modified Attention GRU (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3l1vqn93pzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnGRUCell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(AttnGRUCell, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.Wr = nn.Linear(input_size, hidden_size)\n",
        "    self.Ur = nn.Linear(hidden_size, hidden_size)\n",
        "    self.W = nn.Linear(input_size, hidden_size)\n",
        "    self.U = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    init.xavier_normal_(self.Wr.state_dict()['weight'])\n",
        "    init.xavier_normal_(self.Ur.state_dict()['weight'])\n",
        "    init.xavier_normal_(self.W.state_dict()['weight'])\n",
        "    init.xavier_normal_(self.U.state_dict()['weight'])\n",
        "\n",
        "  def forward(self, fact, hi_1, g):\n",
        "    # fact is the final output of InputModule for each sentence and fact.size() = (batch_size, embedding_length)\n",
        "    # hi_1.size() = (batch_size, embedding_length=hidden_size)\n",
        "    # g.size() = (batch_size, )\n",
        "\n",
        "    r_i = torch.sigmoid(self.Wr(fact) + self.Ur(hi_1))\n",
        "    h_tilda = torch.tanh(self.W(fact) + r_i*self.U(hi_1))\n",
        "    g = g.unsqueeze(1)\n",
        "    hi = g*h_tilda + (1 - g)*hi_1\n",
        "\n",
        "    return hi # Returning the next hidden state considering the first fact and so on.\n",
        "\n",
        "\n",
        "class AttnGRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(AttnGRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.AttnGRUCell = AttnGRUCell(input_size, hidden_size)\n",
        "\n",
        "  def forward(self, D, G):\n",
        "    # D.size() = (batch_size, num_sentences, embedding_length)\n",
        "    # fact.size() = (batch_size, embedding_length=hidden_size)\n",
        "    # G.size() = (batch_size, num_sentences)\n",
        "    # g.size() = (batch_size, )\n",
        "\n",
        "    h_0 = Variable(torch.zeros(self.hidden_size)).cuda()\n",
        "\n",
        "    hs = []\n",
        "\n",
        "    for sen in range(D.size()[1]):\n",
        "      sentence = D[:, sen, :]\n",
        "      g = G[:, sen]\n",
        "      if sen == 0: # Initialization for first sentence only \n",
        "        hi_1 = h_0.unsqueeze(0).expand_as(sentence)\n",
        "      hi_1 = self.AttnGRUCell(sentence, hi_1, g)\n",
        "      hs.append(hi_1.unsqueeze(1))\n",
        "    \n",
        "    hs = torch.cat(hs, dim=1)\n",
        "    C = hi_1 # Final hidden vector as the contextual vector used for updating memory\n",
        "\n",
        "    return C, hs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxucppjSCNCk",
        "colab_type": "text"
      },
      "source": [
        "### Memory Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MdXYtERCPeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MemoryModule(nn.Module): # Takes Document sentences, question and prev_mem as its and output next_mem\n",
        "  def __init__(self, hidden_size):\n",
        "    super(MemoryModule, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.AttnGRU = AttnGRU(hidden_size, hidden_size)\n",
        "    self.W1 = nn.Linear(4*hidden_size, hidden_size)\n",
        "    self.W2 = nn.Linear(hidden_size, 1)\n",
        "    self.W_mem = nn.Linear(3*hidden_size, hidden_size)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    init.xavier_normal_(self.W1.state_dict()['weight'])\n",
        "    init.xavier_normal_(self.W2.state_dict()['weight'])\n",
        "    init.xavier_normal_(self.W_mem.state_dict()['weight'])\n",
        "\n",
        "  def gateMatrix(self, D, Q, prev_mem):\n",
        "    # D.size() = (batch_size, num_sentences, embedding_length=hidden_size)\n",
        "    # Q.size() = (batch_size, 1, embedding_length)\n",
        "    # prev_mem.size() = (batch_size, 1, embedding_length)\n",
        "    # z.size() = (batch_size, num_sentences, 4*embedding_length)\n",
        "    # G.size() = (batch_size, num_sentences)\n",
        "\n",
        "    Q = Q.expand_as(D)\n",
        "    prev_mem = prev_mem.expand_as(D)\n",
        "    embedding_length = D.shape[2]\n",
        "    batch_size = D.shape[0]\n",
        "    z = torch.cat([D*Q, D*prev_mem, torch.abs(D - Q), torch.abs(D - prev_mem)], dim=2)\n",
        "    # z.size() = (batch_size, num_sentences, 4*embedding_length)\n",
        "    z = z.view(-1, 4*embedding_length)\n",
        "    Z = self.W2(torch.tanh(self.W1(z)))\n",
        "    Z = Z.view(batch_size, -1)\n",
        "    G = F.softmax(Z, dim=1)\n",
        "\n",
        "    return G\n",
        "\n",
        "  def forward(self, D, Q, prev_mem):\n",
        "    # Q = Q.unsqueeze(1)\n",
        "    # prev_mem = prev_mem.unsqueeze(1)\n",
        "    G = self.gateMatrix(D, Q, prev_mem)\n",
        "    # print(\"G: \", G.shape)\n",
        "    C, hs = self.AttnGRU(D, G)\n",
        "    # Now considering prev_mem, C and question, we will update the memory state as follows\n",
        "    concat = torch.cat([prev_mem.squeeze(1), C, Q.squeeze(1)], dim=1)\n",
        "    concat = self.dropout(concat)\n",
        "    next_mem = F.relu(self.W_mem(concat))\n",
        "    next_mem = next_mem.unsqueeze(1)\n",
        "\n",
        "    return next_mem, hs, G"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2_Aco-DCom",
        "colab_type": "text"
      },
      "source": [
        "### Final Episodic Memory Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5IpUxekDIOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpisodicMemoryModule(nn.Module):\n",
        "  def __init__(self, embedding_length, hidden_size, num_passes):\n",
        "    super(EpisodicMemoryModule, self).__init__()\n",
        "\n",
        "    self.embedding_length = embedding_length\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_passes = num_passes\n",
        "\n",
        "    self.recurrent = nn.LSTM(self.embedding_length, self.hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(self.embedding_length, self.hidden_size)\n",
        "\n",
        "    self.memory = MemoryModule(self.hidden_size)\n",
        "\n",
        "  def forward(self, D, Q, question_lengths):\n",
        "    #D.size()= (batch_size, num_sentences, embedding_length) \n",
        "    #Q.size() = (batch_size, num_words, embedding_length)\n",
        "\n",
        "    D = F.relu(self.fc(D))\n",
        "    if question_lengths is None:\n",
        "      QPacked = Q\n",
        "    else:\n",
        "      QPacked = nn.utils.rnn.pack_padded_sequence(Q, question_lengths, batch_first=True, enforce_sorted=False)\n",
        "    _, (Q, _) = self.recurrent(QPacked)\n",
        "    Q = Q.permute(1, 0, 2)\n",
        "    all_g = []\n",
        "    hs = D\n",
        "    M = Q\n",
        "    for passes in range(self.num_passes):\n",
        "        M, hs, G = self.memory(hs, Q, M)\n",
        "        all_g.append(G.unsqueeze(1))\n",
        "    return M, hs, torch.cat(all_g, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAAaAyFGB9j",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55cMFZ6iGBkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sketchy_mode = 'word'\n",
        "intensive_mode = 'similarity'\n",
        "if sketchy_mode == 'bert':\n",
        "  from transformers import BertModel\n",
        "\n",
        "  bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "  bert_encoder = bert_encoder.to(device)\n",
        "  vocab_size = len(tokenizer.vocab)\n",
        "  embedding_length = bert_encoder.config.to_dict()['hidden_size']\n",
        "  hidden_size = embedding_length\n",
        "  word_embeddings = None\n",
        "else:\n",
        "  vocab_size = len(FIELD.vocab)\n",
        "  embedding_length = 300\n",
        "  hidden_size = embedding_length\n",
        "  word_embeddings = FIELD.vocab.vectors\n",
        "  bert_encoder = None\n",
        "\n",
        "num_passes = 3\n",
        "\n",
        "\n",
        "input_model = InputModule(sketchy_mode, intensive_mode, vocab_size, embedding_length, word_embeddings, bert_encoder)\n",
        "input_model = input_model.to(device)\n",
        "\n",
        "memory_model = EpisodicMemoryModule(2*embedding_length, hidden_size, num_passes)\n",
        "memory_model = memory_model.to(device)\n",
        "\n",
        "for batch in train_iterator:\n",
        "   #[B, S, W]\n",
        "  if RUN_MODE == 'word':\n",
        "    document = batch.story_text[0]\n",
        "    doc_lengths = batch.story_text[1]\n",
        "    question = batch.question[0]\n",
        "    question_lengths = batch.question[1]\n",
        "  elif RUN_MODE == 'bert':\n",
        "    document = batch.story_text\n",
        "    question = batch.question\n",
        "    doc_lengths = None\n",
        "    question_lengths = None\n",
        "  elif RUN_MODE == 'sentence':\n",
        "    document = batch.story_text[0]\n",
        "    doc_lengths = batch.story_text[2]\n",
        "    question = batch.question[0] #[B, W]\n",
        "    question_lengths = batch.question[1]\n",
        "\n",
        "  D, Q, DPrime, QPrime = input_model(document, doc_lengths, question)\n",
        "  print(\"DP\", DPrime.shape, \"QP\", QPrime.shape)\n",
        "\n",
        "  M, hs, all_g = memory_model(DPrime, QPrime, question_lengths)\n",
        "  print(\"M\", M.shape, \" HS: \", hs.shape, \"all_g\": all_g.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XoLPKDB3p50",
        "colab_type": "text"
      },
      "source": [
        "## Verification Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RdEWfAY3tbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VerificationModule(nn.Module):\n",
        "  def __init__(self, mode, pool_mode, hidden_size):\n",
        "    super(VerificationModule, self).__init__()\n",
        "\n",
        "    if mode not in ['external', 'internal']:\n",
        "      raise ValueError(\"Choose a mode from - external / internal\")\n",
        "    self.mode = mode\n",
        "\n",
        "    if pool_mode not in ['max', 'avg']:\n",
        "      raise ValueError(\"Choose a mode from - max / avg\")\n",
        "    self.pool_mode = pool_mode\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.fc = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "  def forward(self, D, Q=None):\n",
        "    if self.mode == 'external':\n",
        "      if Q is None:\n",
        "        raise ValueError(\"Q cannot be None when mode = external\")\n",
        "      DQ = torch.cat((D, Q), dim=1)\n",
        "    else:\n",
        "      DQ = D\n",
        "\n",
        "    if self.pool_mode == 'max':\n",
        "      DQ, _ = torch.max(DQ, dim=1)\n",
        "    elif self.pool_mode == 'avg':\n",
        "      DQ = torch.mean(DQ, dim=1)\n",
        "    \n",
        "    verifier_score = F.relu(self.fc(DQ))\n",
        "    return verifier_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7GOrTbB3ti0",
        "colab_type": "text"
      },
      "source": [
        "## Output Module (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wle4jui3v4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OutputModule(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(OutputModule, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.fc_start = nn.Linear(self.hidden_size, 1)\n",
        "    self.fc_end = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "  def forward(self, M):\n",
        "    M = self.dropout(M)\n",
        "    start = F.relu(self.fc_start(M))\n",
        "    end = F.relu(self.fc_end(M))\n",
        "    return start, end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1V-ecTQz-Kr",
        "colab_type": "text"
      },
      "source": [
        "## Complete Architecture (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_zCpLIh0Cvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, sketchy_mode, intensive_mode, pool_mode, vocab_size, hidden_size, embedding_length, word_embeddings, bert_encoder=None, num_passes=3, skip_memory = False):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.sketchy_mode = sketchy_mode\n",
        "    self.intensive_mode = intensive_mode\n",
        "    self.pool_mode = pool_mode\n",
        "    self.skip_memory = skip_memory\n",
        "\n",
        "    self.input_model = InputModule(sketchy_mode, intensive_mode, vocab_size, embedding_length, word_embeddings, bert_encoder=bert_encoder)\n",
        "    self.input_model = self.input_model.to(device)\n",
        "\n",
        "    self.memory_model = EpisodicMemoryModule(2*embedding_length, hidden_size, num_passes)\n",
        "    self.memory_model = self.memory_model.to(device)\n",
        "\n",
        "    self.external_verifier = VerificationModule('external', pool_mode, hidden_size)\n",
        "    self.external_verifier = self.external_verifier.to(device)\n",
        "\n",
        "    self.internal_verifier = VerificationModule('internal', pool_mode, hidden_size)\n",
        "    self.internal_verifier = self.internal_verifier.to(device)\n",
        "\n",
        "    self.output_model = OutputModule(hidden_size)\n",
        "    self.output_model = self.output_model.to(device)\n",
        "\n",
        "    self.fc = nn.Linear(2*embedding_length, embedding_length)\n",
        "\n",
        "  def forward(self, document, doc_lengths, question, question_lengths):\n",
        "    D, Q, DPrime, QPrime = self.input_model(document, doc_lengths, question)\n",
        "    # print(\"DP\", DPrime.shape, \"QP\", QPrime.shape)\n",
        "\n",
        "    external_verifier_score = self.external_verifier(D, Q)\n",
        "    \n",
        "    all_g = None\n",
        "    if self.skip_memory:\n",
        "      all_hidden = F.relu(self.fc(DPrime))\n",
        "      M = None\n",
        "    else:\n",
        "      M, all_hidden, all_g = self.memory_model(DPrime, QPrime, question_lengths)\n",
        "      seq_length = DPrime.shape[1]\n",
        "      MPrime = M.repeat(1, seq_length, 1)\n",
        "      DPrime = F.relu(self.fc(DPrime))\n",
        "      I = torch.cat((DPrime, MPrime), dim=2)\n",
        "      all_hidden = F.relu(self.fc(I))\n",
        "\n",
        "    internal_verifier_score = self.internal_verifier(all_hidden)\n",
        "\n",
        "    if self.sketchy_mode in ['word', 'bert']:\n",
        "      start, end = self.output_model(all_hidden)\n",
        "      start = start.squeeze(2)\n",
        "      end = end.squeeze(2)\n",
        "    else:\n",
        "      M = M.squeeze(1)\n",
        "      start, end = self.output_model(M)\n",
        "    return start, end, external_verifier_score, internal_verifier_score, all_g\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2RG0NU37Uls",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dw10qo73WM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sketchy_mode = 'word'\n",
        "intensive_mode = 'similarity'\n",
        "pool_mode = 'max'\n",
        "if sketchy_mode == 'bert':\n",
        "  from transformers import BertModel\n",
        "  bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "  bert_encoder = bert_encoder.to(device)\n",
        "  vocab_size = len(tokenizer.vocab)\n",
        "  embedding_length = bert_encoder.config.to_dict()['hidden_size']\n",
        "  hidden_size = embedding_length\n",
        "  word_embeddings = None\n",
        "else:\n",
        "  vocab_size = len(FIELD.vocab)\n",
        "  embedding_length = 300\n",
        "  hidden_size = embedding_length\n",
        "  word_embeddings = FIELD.vocab.vectors\n",
        "  bert_encoder = None\n",
        "num_passes = 3\n",
        "\n",
        "model = Model(sketchy_mode, intensive_mode, pool_mode, vocab_size, hidden_size, embedding_length, word_embeddings, bert_encoder, num_passes)\n",
        "model = model.to(device)\n",
        "\n",
        "for batch in train_iterator:\n",
        "  #[B, S, W]\n",
        "  if RUN_MODE == 'word':\n",
        "    document = batch.story_text[0]\n",
        "    doc_lengths = batch.story_text[1]\n",
        "    question = batch.question[0]\n",
        "    question_lengths = batch.question[1]\n",
        "  elif RUN_MODE == 'bert':\n",
        "    document = batch.story_text\n",
        "    question = batch.question\n",
        "    doc_lengths = None\n",
        "    question_lengths = None\n",
        "  elif RUN_MODE == 'sentence':\n",
        "    document = batch.story_text[0]\n",
        "    doc_lengths = batch.story_text[2]\n",
        "    question = batch.question[0] #[B, W]\n",
        "    question_lengths = batch.question[1]\n",
        "  \n",
        "  start, end, ext_score, int_score = model(document, doc_lengths, question, question_lengths)\n",
        "\n",
        "  if RUN_MODE == 'bert':\n",
        "    bert_tokenizer = tokenizer\n",
        "    vocab_itos = None\n",
        "  else:\n",
        "    vocab_itos = FIELD.vocab.itos\n",
        "    bert_tokenizer = None\n",
        "  EvaluationMetrics.formulate_answer_as_string(sketchy_mode, start, end, document, vocab_itos, bert_tokenizer)\n",
        "  print(\"Start: \", start.shape, \" End: \", end.shape, \" ext_score: \", ext_score.shape, \" int_score: \", int_score.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc81mpPeVxeP",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPEuF5XaMYKI",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Metrics (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw50fLTZMapt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "\n",
        "class EvaluationMetrics():\n",
        "\n",
        "  def __init__(self):\n",
        "    return 0\n",
        "\n",
        "  @staticmethod\n",
        "  def formulate_answer_as_string(mode, start_index, end_index, document, vocab_itos=None, bert_tokenizer=None):\n",
        "    batch_size = document.shape[0]\n",
        "    document = document.view(batch_size, -1)\n",
        "    all_text = []\n",
        "    # print(start_index.shape, end_index.shape)\n",
        "    for idx in range(batch_size):\n",
        "      doc = document[idx, :]\n",
        "      if mode == 'word' or mode == 'bert':\n",
        "        s = torch.argmax(start_index[idx, :])\n",
        "        e = torch.argmax(end_index[idx, :])\n",
        "      elif mode == 'true':\n",
        "        s = start_index[idx, :]\n",
        "        e = end_index[idx, :]\n",
        "      elif mode == 'test':\n",
        "        s = start_index[idx]\n",
        "        e = end_index[idx]\n",
        "      else:\n",
        "        s = torch.floor(start_index[idx, :]).long()\n",
        "        e = torch.floor(end_index[idx, :]).long()\n",
        "      \n",
        "      if s < 0 and e < 0:\n",
        "        text = ''\n",
        "      else:\n",
        "        if vocab_itos is None:\n",
        "          text = ' '.join(tokenizer.convert_ids_to_tokens(doc)[s:e+1])\n",
        "        else:\n",
        "          if e >= len(doc) or s >= len(doc):\n",
        "            text = ''\n",
        "          else:\n",
        "            text = ' '.join([vocab_itos[doc[i]] for i in range(s, e+1)])\n",
        "      all_text.append(text)\n",
        "\n",
        "    # print(all_text)\n",
        "    return all_text\n",
        "\n",
        "  @staticmethod\n",
        "  def f1_score(prediction, ground_truth):\n",
        "    f1 = 0\n",
        "    result = []\n",
        "    for idx in range(len(prediction)):\n",
        "      prediction_tokens = EvaluationMetrics.normalize_answer(prediction[idx]).split()\n",
        "      ground_truth_tokens = EvaluationMetrics.normalize_answer(ground_truth[idx]).split()\n",
        "      common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "      num_same = sum(common.values())\n",
        "      if num_same == 0:\n",
        "          f1 += 0\n",
        "          result.append(0)\n",
        "          continue\n",
        "      precision = 1.0 * num_same / len(prediction_tokens)\n",
        "      recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "      curr_f1 = (2 * precision * recall) / (precision + recall)\n",
        "      f1 += curr_f1\n",
        "      result.append(curr_f1)\n",
        "    return f1/len(prediction), result\n",
        "\n",
        "  @staticmethod\n",
        "  def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "      return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "      return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "      exclude = set(string.punctuation)\n",
        "      return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "      return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "  @staticmethod\n",
        "  def exact_match_score(prediction, ground_truth):\n",
        "    em = 0\n",
        "    result = []\n",
        "    for idx in range(len(prediction)):\n",
        "      curr_em = EvaluationMetrics.normalize_answer(prediction[idx]) == EvaluationMetrics.normalize_answer(ground_truth[idx])\n",
        "      if curr_em == 1:\n",
        "        result.append(1)\n",
        "      else:\n",
        "        result.append(0)\n",
        "      em += (curr_em)\n",
        "    return em/len(prediction), result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV0yF0FAT0rf",
        "colab_type": "code",
        "outputId": "bbf4e2f5-50f3-4623-dda1-773b471707f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p = [\"hello how are you\", \"hi hello\", \"bye\"]\n",
        "t = [\"hello how are you\", \"dog hello\", \"bye hi\"]\n",
        "f1 = EvaluationMetrics.f1_score(p, t)\n",
        "em = EvaluationMetrics.exact_match_score(p, t)\n",
        "print(f1, em)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7222222222222222 0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUWnFP0NgC6_",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL2Oj34MgFDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# true_labels --> {'start_index': val, 'end_index': val, 'unanswerable_ext': 1/0, 'unanswerable_int': 1/0}\n",
        "# predictions --> {'start_index': val, 'end_index': val, 'unanswerable_ext': 1/0, 'unanswerable_int': 1/0}\n",
        "def retrospective_loss(true_labels, predictions, mode='word', alpha1=0.5, alpha2=0.5):\n",
        "  ans_loss = nn.BCEWithLogitsLoss()\n",
        "  if mode in ['word', 'bert']:\n",
        "    span_loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "  else:\n",
        "    span_loss = nn.MSELoss()\n",
        "  # print(true_labels['start_index'].dtype)\n",
        "  loss_span = (span_loss(predictions['start_index'], true_labels['start_index']) \\\n",
        "               + span_loss(predictions['end_index'], true_labels['end_index']))/2.0\n",
        "  loss_ans = (ans_loss(predictions['unanswerable_ext'], true_labels['unanswerable_ext'].float()) \\\n",
        "              + ans_loss(predictions['unanswerable_int'], true_labels['unanswerable_int'].float()))/2.0\n",
        "\n",
        "  return (alpha1 * loss_span + alpha2 * loss_ans), None\n",
        "\n",
        "# true_labels --> {'start_index': val, 'end_index': val, 'unanswerable_ext': 1/0, 'unanswerable_int': 1/0}\n",
        "# predictions --> {'start_index': val, 'end_index': val, 'unanswerable_ext': 1/0, 'unanswerable_int': 1/0}\n",
        "def retrospective_parallel_loss(true_labels, predictions, mode='word', alpha1=0.1, alpha2=1):\n",
        "  batch_size = len(true_labels['unanswerable_int'])\n",
        "  with torch.no_grad():\n",
        "    positives = torch.sum(true_labels['unanswerable_int'], dim=0)\n",
        "    pos_weight = (batch_size - positives) / (positives + 1e-4)\n",
        "  \n",
        "  ans_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "  if mode in ['word', 'bert']:\n",
        "    span_loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "  else:\n",
        "    span_loss = nn.MSELoss()\n",
        "  # print(true_labels['start_index'].dtype)\n",
        "  loss_span = (span_loss(predictions['start_index'], true_labels['start_index']) \\\n",
        "               + span_loss(predictions['end_index'], true_labels['end_index']))\n",
        "  # loss_ans = (ans_loss(predictions['unanswerable_ext'], true_labels['unanswerable_ext'].float()) \\\n",
        "  #             + ans_loss(predictions['unanswerable_int'], true_labels['unanswerable_int'].float()))/2.0\n",
        "  loss_ans_int = ans_loss(predictions['unanswerable_int'], true_labels['unanswerable_int'].float())\n",
        "  loss_ans_ext = ans_loss(predictions['unanswerable_ext'], true_labels['unanswerable_ext'].float())\n",
        "\n",
        "  # print(\"Span: \", loss_span, \" int: \", loss_ans_int, \" ext: \", loss_ans_ext)\n",
        "  return (alpha1 * loss_span + alpha2 * loss_ans_int), loss_ans_ext\n",
        "\n",
        "# true_labels --> {'start_index': val, 'end_index': val, 'unanswerable_ext': 1/0, 'unanswerable_int': 1/0}\n",
        "# predictions --> {'start_index': val, 'end_index': val, 'unanswerable_ext': 1/0, 'unanswerable_int': 1/0}\n",
        "def retrospective_loss_span(true_labels, predictions, mode='word', alpha1=0.5, alpha2=0.5):\n",
        "  ans_loss = nn.BCEWithLogitsLoss()\n",
        "  if mode in ['word', 'bert']:\n",
        "    span_loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "  else:\n",
        "    span_loss = nn.MSELoss()\n",
        "  # print(true_labels['start_index'].dtype)\n",
        "  loss_span = (span_loss(predictions['start_index'], true_labels['start_index']) \\\n",
        "               + span_loss(predictions['end_index'], true_labels['end_index']))\n",
        "  # loss_ans = (ans_loss(predictions['unanswerable_ext'], true_labels['unanswerable_ext'].float()) \\\n",
        "  #             + ans_loss(predictions['unanswerable_int'], true_labels['unanswerable_int'].float()))/2.0\n",
        "\n",
        "  return loss_span, None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj3Ilfsp6RDW",
        "colab_type": "text"
      },
      "source": [
        "### Trainer and Evaluator (R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtcGB8D1SBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Experiment():\n",
        "  def __init__(self, hyperparameters, log_dir=\"runs\", verbose=True, print_every=100):\n",
        "    self.hyperparameters = hyperparameters\n",
        "    self.log_dir = log_dir\n",
        "    self.verbose = verbose\n",
        "    self.print_every = print_every\n",
        "\n",
        "    self.model = Model(self.hyperparameters['sketchy_mode'], \\\n",
        "                       self.hyperparameters['intensive_mode'],\\\n",
        "                       self.hyperparameters['pool_mode'],\\\n",
        "                       self.hyperparameters['vocab_size'],\\\n",
        "                       self.hyperparameters['hidden_size'],\\\n",
        "                       self.hyperparameters['embedding_length'],\\\n",
        "                       self.hyperparameters['word_embeddings'],\\\n",
        "                       self.hyperparameters['bert_encoder'],\\\n",
        "                       self.hyperparameters['num_passes'],\\\n",
        "                       self.hyperparameters['skip_memory'])\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "\n",
        "  def train_classifier(self, model, dataset_iterator, loss_function, optimizer, vocab_itos, \\\n",
        "                     num_epochs = 10, log = \"runs\", verbose = True, print_every = 100, expt_name = \"default\", start_epoch = 0):\n",
        "    #tensorboard writer\n",
        "    writer = SummaryWriter(log_dir=log)\n",
        "    model.train()\n",
        "    step = 0\n",
        "    parallel = False\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "      total = 0\n",
        "      total_loss = 0\n",
        "      total_ext_loss = 0\n",
        "      total_f1 = 0\n",
        "      total_em = 0\n",
        "      correct_start = 0\n",
        "      correct_end = 0\n",
        "      correct_both = 0\n",
        "      epoch_step = 0\n",
        "      all_true_ans = []\n",
        "      all_pred_ans = []\n",
        "      all_result_em = []\n",
        "      all_result_f1 = []\n",
        "      for batch in dataset_iterator:\n",
        "        if RUN_MODE == 'word':\n",
        "          document = batch.story_text[0]\n",
        "          doc_lengths = batch.story_text[1]\n",
        "          question = batch.question[0]\n",
        "          question_lengths = batch.question[1]\n",
        "        elif RUN_MODE == 'bert':\n",
        "          document = batch.story_text\n",
        "          question = batch.question\n",
        "          doc_lengths = None\n",
        "          question_lengths = None\n",
        "        elif RUN_MODE == 'sentence':\n",
        "          document = batch.story_text[0]\n",
        "          doc_lengths = batch.story_text[2]\n",
        "          question = batch.question[0] #[B, W]\n",
        "          question_lengths = batch.question[1]\n",
        "        # true_start_index = batch.word_start_index_1\n",
        "        # true_end_index = batch.word_end_index_1\n",
        "        true_start_index = batch.word_start_index\n",
        "        true_end_index = batch.word_end_index\n",
        "        unanswerable = batch.is_answer_absent\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_start_index, pred_end_index, pred_ext_score, pred_int_score = model(document, doc_lengths, question, question_lengths)\n",
        "        # print(pred_start_index, pred_end_index)\n",
        "        # print(true_start_index, true_end_index)\n",
        "        predictions = {'start_index': pred_start_index, 'end_index': pred_end_index, 'unanswerable_ext': pred_ext_score.squeeze(1), 'unanswerable_int': pred_int_score.squeeze(1)}\n",
        "        if model.sketchy_mode in ['word', 'bert']:\n",
        "          true_labels = {'start_index': true_start_index, 'end_index': true_end_index, 'unanswerable_ext': unanswerable, 'unanswerable_int': unanswerable}\n",
        "        else:\n",
        "          true_labels = {'start_index': true_start_index.float(), 'end_index': true_end_index.float(), 'unanswerable_ext': unanswerable, 'unanswerable_int': unanswerable}\n",
        "        \n",
        "        loss = loss_function(true_labels, predictions, model.sketchy_mode)\n",
        "        # print(loss)\n",
        "        if loss[1] is not None:\n",
        "          # Parallel training mode\n",
        "          parallel = True\n",
        "          ext_loss = loss[1]\n",
        "          final_loss = loss[0]\n",
        "          ext_loss.backward()\n",
        "          final_loss.backward()\n",
        "          total_ext_loss += ext_loss.item()\n",
        "          total_loss += final_loss.item()\n",
        "        else:\n",
        "          # print(\"l: \", loss)\n",
        "          loss[0].backward()\n",
        "          total_loss += loss[0].item()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # print(model.sketchy_mode, pred_start_index, pred_end_index, document)\n",
        "        all_pred_text = EvaluationMetrics.formulate_answer_as_string(model.sketchy_mode, pred_start_index, pred_end_index, document, self.hyperparameters['vocab_itos'], self.hyperparameters['bert_tokenizer'])\n",
        "        all_true_text = EvaluationMetrics.formulate_answer_as_string(\"true\", true_start_index.unsqueeze(1), true_end_index.unsqueeze(1), document, self.hyperparameters['vocab_itos'], self.hyperparameters['bert_tokenizer'])\n",
        "        \n",
        "        f1, result_f1 = EvaluationMetrics.f1_score(all_pred_text, all_true_text)\n",
        "        total_f1 += f1\n",
        "        all_result_f1 += result_f1\n",
        "        \n",
        "        em, result_em = EvaluationMetrics.exact_match_score(all_pred_text, all_true_text)\n",
        "        total_em += em\n",
        "        all_result_em += result_em\n",
        "\n",
        "        all_true_ans += all_true_text\n",
        "        all_pred_ans += all_pred_text\n",
        "        \n",
        "\n",
        "        start_pred = torch.argmax(pred_start_index, dim=1)\n",
        "        correct_start += (torch.sum(start_pred == true_start_index)).item()\n",
        "        # start_acc = (correct_start / len(true_start_index))\n",
        "\n",
        "        end_pred = torch.argmax(pred_end_index, dim=1)\n",
        "        correct_end += (torch.sum(end_pred == true_end_index)).item()\n",
        "        # end_acc += (correct_end / len(true_end_index))\n",
        "\n",
        "        correct_both += (torch.sum((start_pred == true_start_index) & (end_pred == true_end_index))).item()\n",
        "\n",
        "        epoch_step += 1\n",
        "\n",
        "        # batch_size = document.shape[0]\n",
        "        # seq_length = document.shape[1]\n",
        "        # s = [0 for _ in range(batch_size)]\n",
        "        # e = [seq_length-1 for _ in range(batch_size)]\n",
        "        # doc_text = EvaluationMetrics.formulate_answer_as_string(\"test\", s, e, document, self.hyperparameters['vocab_itos'], None)\n",
        "        # print(\"Doc: \", doc_text)\n",
        "        # print(\"True indices: \", true_start_index, true_end_index, \"True: \", all_true_text)\n",
        "        # print(\"Pred indices: \", pred_start_index, pred_end_index,\"Pred: \", all_pred_text)\n",
        "\n",
        "        total += len(true_start_index)\n",
        "        \n",
        "\n",
        "        if ((step % print_every) == 0):\n",
        "          if parallel:\n",
        "            writer.add_scalar(\"External Loss/train\", total_ext_loss/epoch_step, step)\n",
        "            writer.add_scalar(\"Final Loss/train\", total_loss/epoch_step, step)\n",
        "          else:\n",
        "            writer.add_scalar(\"Loss/train\", total_loss/epoch_step, step)\n",
        "          writer.add_scalar(\"F1/train\", total_f1/epoch_step, step)\n",
        "          writer.add_scalar(\"EM/train\", total_em/epoch_step, step)\n",
        "          writer.add_scalar(\"Start Acc/train\", correct_start/total, step)\n",
        "          writer.add_scalar(\"End Acc/train\", correct_end/total, step)\n",
        "          writer.add_scalar(\"Acc/train\", correct_both/total, step)\n",
        "          \n",
        "          writer.add_text(\"Predictions\", '\\n'.join(all_pred_text), step)\n",
        "          writer.add_text(\"True\", '\\n'.join(all_true_text), step)\n",
        "          if verbose:\n",
        "            if parallel:\n",
        "              print(\"--- Step: %s Ext Loss: %s Final Loss: %s Start Acc: %s End Acc: %s Acc: %s F1: %s EM: %s\" %(step, total_ext_loss/epoch_step, total_loss/epoch_step, (correct_start*100)/total, (correct_end*100)/total, (correct_both*100)/total, (total_f1 * 100)/epoch_step, (total_em * 100)/epoch_step))\n",
        "            else:\n",
        "              print(\"--- Step: %s Loss: %s Start Acc: %s End Acc: %s Acc: %s F1: %s EM: %s\" %(step, total_loss/epoch_step,  (correct_start*100)/total, (correct_end*100)/total, (correct_both*100)/total, (total_f1 * 100)/epoch_step, (total_em * 100)/epoch_step))\n",
        "        step = step+1\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Move everything back to cpu\n",
        "        # document = document.to('cpu')\n",
        "        # question = question.to('cpu')\n",
        "        # true_start_index = true_start_index.to('cpu')\n",
        "        # true_end_index = true_end_index.to('cpu')\n",
        "        # unanswerable = unanswerable.to('cpu')\n",
        "\n",
        "\n",
        "      print(\"Saving model...\")\n",
        "      state_to_save = {'epoch': epoch,\\\n",
        "                       'model_state_dict': model.state_dict(),\\\n",
        "                       'optimizer_state_dict': optimizer.state_dict()}\n",
        "      torch.save(state_to_save, '/content/drive/Shared drives/CIS 700-1 Final Project/models/' + expt_name + '_' + str(epoch) + '.pt')\n",
        "\n",
        "      print(\"Saving outputs...\")\n",
        "      output_dict = {'true_answer': all_true_ans,\\\n",
        "                     'pred_answer': all_pred_ans,\\\n",
        "                     'EM': all_result_em,\\\n",
        "                     'F1': all_result_f1}\n",
        "      output_df = pd.DataFrame(output_dict)\n",
        "      output_df.to_csv('/content/drive/Shared drives/CIS 700-1 Final Project/models/' + expt_name + '_' + str(epoch) + '.csv')\n",
        "      \n",
        "      if parallel:\n",
        "        print(\"Epoch: %s External Loss: %s Final Loss: %s Start Acc: %s End Acc: %s Acc: %s F1: %s EM: %s\"%(epoch+1, total_ext_loss/epoch_step, total_loss/epoch_step, (correct_start*100)/total, (correct_end*100)/total, (correct_both*100)/total, (total_f1 * 100)/epoch_step, (total_em * 100)/epoch_step))\n",
        "      else:\n",
        "        print(\"Epoch: %s Loss: %s Start Acc: %s End Acc: %s Acc: %s F1: %s EM: %s\"%(epoch+1, total_loss/epoch_step,  (correct_start*100)/total, (correct_end*100)/total, (correct_both*100)/total, (total_f1 * 100)/epoch_step, (total_em * 100)/epoch_step))\n",
        "\n",
        "      self.model = model\n",
        "\n",
        "\n",
        "  def train(self, train_iterator, expt_name=\"default\", load_model=False):\n",
        "    loss_function = self.hyperparameters['loss_function']\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hyperparameters['learning_rate'])\n",
        "    if load_model:\n",
        "      print(\"Loading model...\")\n",
        "      checkpoint = torch.load('/content/drive/Shared drives/CIS 700-1 Final Project/models/' + expt_name + '.pt')\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      start_epoch = checkpoint['epoch']\n",
        "    else:\n",
        "      start_epoch = 0\n",
        "    \n",
        "    self.train_classifier(self.model,\\\n",
        "                     train_iterator,\\\n",
        "                     loss_function,\\\n",
        "                     optimizer,\\\n",
        "                     self.hyperparameters['vocab_itos'],\\\n",
        "                     self.hyperparameters['num_epochs'],\\\n",
        "                     self.log_dir,\\\n",
        "                     self.verbose,\\\n",
        "                     self.print_every,\\\n",
        "                     expt_name,\\\n",
        "                     start_epoch)\n",
        "    \n",
        "    return self.model\n",
        "\n",
        "  # predictions --> {'start_index': val, 'end_index': val, 'ext_score': 1/0, 'int_score': 1/0}\n",
        "  # parameters --> {'beta1': val, 'beta2': val, 'lambda1': val, 'lambda2': val, 'delta': val}\n",
        "  def rear_verification(self, predictions, parameters):\n",
        "    all_start = []\n",
        "    all_end = []\n",
        "\n",
        "    batch_size = predictions['start_index'].shape[0]\n",
        "    seq_length = predictions['start_index'].shape[1]\n",
        "    for idx in range(batch_size):\n",
        "      s = predictions['start_index'][idx, :]\n",
        "      e = predictions['end_index'][idx, :]\n",
        "\n",
        "      grid_s, grid_e = torch.meshgrid(s.squeeze(), e.squeeze())\n",
        "      sums = torch.triu(grid_s + grid_e)\n",
        "      # print(sums.shape)\n",
        "      score_has = torch.max(sums)\n",
        "      indices = torch.argmax(sums)\n",
        "      final_start = int(indices/seq_length)\n",
        "      final_end = int(indices%seq_length)\n",
        "      \n",
        "      pooled_indices = torch.max(s) + torch.max(e)\n",
        "      v = parameters['beta1']*predictions['unanswerable_ext'][idx] + parameters['beta2']*predictions['unanswerable_int'][idx]\n",
        "      score_na = parameters['lambda1']*pooled_indices + parameters['lambda2']*v\n",
        "\n",
        "      final_score = torch.abs(score_has - score_na)\n",
        "      if final_score >= parameters['delta']:\n",
        "        all_start.append(final_start)\n",
        "        all_end.append(final_end)\n",
        "      else:\n",
        "        all_start.append(-1)\n",
        "        all_end.append(-1)\n",
        "    return torch.tensor(all_start), torch.tensor(all_end)\n",
        "\n",
        "  def evaluate_classifier(self, model, dataset_iterator, evaluation_parameters, vocab_itos, expt_name = \"default\"):\n",
        "    #tensorboard writer\n",
        "    model.eval()\n",
        "    step = 0\n",
        "    total_f1 = 0\n",
        "    total_em = 0\n",
        "    total = 0\n",
        "    correct_start = 0\n",
        "    correct_end = 0\n",
        "    correct_both = 0\n",
        "    # all_true_ans = []\n",
        "    # all_pred_ans = []\n",
        "    all_start = []\n",
        "    all_end = []\n",
        "    # all_true_start = []\n",
        "    # all_true_end = []\n",
        "    # all_doc_text = []\n",
        "    # all_question_text = []\n",
        "    all_output = []\n",
        "    for batch in dataset_iterator:\n",
        "      if RUN_MODE == 'word':\n",
        "        document = batch.story_text[0]\n",
        "        doc_lengths = batch.story_text[1]\n",
        "        question = batch.question[0]\n",
        "        question_lengths = batch.question[1]\n",
        "      elif RUN_MODE == 'bert':\n",
        "        document = batch.story_text\n",
        "        question = batch.question\n",
        "        doc_lengths = None\n",
        "        question_lengths = None\n",
        "      elif RUN_MODE == 'sentence':\n",
        "        document = batch.story_text[0]\n",
        "        doc_lengths = batch.story_text[2]\n",
        "        question = batch.question[0] #[B, W]\n",
        "        question_lengths = batch.question[1]\n",
        "      true_start_index = batch.word_start_index\n",
        "      true_end_index = batch.word_end_index\n",
        "      unanswerable = batch.is_answer_absent\n",
        "\n",
        "      with torch.no_grad():\n",
        "        pred_start_index, pred_end_index, pred_ext_score, pred_int_score, attention_scores = model(document, doc_lengths, question, question_lengths)\n",
        "\n",
        "        predictions = {'start_index': pred_start_index, 'end_index': pred_end_index, 'unanswerable_ext': pred_ext_score.squeeze(1), 'unanswerable_int': pred_int_score.squeeze(1)}\n",
        "        final_pred_start, final_pred_end = self.rear_verification(predictions, evaluation_parameters)\n",
        "        all_pred_text = EvaluationMetrics.formulate_answer_as_string(\"test\", final_pred_start.data, final_pred_end.data, document.data, self.hyperparameters['vocab_itos'], self.hyperparameters['bert_tokenizer'])\n",
        "\n",
        "        if model.sketchy_mode in ['word', 'bert']:\n",
        "          true_labels = {'start_index': true_start_index.data, 'end_index': true_end_index.data, 'unanswerable_ext': unanswerable.data, 'unanswerable_int': unanswerable.daya}\n",
        "        else:\n",
        "          true_labels = {'start_index': true_start_index.data.float(), 'end_index': true_end_index.data.float(), 'unanswerable_ext': unanswerable.data, 'unanswerable_int': unanswerable.data}\n",
        "        all_true_text = EvaluationMetrics.formulate_answer_as_string(\"true\", true_start_index.data.unsqueeze(1), true_end_index.data.unsqueeze(1), document.data, self.hyperparameters['vocab_itos'], self.hyperparameters['bert_tokenizer'])\n",
        "\n",
        "        # f1, result_f1 = EvaluationMetrics.f1_score(all_pred_text, all_true_text)\n",
        "        # total_f1 += f1\n",
        "        # # all_result_f1 += result_f1\n",
        "        \n",
        "        # em, result_em = EvaluationMetrics.exact_match_score(all_pred_text, all_true_text)\n",
        "        # total_em += em\n",
        "        # # all_result_em += result_em\n",
        "\n",
        "        # all_true_ans += all_true_text\n",
        "        # all_pred_ans += all_pred_text\n",
        "\n",
        "        # all_true_start += true_start_index.data.tolist()\n",
        "        # all_true_end += true_end_index.data.tolist()\n",
        "        # print(\"TS: \", all_true_start)\n",
        "        # print(\"TE: \", all_true_end)\n",
        "\n",
        "        batch_size = document.shape[0]\n",
        "        seq_length = document.shape[1]\n",
        "        s = [0 for _ in range(batch_size)]\n",
        "        e = [seq_length-1 for _ in range(batch_size)]\n",
        "        all_doc_text = EvaluationMetrics.formulate_answer_as_string(\"test\", s, e, document.data, self.hyperparameters['vocab_itos'], None)\n",
        "        # print(\"D: \", all_doc_text)\n",
        "\n",
        "        # seq_length = question.shape[1]\n",
        "        # s = [0 for _ in range(batch_size)]\n",
        "        # e = [seq_length-1 for _ in range(batch_size)]\n",
        "        # all_question_text += EvaluationMetrics.formulate_answer_as_string(\"test\", s, e, question.data, self.hyperparameters['vocab_itos'], None)\n",
        "        # print(\"Q: \", all_question_text)\n",
        "        \n",
        "\n",
        "        start_pred = torch.argmax(pred_start_index, dim=1)\n",
        "        all_start += start_pred\n",
        "        correct_start += (torch.sum(start_pred == true_start_index)).item()\n",
        "        # start_acc = (correct_start / len(true_start_index))\n",
        "\n",
        "        end_pred = torch.argmax(pred_end_index, dim=1)\n",
        "        all_end += end_pred\n",
        "        correct_end += (torch.sum(end_pred == true_end_index)).item()\n",
        "        # end_acc += (correct_end / len(true_end_index))\n",
        "\n",
        "        correct_both += (torch.sum((start_pred == true_start_index) & (end_pred == true_end_index))).item()\n",
        "\n",
        "        total += len(true_start_index)\n",
        "        step = step+1\n",
        "\n",
        "        # Move everything back to cpu\n",
        "        # document = document.to('cpu')\n",
        "        # question = question.to('cpu')\n",
        "        # true_start_index = true_start_index.to('cpu')\n",
        "        # true_end_index = true_end_index.to('cpu')\n",
        "        # unanswerable = unanswerable.to('cpu')\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "        \n",
        "        output_dict = {'document': all_doc_text,\\\n",
        "                      'attention': attention_scores,\\\n",
        "                       'true_answer': all_true_text,\\\n",
        "                       'predicted_answer': all_pred_text,\\\n",
        "                       'accuracy': correct_both/total}\n",
        "        all_output.append(output_dict)\n",
        "    \n",
        "    print(\"Saving outputs...\")\n",
        "    output_df = pd.DataFrame(all_output)\n",
        "    output_df.to_csv('/content/drive/Shared drives/CIS 700-1 Final Project/models/' + expt_name + '.csv')\n",
        "    # print(\"VALIDATION --> Start Acc: %s End Acc: %s Acc: %s F1: %s EM: %s\" %((correct_start*100)/total, (correct_end*100)/total, (correct_both*100)/total, (total_f1 * 100)step, (total_em * 100)/step))\n",
        "    \n",
        "  def evaluate(self, test_iterator, expt_name = \"default\", load_model=False):\n",
        "    if load_model:\n",
        "      print(\"Loading model...\")\n",
        "      checkpoint = torch.load('/content/drive/Shared drives/CIS 700-1 Final Project/models/' + expt_name + '.pt')\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    evaluation_parameters = {'beta1': self.hyperparameters['beta1'],\\\n",
        "                             'beta2': self.hyperparameters['beta2'],\\\n",
        "                             'lambda1': self.hyperparameters['lambda1'],\\\n",
        "                             'lambda2': self.hyperparameters['lambda2'],\\\n",
        "                             'delta': self.hyperparameters['delta']}\n",
        "    self.evaluate_classifier(self.model,\\\n",
        "                     test_iterator,\\\n",
        "                     evaluation_parameters,\\\n",
        "                     self.hyperparameters['vocab_itos'],\\\n",
        "                     expt_name)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seYVJijgW19f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKmslpfpV1WM",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP3vQKRIcDOs",
        "colab_type": "text"
      },
      "source": [
        "## Expt 1 - Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMexz321hux8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = \"/content/drive/Shared\\ drives/CIS\\ 700-1\\ Final\\ Project/runs\"\n",
        "%tensorboard --logdir /content/drive/Shared\\ drives/CIS\\ 700-1\\ Final\\ Project/runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYmzaxImV3Pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters1 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 2,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': False,\\\n",
        "                   'num_epochs': 3,\\\n",
        "                   'learning_rate': 1e-2,\\\n",
        "                   'loss_function': retrospective_parallel_loss,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt1 = Experiment(hyperparameters1, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt1_glove_5')\n",
        "trained_model_glove = expt1.train(train_iterator)\n",
        "expt1.evaluate(val_iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ2iQj9DcH3O",
        "colab_type": "text"
      },
      "source": [
        "## Expt 2 - BERT embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8kQOkpl4MTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel, AlbertModel\n",
        "bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_encoder = bert_encoder.to(device)\n",
        "# bert_encoder = AlbertModel.from_pretrained('albert-base-v2')\n",
        "# bert_encoder = bert_encoder.to(device)\n",
        "\n",
        "hyperparameters2 = {'vocab_size': len(tokenizer.vocab),\\\n",
        "                   'embedding_length': bert_encoder.config.to_dict()['hidden_size'],\\\n",
        "                   'word_embeddings': None,\\\n",
        "                   'vocab_itos':  None,\\\n",
        "                   'hidden_size': bert_encoder.config.to_dict()['hidden_size'],\\\n",
        "                   'num_passes': 3,\\\n",
        "                   'sketchy_mode': 'bert',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': False,\\\n",
        "                   'num_epochs': 3,\\\n",
        "                   'learning_rate': 1e-4,\\\n",
        "                   'loss_function': retrospective_parallel_loss,\\\n",
        "                   'bert_encoder': bert_encoder,\\\n",
        "                   'bert_tokenizer': tokenizer,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0.5}\n",
        "\n",
        "expt2 = Experiment(hyperparameters2, print_every = 20)\n",
        "trained_model_bert = expt2.train(train_iterator)\n",
        "expt2.evaluate(val_iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKAI4DSqrtOn",
        "colab_type": "text"
      },
      "source": [
        "## Expt 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml-IQh6IsBAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /content/drive/Shared\\ drives/CIS\\ 700-1\\ Final\\ Project/runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S32pyf58rtWJ",
        "colab_type": "code",
        "outputId": "281cb200-4766-4452-ee3e-18075f332a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        }
      },
      "source": [
        "hyperparameters3 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 2,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': False,\\\n",
        "                   'num_epochs': 3,\\\n",
        "                   'learning_rate': 1e-2,\\\n",
        "                   'loss_function': retrospective_parallel_loss,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt3 = Experiment(hyperparameters3, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt3_glove2')\n",
        "trained_model_glove = expt3.train(train_iterator)\n",
        "expt3.evaluate(val_iterator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Step: 0 Ext Loss: 0.6964178681373596 Final Loss: 3.656561851501465 Start Acc: 0.0 End Acc: 0.0 Acc: 0.0 F1: 3.3196556173026304 EM: 3.125\n",
            "--- Step: 100 Ext Loss: 0.6931795654910626 Final Loss: 3.498922921643399 Start Acc: 1.8873762376237624 End Acc: 1.021039603960396 Acc: 0.09282178217821782 F1: 2.546528802970907 EM: 5.538366336633663\n",
            "--- Step: 200 Ext Loss: 0.6931634545326233 Final Loss: 3.436397770744058 Start Acc: 1.3215174129353233 End Acc: 0.792910447761194 Acc: 0.07773631840796019 F1: 2.517580535961532 EM: 4.912935323383085\n",
            "--- Step: 300 Ext Loss: 0.6931580485299577 Final Loss: 3.4063127991369 Start Acc: 1.173172757475083 End Acc: 0.8305647840531561 Acc: 0.07267441860465117 F1: 2.5262184987421823 EM: 4.900332225913621\n",
            "--- Step: 400 Ext Loss: 0.6931553387879731 Final Loss: 3.385046112269832 Start Acc: 1.0910224438902743 End Acc: 0.8338528678304239 Acc: 0.07793017456359103 F1: 2.5535902238985027 EM: 4.800498753117207\n",
            "--- Step: 500 Ext Loss: 0.6931537107793156 Final Loss: 3.3730135092478313 Start Acc: 1.0416666666666667 End Acc: 0.8420658682634731 Acc: 0.08732534930139721 F1: 2.527678314155097 EM: 4.497255489021956\n",
            "--- Step: 600 Ext Loss: 0.6931526245372664 Final Loss: 3.365027539940325 Start Acc: 1.0035357737104826 End Acc: 0.8215474209650583 Acc: 0.08839434276206323 F1: 2.530270201899066 EM: 4.196131447587354\n",
            "--- Step: 700 Ext Loss: 0.6931518482073568 Final Loss: 3.358778324684981 Start Acc: 1.0208630527817404 End Acc: 0.842546362339515 Acc: 0.09807417974322397 F1: 2.563018709028457 EM: 3.9630884450784594\n",
            "--- Step: 800 Ext Loss: 0.6931512657176243 Final Loss: 3.355286625589473 Start Acc: 1.131398252184769 End Acc: 0.8973158551810237 Acc: 0.1404494382022472 F1: 2.578392134930151 EM: 3.8038389513108615\n",
            "--- Step: 900 Ext Loss: 0.6931508125263895 Final Loss: 3.354172859022541 Start Acc: 1.2590177580466149 End Acc: 0.9433962264150944 Acc: 0.17688679245283018 F1: 2.6243955660315312 EM: 3.7215593784683683\n",
            "--- Step: 1000 Ext Loss: 0.6931504498828541 Final Loss: 3.351493104949936 Start Acc: 1.3455294705294705 End Acc: 0.999000999000999 Acc: 0.19667832167832167 F1: 2.584994150842213 EM: 3.743131868131868\n",
            "--- Step: 1100 Ext Loss: 0.6931501531146202 Final Loss: 3.3481156652781445 Start Acc: 1.4560626702997275 End Acc: 1.141008174386921 Acc: 0.22706630336058128 F1: 2.5248394218829886 EM: 3.85161217075386\n",
            "--- Step: 1200 Ext Loss: 0.6931499057665753 Final Loss: 3.3460463155417717 Start Acc: 1.5507910074937552 End Acc: 1.2411532056619483 Acc: 0.26280183180682765 F1: 2.4725900158332723 EM: 3.991465445462115\n",
            "--- Step: 1300 Ext Loss: 0.6931496964428262 Final Loss: 3.3441824392572355 Start Acc: 1.6141429669485012 End Acc: 1.2898731744811682 Acc: 0.2834358186010761 F1: 2.422622924786496 EM: 4.07619139123751\n",
            "--- Step: 1400 Ext Loss: 0.6931495170011255 Final Loss: 3.341734973470455 Start Acc: 1.7130620985010707 End Acc: 1.3650963597430408 Acc: 0.32119914346895073 F1: 2.406333439833849 EM: 4.146591720199857\n",
            "--- Step: 1500 Ext Loss: 0.6931493614690452 Final Loss: 3.340595945686122 Start Acc: 1.7717355096602265 End Acc: 1.4261325782811458 Acc: 0.3435209860093271 F1: 2.3749228314669772 EM: 4.19095602931379\n",
            "--- Step: 1600 Ext Loss: 0.6931492253663315 Final Loss: 3.339169938440698 Start Acc: 1.8269831355402872 End Acc: 1.4580730793254215 Acc: 0.3591505309181761 F1: 2.348907086800268 EM: 4.286383510306059\n",
            "--- Step: 1700 Ext Loss: 0.6931491052662885 Final Loss: 3.3369148013032515 Start Acc: 1.8830834803057026 End Acc: 1.4825837742504409 Acc: 0.36926807760141095 F1: 2.3267843682859612 EM: 4.313639035861258\n",
            "--- Step: 1800 Ext Loss: 0.6931489985032852 Final Loss: 3.334963581020603 Start Acc: 1.953775680177679 End Acc: 1.5078428650749585 Acc: 0.3921432537479178 F1: 2.31854135591725 EM: 4.3517490283176015\n",
            "--- Step: 1900 Ext Loss: 0.693148902972581 Final Loss: 3.3350083007742266 Start Acc: 1.9512756443976855 End Acc: 1.520581273014203 Acc: 0.3994608100999474 F1: 2.2813003744335227 EM: 4.4022882693319305\n",
            "--- Step: 2000 Ext Loss: 0.6931488169901732 Final Loss: 3.333831695602394 Start Acc: 1.9708895552223888 End Acc: 1.5648425787106446 Acc: 0.4185407296351824 F1: 2.2648165604952504 EM: 4.4305972013993005\n",
            "--- Step: 2100 Ext Loss: 0.6931487391926685 Final Loss: 3.333362441696138 Start Acc: 1.9945859114707283 End Acc: 1.5721680152308424 Acc: 0.4328296049500238 F1: 2.2544918915649927 EM: 4.477034745359353\n",
            "--- Step: 2200 Ext Loss: 0.6931486684644509 Final Loss: 3.3320726496910087 Start Acc: 2.0104497955474785 End Acc: 1.5873466606088142 Acc: 0.4373012267151295 F1: 2.235343707538449 EM: 4.520672421626533\n",
            "--- Step: 2300 Ext Loss: 0.6931486038838403 Final Loss: 3.331149662748102 Start Acc: 2.0371577574967406 End Acc: 1.6242937853107344 Acc: 0.4495328118209474 F1: 2.2277397092558036 EM: 4.548294219904389\n",
            "--- Step: 2400 Ext Loss: 0.6931485446827057 Final Loss: 3.3311999622656376 Start Acc: 2.0538317367763432 End Acc: 1.6217201166180757 Acc: 0.4516347355268638 F1: 2.2094727724356136 EM: 4.552790503956684\n",
            "--- Step: 2500 Ext Loss: 0.6931484902157683 Final Loss: 3.3310793069590097 Start Acc: 2.0679228308676527 End Acc: 1.6455917632946822 Acc: 0.4610655737704918 F1: 2.1929283280387466 EM: 4.560675729708117\n",
            "Epoch: 1 External Loss: 0.6931484681114836 Final Loss: 3.331024226808698 Start Acc: 2.0655165634522756 End Acc: 1.6452865428093975 Acc: 0.45832104590582917 F1: 2.180529712860033 EM: 4.558520047169812\n",
            "--- Step: 2600 Ext Loss: 0.6931471824645996 Final Loss: 3.3185835846683434 Start Acc: 1.9736842105263157 End Acc: 1.918859649122807 Acc: 0.38377192982456143 F1: 1.3195711812148054 EM: 4.660087719298246\n",
            "--- Step: 2700 Ext Loss: 0.6931471824645996 Final Loss: 3.3125863880108875 Start Acc: 2.5676751592356686 End Acc: 2.3487261146496814 Acc: 0.5772292993630573 F1: 1.8642386943169378 EM: 5.234872611464968\n",
            "--- Step: 2800 Ext Loss: 0.6931471824645996 Final Loss: 3.313198091455007 Start Acc: 2.5413424124513617 End Acc: 2.237354085603113 Acc: 0.5714980544747081 F1: 1.8693226198289965 EM: 5.289396887159533\n",
            "--- Step: 2900 Ext Loss: 0.6931471824645996 Final Loss: 3.303938932445537 Start Acc: 2.4072128851540615 End Acc: 2.162114845938375 Acc: 0.5952380952380952 F1: 1.8278881687013717 EM: 5.304621848739496\n",
            "--- Step: 3000 Ext Loss: 0.6931471824645996 Final Loss: 3.312648496690412 Start Acc: 2.3454595185995624 End Acc: 2.037746170678337 Acc: 0.588074398249453 F1: 1.7995229855591557 EM: 5.237964989059081\n",
            "--- Step: 3100 Ext Loss: 0.6931471824645996 Final Loss: 3.310473884671552 Start Acc: 2.43491921005386 End Acc: 2.0926840215439855 Acc: 0.6227558348294434 F1: 1.8375841839935672 EM: 5.458931777378815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-714dfd873e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt3_glove2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrained_model_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mexpt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-266fcafc6761>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_iterator)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_itos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-266fcafc6761>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(self, model, dataset_iterator, loss_function, optimizer, vocab_itos, num_epochs, log, verbose, print_every)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mpred_start_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_end_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ext_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_int_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# print(pred_start_index, pred_end_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# print(true_start_index, true_end_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2d11f8f8fac4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, document, doc_lengths, question, question_lengths)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mexternal_verifier_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_verifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDPrime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQPrime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0minternal_verifier_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_verifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a07f37c87194>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, D, Q, question_lengths)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpasses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_passes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-869231b89e3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, D, Q, prev_mem)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# prev_mem = prev_mem.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttnGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Now considering prev_mem, C and question, we will update the memory state as follows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-44f0c2c96778>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, D, G)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msen\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Initialization for first sentence only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mhi_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0mhi_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttnGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-44f0c2c96778>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fact, hi_1, g)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# g.size() = (batch_size, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mr_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mh_tilda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr_i\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-CGgONTL7I8",
        "colab_type": "text"
      },
      "source": [
        "## Expt 4 - Glove + Skip Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX4Bk8e_L7PL",
        "colab_type": "code",
        "outputId": "4f20550d-a0e8-44e8-d88d-c36aed90dc37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hyperparameters4 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 2,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': True,\\\n",
        "                   'num_epochs': 10,\\\n",
        "                   'learning_rate': 1e-4,\\\n",
        "                   'loss_function': retrospective_parallel_loss,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt4 = Experiment(hyperparameters4, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt4_glove3')\n",
        "trained_model_glove = expt4.train(train_iterator)\n",
        "expt4.evaluate(val_iterator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Step: 0 Ext Loss: 0.6937507390975952 Final Loss: 3.936500072479248 Start Acc: 0.0 End Acc: 3.125 Acc: 0.0 F1: 0.178216211108942 EM: 12.5\n",
            "--- Step: 100 Ext Loss: 0.6932281934388793 Final Loss: 3.5481256376398673 Start Acc: 0.8044554455445545 End Acc: 0.5878712871287128 Acc: 0.03094059405940594 F1: 1.264611654507865 EM: 9.993811881188119\n",
            "--- Step: 200 Ext Loss: 0.693187889471576 Final Loss: 3.501665121287256 Start Acc: 0.9172885572139303 End Acc: 0.6840796019900498 Acc: 0.07773631840796019 F1: 1.3063344971131807 EM: 8.53544776119403\n",
            "--- Step: 300 Ext Loss: 0.6931743655489925 Final Loss: 3.4648907794508825 Start Acc: 1.1524086378737541 End Acc: 0.7994186046511628 Acc: 0.1972591362126246 F1: 1.4795119880420928 EM: 7.879983388704319\n",
            "--- Step: 400 Ext Loss: 0.6931675867249544 Final Loss: 3.435728432828946 Start Acc: 1.4027431421446384 End Acc: 0.8104738154613467 Acc: 0.24937655860349128 F1: 1.6025694419016165 EM: 7.769638403990025\n",
            "--- Step: 500 Ext Loss: 0.6931635140182967 Final Loss: 3.413910558361731 Start Acc: 1.5219560878243512 End Acc: 0.8483033932135728 Acc: 0.2682135728542914 F1: 1.6006758475215606 EM: 7.996506986027944\n",
            "--- Step: 600 Ext Loss: 0.693160796621675 Final Loss: 3.393419978225886 Start Acc: 1.533901830282862 End Acc: 0.9255407653910149 Acc: 0.30158069883527455 F1: 1.652051467620985 EM: 8.132279534109816\n",
            "--- Step: 700 Ext Loss: 0.6931588545165286 Final Loss: 3.3735922304607833 Start Acc: 1.649429386590585 End Acc: 0.9629101283880172 Acc: 0.3209700427960057 F1: 1.7425783030436401 EM: 8.260520684736091\n",
            "--- Step: 800 Ext Loss: 0.6931573973315188 Final Loss: 3.3617940502666803 Start Acc: 1.6931960049937578 End Acc: 0.9909488139825219 Acc: 0.3316167290886392 F1: 1.795691589764594 EM: 8.438670411985019\n",
            "--- Step: 900 Ext Loss: 0.6931562636060006 Final Loss: 3.353865490372517 Start Acc: 1.716842397336293 End Acc: 1.0127635960044394 Acc: 0.34683684794672587 F1: 1.8363125564072758 EM: 8.51484461709212\n",
            "--- Step: 1000 Ext Loss: 0.6931553563990674 Final Loss: 3.3430037350802273 Start Acc: 1.820054945054945 End Acc: 1.027097902097902 Acc: 0.3371628371628372 F1: 1.846639981730703 EM: 8.51023976023976\n",
            "--- Step: 1100 Ext Loss: 0.693154613989034 Final Loss: 3.335274725367436 Start Acc: 1.887488646684832 End Acc: 1.0757266121707538 Acc: 0.351952770208901 F1: 1.8903973361698623 EM: 8.526339691189827\n",
            "--- Step: 1200 Ext Loss: 0.6931539952109795 Final Loss: 3.328297292263085 Start Acc: 1.896856786011657 End Acc: 1.108451290591174 Acc: 0.3590757701915071 F1: 1.9316606521595918 EM: 8.599604496253122\n",
            "--- Step: 1300 Ext Loss: 0.6931534715563769 Final Loss: 3.319728927187146 Start Acc: 1.9023827824750192 End Acc: 1.1553612605687933 Acc: 0.3699077632590315 F1: 1.954294910218208 EM: 8.651998462720984\n",
            "--- Step: 1400 Ext Loss: 0.6931530226561786 Final Loss: 3.3145873647005026 Start Acc: 1.9405781584582442 End Acc: 1.1754996431120628 Acc: 0.38588508208422556 F1: 1.983752804509544 EM: 8.77498215560314\n",
            "--- Step: 1500 Ext Loss: 0.6931526335694645 Final Loss: 3.3090043066343733 Start Acc: 2.0069953364423716 End Acc: 1.192954696868754 Acc: 0.39140572951365754 F1: 2.0292069596729476 EM: 8.817038640906063\n",
            "--- Step: 1600 Ext Loss: 0.6931522930882112 Final Loss: 3.304312842105196 Start Acc: 2.0241255465334165 End Acc: 1.2101811367895066 Acc: 0.3942848219862586 F1: 2.031749754140753 EM: 8.894831355402873\n",
            "--- Step: 1700 Ext Loss: 0.6931519926400271 Final Loss: 3.2992839806224232 Start Acc: 2.0465902410346857 End Acc: 1.2235449735449735 Acc: 0.393151087595532 F1: 2.054482147545997 EM: 8.954291593180482\n",
            "--- Step: 1800 Ext Loss: 0.6931517255564388 Final Loss: 3.2936868777478954 Start Acc: 2.0839117157134925 End Acc: 1.252776235424764 Acc: 0.4025541365907829 F1: 2.077850861944826 EM: 9.003678511937812\n",
            "--- Step: 1900 Ext Loss: 0.6931514865721231 Final Loss: 3.288247033334418 Start Acc: 2.120594423987375 End Acc: 1.3118095739084692 Acc: 0.41754339821146763 F1: 2.1186931498928683 EM: 9.069239873750657\n",
            "--- Step: 2000 Ext Loss: 0.6931512714742959 Final Loss: 3.2840550267535527 Start Acc: 2.1536106946526736 End Acc: 1.330584707646177 Acc: 0.42634932533733133 F1: 2.1369563803387033 EM: 9.104822588705646\n",
            "--- Step: 2100 Ext Loss: 0.6931510768522257 Final Loss: 3.2805452974338523 Start Acc: 2.16563541170871 End Acc: 1.3460851975249881 Acc: 0.43729176582579726 F1: 2.1664004521021956 EM: 9.159328891004284\n",
            "--- Step: 2200 Ext Loss: 0.6931508999150322 Final Loss: 3.276945214767664 Start Acc: 2.1808268968650615 End Acc: 1.3516583371194912 Acc: 0.44014084507042256 F1: 2.182402458801285 EM: 9.176226715129486\n",
            "--- Step: 2300 Ext Loss: 0.6931507383569951 Final Loss: 3.273239171385817 Start Acc: 2.225934376358105 End Acc: 1.3635375923511517 Acc: 0.4413841807909605 F1: 2.2033985758775523 EM: 9.188939591481965\n",
            "--- Step: 2400 Ext Loss: 0.6931505902565206 Final Loss: 3.2699383053865 Start Acc: 2.2659829237817575 End Acc: 1.365316534777176 Acc: 0.44252394835485215 F1: 2.2245096753909235 EM: 9.197990420658058\n",
            "--- Step: 2500 Ext Loss: 0.6931504539993466 Final Loss: 3.266515015221176 Start Acc: 2.2903338664534187 End Acc: 1.3781987205117954 Acc: 0.4398240703718513 F1: 2.229349947464441 EM: 9.211315473810476\n",
            "Epoch: 1 External Loss: 0.6931503612168927 Final Loss: 3.264710099639685 Start Acc: 2.3218817929007733 End Acc: 1.3795280945268193 Acc: 0.439603142798159 F1: 2.2335570147036807 EM: 9.224345730027547\n",
            "--- Step: 2600 Ext Loss: 0.6931471824645996 Final Loss: 3.2417987011097096 Start Acc: 2.5462962962962963 End Acc: 1.9675925925925926 Acc: 0.46296296296296297 F1: 2.719043113273043 EM: 9.953703703703704\n",
            "--- Step: 2700 Ext Loss: 0.6931471824645996 Final Loss: 3.188914274606179 Start Acc: 2.5836614173228347 End Acc: 1.8700787401574803 Acc: 0.3937007874015748 F1: 2.3372483310313883 EM: 9.916338582677165\n",
            "--- Step: 2800 Ext Loss: 0.6931471824645996 Final Loss: 3.1776362442235064 Start Acc: 2.767070484581498 End Acc: 1.8860132158590308 Acc: 0.44052863436123346 F1: 2.4101582234719587 EM: 9.691629955947137\n",
            "--- Step: 2900 Ext Loss: 0.6931471824645996 Final Loss: 3.1795683825781587 Start Acc: 2.86697247706422 End Acc: 1.863532110091743 Acc: 0.46827217125382264 F1: 2.426219099500428 EM: 9.919724770642201\n",
            "--- Step: 3000 Ext Loss: 0.6931471824645996 Final Loss: 3.172050420796843 Start Acc: 2.9127634660421546 End Acc: 1.9247658079625294 Acc: 0.49033957845433257 F1: 2.540369681598833 EM: 10.092213114754099\n",
            "--- Step: 3100 Ext Loss: 0.6931471824645996 Final Loss: 3.1702169050980338 Start Acc: 3.030123339658444 End Acc: 1.9153225806451613 Acc: 0.5336812144212524 F1: 2.6746648073678263 EM: 9.944259962049335\n",
            "--- Step: 3200 Ext Loss: 0.6931471824645996 Final Loss: 3.169459201122205 Start Acc: 3.0003987240829346 End Acc: 1.9138755980861244 Acc: 0.5582137161084529 F1: 2.640938442044081 EM: 9.85346889952153\n",
            "--- Step: 3300 Ext Loss: 0.6931471824645996 Final Loss: 3.1675037812989877 Start Acc: 3.0390302613480054 End Acc: 1.955811554332875 Acc: 0.563101788170564 F1: 2.681335132144113 EM: 9.804848693259972\n",
            "--- Step: 3400 Ext Loss: 0.6931471824645996 Final Loss: 3.1639249275788717 Start Acc: 3.034310761789601 End Acc: 1.961154776299879 Acc: 0.5554715840386941 F1: 2.6803402177049915 EM: 9.798216444981863\n",
            "--- Step: 3500 Ext Loss: 0.6931471824645996 Final Loss: 3.1630837343785885 Start Acc: 3.040722761596548 End Acc: 1.941747572815534 Acc: 0.5528586839266451 F1: 2.675893081432433 EM: 9.782901833872709\n",
            "--- Step: 3600 Ext Loss: 0.6931471824645996 Final Loss: 3.163167706947364 Start Acc: 3.0398003894839336 End Acc: 1.9291626095423564 Acc: 0.5446689386562804 F1: 2.654187195540619 EM: 9.846640701071081\n",
            "--- Step: 3700 Ext Loss: 0.6931471824645996 Final Loss: 3.160019346328573 Start Acc: 3.0750887311446315 End Acc: 1.921583850931677 Acc: 0.5434782608695652 F1: 2.6670194215814123 EM: 9.85192990239574\n",
            "--- Step: 3800 Ext Loss: 0.6931471824645996 Final Loss: 3.1578304746234522 Start Acc: 3.0817033414832924 End Acc: 1.9330684596577017 Acc: 0.5424816625916871 F1: 2.667544678264386 EM: 9.83598207008965\n",
            "--- Step: 3900 Ext Loss: 0.6931471824645996 Final Loss: 3.157127863855944 Start Acc: 3.0614167294649586 End Acc: 1.9239826676714393 Acc: 0.5487000753579503 F1: 2.679237061140673 EM: 9.817727957799548\n",
            "--- Step: 4000 Ext Loss: 0.6931471824645996 Final Loss: 3.1559254912621673 Start Acc: 3.0790119131044147 End Acc: 1.9161702873160475 Acc: 0.549667133847232 F1: 2.654388447258614 EM: 9.85021023125438\n",
            "--- Step: 4100 Ext Loss: 0.6931471824645996 Final Loss: 3.155599241562739 Start Acc: 3.0554191224623444 End Acc: 1.9257531106745251 Acc: 0.5361820563195808 F1: 2.650026989149321 EM: 9.864112639161755\n",
            "--- Step: 4200 Ext Loss: 0.6931471824645996 Final Loss: 3.1540115147774106 Start Acc: 3.078902888752305 End Acc: 1.936078672403196 Acc: 0.53779963122311 F1: 2.6573268529499745 EM: 9.849416103257528\n",
            "--- Step: 4300 Ext Loss: 0.6931471824645996 Final Loss: 3.1540854347187035 Start Acc: 3.05804863925883 End Acc: 1.9289229878401852 Acc: 0.5301823972206138 F1: 2.643528641019087 EM: 9.820136074116967\n",
            "--- Step: 4400 Ext Loss: 0.6931471824645996 Final Loss: 3.152310217412104 Start Acc: 3.0753968253968256 End Acc: 1.948207443897099 Acc: 0.5524767378215654 F1: 2.6666034637075575 EM: 9.896688560481664\n",
            "--- Step: 4500 Ext Loss: 0.6931471824645996 Final Loss: 3.1507376065573425 Start Acc: 3.095809548521017 End Acc: 1.9541385573430203 Acc: 0.5708354955889985 F1: 2.6928669424074902 EM: 9.978269330565647\n",
            "--- Step: 4600 Ext Loss: 0.6931471824645996 Final Loss: 3.149254792302362 Start Acc: 3.1080414405525407 End Acc: 1.9795263936852492 Acc: 0.5796743956586088 F1: 2.7069508934617135 EM: 10.00400838677849\n",
            "--- Step: 4700 Ext Loss: 0.6931471824645996 Final Loss: 3.1476080775204878 Start Acc: 3.1440996708979783 End Acc: 1.9819581570286788 Acc: 0.5803361542078044 F1: 2.707743218511749 EM: 10.021450399623884\n",
            "--- Step: 4800 Ext Loss: 0.6931471824645996 Final Loss: 3.1466528941121505 Start Acc: 3.160080826223619 End Acc: 1.984171531207903 Acc: 0.5907611136057477 F1: 2.7168859891592216 EM: 10.04293893129771\n",
            "--- Step: 4900 Ext Loss: 0.6931471824645996 Final Loss: 3.14445314808982 Start Acc: 3.182746024924796 End Acc: 2.004995702621401 Acc: 0.6070047271164589 F1: 2.736811078754793 EM: 10.04109368285346\n",
            "--- Step: 5000 Ext Loss: 0.6931471824645996 Final Loss: 3.143267869556397 Start Acc: 3.194530284301607 End Acc: 2.011227853316852 Acc: 0.6154717758549649 F1: 2.74619519892177 EM: 10.060002060156572\n",
            "--- Step: 5100 Ext Loss: 0.6931471824645996 Final Loss: 3.1422761317812786 Start Acc: 3.1930154333201424 End Acc: 2.0095468935496634 Acc: 0.6183221210922042 F1: 2.734549530440683 EM: 10.074940641076376\n",
            "Epoch: 2 External Loss: 0.6931471824645996 Final Loss: 3.1425683660477564 Start Acc: 3.175586238721508 End Acc: 2.013430968948474 Acc: 0.6169016479045988 F1: 2.725701985634556 EM: 10.087545030726849\n",
            "--- Step: 5200 Ext Loss: 0.6931471824645996 Final Loss: 3.1012445620770723 Start Acc: 3.5377358490566038 End Acc: 1.5919811320754718 Acc: 0.5306603773584906 F1: 2.9552017095219107 EM: 9.375\n",
            "--- Step: 5300 Ext Loss: 0.6931471824645996 Final Loss: 3.1029989485647165 Start Acc: 3.5743464052287583 End Acc: 2.144607843137255 Acc: 0.6331699346405228 F1: 3.01535317426015 EM: 9.517973856209151\n",
            "--- Step: 5400 Ext Loss: 0.6931471824645996 Final Loss: 3.113913527590484 Start Acc: 3.3102766798418974 End Acc: 1.9515810276679841 Acc: 0.6299407114624506 F1: 2.921135459309101 EM: 10.004940711462451\n",
            "--- Step: 5500 Ext Loss: 0.6931471824645996 Final Loss: 3.1163972822194737 Start Acc: 3.2754957507082154 End Acc: 2.071529745042493 Acc: 0.6108356940509915 F1: 2.8492216267516897 EM: 9.897308781869688\n",
            "--- Step: 5600 Ext Loss: 0.6931471824645996 Final Loss: 3.1147062499528424 Start Acc: 3.3457505518763795 End Acc: 2.0212472406181017 Acc: 0.6691501103752759 F1: 2.90727774185385 EM: 9.926876379690949\n",
            "--- Step: 5700 Ext Loss: 0.6931471824645996 Final Loss: 3.1122695308912944 Start Acc: 3.373643761301989 End Acc: 2.04000904159132 Acc: 0.6159584086799277 F1: 2.8696277853851018 EM: 9.957052441229656\n",
            "--- Step: 5800 Ext Loss: 0.6931471824645996 Final Loss: 3.1083888094786665 Start Acc: 3.388208269525268 End Acc: 2.1248085758039816 Acc: 0.6364854517611026 F1: 2.9135070935017637 EM: 10.073698315467075\n",
            "--- Step: 5900 Ext Loss: 0.6931471824645996 Final Loss: 3.105902915298543 Start Acc: 3.4570053120849935 End Acc: 2.1455843293492696 Acc: 0.6723107569721115 F1: 2.9418487747828985 EM: 10.167662682602922\n",
            "--- Step: 6000 Ext Loss: 0.6931471824645996 Final Loss: 3.1070480855380525 Start Acc: 3.527989449003517 End Acc: 2.1468347010550994 Acc: 0.6557737397420867 F1: 2.9403437262702368 EM: 10.20662368112544\n",
            "--- Step: 6100 Ext Loss: 0.6931471824645996 Final Loss: 3.106153854918505 Start Acc: 3.587355718782791 End Acc: 2.1478226652675763 Acc: 0.6853357817418678 F1: 2.9582480527339383 EM: 10.237408184679959\n",
            "--- Step: 6200 Ext Loss: 0.6931471824645996 Final Loss: 3.1038144261975122 Start Acc: 3.5998338081671415 End Acc: 2.157526115859449 Acc: 0.6766381766381766 F1: 2.9589916451768423 EM: 10.363247863247864\n",
            "--- Step: 6300 Ext Loss: 0.6931471824645996 Final Loss: 3.1049957600036913 Start Acc: 3.534258456201214 End Acc: 2.1438638334778837 Acc: 0.6667389418907199 F1: 2.9573817066345103 EM: 10.450997398091934\n",
            "--- Step: 6400 Ext Loss: 0.6931471824645996 Final Loss: 3.1028251145614782 Start Acc: 3.526536312849162 End Acc: 2.144852354349561 Acc: 0.6808659217877095 F1: 2.9443098548047097 EM: 10.449920191540304\n",
            "--- Step: 6500 Ext Loss: 0.6931471824645996 Final Loss: 3.1016175438541884 Start Acc: 3.5268847006651884 End Acc: 2.1526237989652626 Acc: 0.6813562453806357 F1: 2.9613282865671935 EM: 10.4490022172949\n",
            "--- Step: 6600 Ext Loss: 0.6931471824645996 Final Loss: 3.101379167762856 Start Acc: 3.527185134205093 End Acc: 2.1571748107364073 Acc: 0.6710254645560908 F1: 2.9595736504068597 EM: 10.43100481761872\n",
            "--- Step: 6700 Ext Loss: 0.6931471824645996 Final Loss: 3.101125335662655 Start Acc: 3.5254346426271734 End Acc: 2.1571152607855764 Acc: 0.6580006439150032 F1: 2.954835469717457 EM: 10.375080489375403\n",
            "--- Step: 6800 Ext Loss: 0.6931471824645996 Final Loss: 3.1008053560078106 Start Acc: 3.5238959467634605 End Acc: 2.16840592861464 Acc: 0.6578947368421053 F1: 2.965164812450951 EM: 10.37885662431942\n",
            "--- Step: 6900 Ext Loss: 0.6931471824645996 Final Loss: 3.098343935953979 Start Acc: 3.558185966913862 End Acc: 2.205148317170565 Acc: 0.672062179121506 F1: 2.9772826000428174 EM: 10.35189674843126\n",
            "--- Step: 7000 Ext Loss: 0.6931471824645996 Final Loss: 3.0995365286671146 Start Acc: 3.5432406907717215 End Acc: 2.2075688073394497 Acc: 0.6695223961144091 F1: 2.9837337149641576 EM: 10.410483000539665\n",
            "--- Step: 7100 Ext Loss: 0.6931471824645996 Final Loss: 3.099650081950948 Start Acc: 3.533026113671275 End Acc: 2.1985407066052227 Acc: 0.664042498719918 F1: 2.9705768681275466 EM: 10.431067588325654\n",
            "--- Step: 7200 Ext Loss: 0.6931471824645996 Final Loss: 3.098639848461049 Start Acc: 3.5405504140282513 End Acc: 2.202569410618607 Acc: 0.6560521188504628 F1: 2.9561195731331984 EM: 10.429858743302484\n",
            "--- Step: 7300 Ext Loss: 0.6931471824645996 Final Loss: 3.0984758717261855 Start Acc: 3.5502786809103575 End Acc: 2.2047724105898747 Acc: 0.6633186251741756 F1: 2.95918952401055 EM: 10.440373896888063\n",
            "--- Step: 7400 Ext Loss: 0.6931471824645996 Final Loss: 3.098053139789444 Start Acc: 3.548047048379938 End Acc: 2.224811362627608 Acc: 0.6754882379050156 F1: 2.9657644538563166 EM: 10.466600088770528\n",
            "--- Step: 7500 Ext Loss: 0.6931471824645996 Final Loss: 3.0977379065901482 Start Acc: 3.542020824479388 End Acc: 2.223225669358266 Acc: 0.6680301742456438 F1: 2.9503189556725613 EM: 10.507862303442414\n",
            "--- Step: 7600 Ext Loss: 0.6931471824645996 Final Loss: 3.096476950258612 Start Acc: 3.554321239298818 End Acc: 2.22431716265797 Acc: 0.6624541377904607 F1: 2.964126071231955 EM: 10.488432531593967\n",
            "--- Step: 7700 Ext Loss: 0.6931471824645996 Final Loss: 3.0964137691635267 Start Acc: 3.5555555555555554 End Acc: 2.2112029384756657 Acc: 0.6636057545148454 F1: 2.9644637861310814 EM: 10.498566748566748\n",
            "Epoch: 3 External Loss: 0.6931471824645996 Final Loss: 3.095903954213223 Start Acc: 3.5556850887099714 End Acc: 2.215016940507851 Acc: 0.6654765808104728 F1: 2.966135347317688 EM: 10.484543865225683\n",
            "--- Step: 7800 Ext Loss: 0.6931471824645996 Final Loss: 3.103623891178566 Start Acc: 3.045886075949367 End Acc: 2.2943037974683542 Acc: 0.7911392405063291 F1: 3.1177109530702434 EM: 11.155063291139241\n",
            "--- Step: 7900 Ext Loss: 0.6931471824645996 Final Loss: 3.075513063196363 Start Acc: 3.2297486033519553 End Acc: 2.4615921787709496 Acc: 0.715782122905028 F1: 3.2445658099508754 EM: 10.824022346368714\n",
            "--- Step: 8000 Ext Loss: 0.6931471824645996 Final Loss: 3.0621977724054807 Start Acc: 3.4946236559139785 End Acc: 2.408154121863799 Acc: 0.7056451612903226 F1: 3.0585007531064106 EM: 11.021505376344086\n",
            "--- Step: 8100 Ext Loss: 0.6931471824645996 Final Loss: 3.06899458877644 Start Acc: 3.4465699208443272 End Acc: 2.37467018469657 Acc: 0.6843667546174143 F1: 2.9903320973215917 EM: 10.653034300791557\n",
            "--- Step: 8200 Ext Loss: 0.6931471824645996 Final Loss: 3.067467705939657 Start Acc: 3.549060542797495 End Acc: 2.36821503131524 Acc: 0.7306889352818372 F1: 3.0959823550521652 EM: 10.725469728601253\n",
            "--- Step: 8300 Ext Loss: 0.6931471824645996 Final Loss: 3.0666382543369486 Start Acc: 3.589162348877375 End Acc: 2.3639896373056994 Acc: 0.7502158894645942 F1: 3.104561827041617 EM: 10.816062176165802\n",
            "--- Step: 8400 Ext Loss: 0.6931471824645996 Final Loss: 3.0692789758252466 Start Acc: 3.6496686303387333 End Acc: 2.388622974963181 Acc: 0.7916053019145802 F1: 3.1138982471637133 EM: 10.833946980854197\n",
            "--- Step: 8500 Ext Loss: 0.6931471824645996 Final Loss: 3.0697110195061974 Start Acc: 3.6384788189987165 End Acc: 2.3668164313222078 Acc: 0.7902759948652118 F1: 3.1259604319406225 EM: 10.951540436456996\n",
            "--- Step: 8600 Ext Loss: 0.6931471824645996 Final Loss: 3.0699685729268738 Start Acc: 3.6405005688282137 End Acc: 2.3570819112627985 Acc: 0.7892491467576792 F1: 3.1342620677485313 EM: 10.89306029579067\n",
            "--- Step: 8700 Ext Loss: 0.6931471824645996 Final Loss: 3.070555732978377 Start Acc: 3.6453013278855977 End Acc: 2.3493360572012256 Acc: 0.7884320735444331 F1: 3.1450102542279392 EM: 10.833758937691522\n",
            "--- Step: 8800 Ext Loss: 0.6931471824645996 Final Loss: 3.0713270618696806 Start Acc: 3.6231464318813718 End Acc: 2.3459221501390175 Acc: 0.7703892493049119 F1: 3.1462113105420517 EM: 10.71883688600556\n",
            "--- Step: 8900 Ext Loss: 0.6931471824645996 Final Loss: 3.068933099483008 Start Acc: 3.628604749787956 End Acc: 2.369592875318066 Acc: 0.7792620865139949 F1: 3.1636561464277264 EM: 10.761238337574216\n",
            "--- Step: 9000 Ext Loss: 0.6931471824645996 Final Loss: 3.0691217211096244 Start Acc: 3.635652853792025 End Acc: 2.3749022673964033 Acc: 0.7818608287724785 F1: 3.163853768674201 EM: 10.792122752150117\n",
            "--- Step: 9100 Ext Loss: 0.6931471824645996 Final Loss: 3.0697130942534847 Start Acc: 3.6280819434372735 End Acc: 2.340917331399565 Acc: 0.7772842639593909 F1: 3.1446603663419346 EM: 10.780003625815809\n",
            "--- Step: 9200 Ext Loss: 0.6931471824645996 Final Loss: 3.0697154731183054 Start Acc: 3.6173089925625423 End Acc: 2.3580121703853956 Acc: 0.7712136578769438 F1: 3.1383706484157736 EM: 10.739942528735632\n",
            "--- Step: 9300 Ext Loss: 0.6931471824645996 Final Loss: 3.0721874927100385 Start Acc: 3.6118587713742873 End Acc: 2.3531507283090565 Acc: 0.7718492716909436 F1: 3.1155664806174777 EM: 10.750474984167194\n",
            "--- Step: 9400 Ext Loss: 0.6931471824645996 Final Loss: 3.0701526127236454 Start Acc: 3.6517272185824896 End Acc: 2.367480643240024 Acc: 0.7779928528886242 F1: 3.1208098305041005 EM: 10.724389517569982\n",
            "--- Step: 9500 Ext Loss: 0.6931471824645996 Final Loss: 3.0687973134768285 Start Acc: 3.6677908937605395 End Acc: 2.37141652613828 Acc: 0.7764193367060146 F1: 3.1466696274730412 EM: 10.790823496346261\n",
            "--- Step: 9600 Ext Loss: 0.6931471824645996 Final Loss: 3.067677714584862 Start Acc: 3.7519957424161787 End Acc: 2.383249068653539 Acc: 0.7899813730707823 F1: 3.192746542771917 EM: 10.780335284725918\n",
            "--- Step: 9700 Ext Loss: 0.6931471824645996 Final Loss: 3.067075278916824 Start Acc: 3.77400202122284 End Acc: 2.411255684689237 Acc: 0.7958564931783729 F1: 3.2166202721815074 EM: 10.767748863062153\n",
            "--- Step: 9800 Ext Loss: 0.6931471824645996 Final Loss: 3.0678583449500865 Start Acc: 3.7743506493506493 End Acc: 2.386964886964887 Acc: 0.7801226551226551 F1: 3.1973866992054862 EM: 10.787938912938912\n",
            "--- Step: 9900 Ext Loss: 0.6931471824645996 Final Loss: 3.0666603683167066 Start Acc: 3.7904428636989445 End Acc: 2.390718219366682 Acc: 0.7844768242312987 F1: 3.1928154546467606 EM: 10.789066085360258\n",
            "--- Step: 10000 Ext Loss: 0.6931471824645996 Final Loss: 3.0667540420299564 Start Acc: 3.7735849056603774 End Acc: 2.407854322071084 Acc: 0.7829640193067134 F1: 3.2057692865116953 EM: 10.828488372093023\n",
            "--- Step: 10100 Ext Loss: 0.6931471824645996 Final Loss: 3.066596899277125 Start Acc: 3.7936107608238756 End Acc: 2.4104140395124003 Acc: 0.794714165615805 F1: 3.225629228544672 EM: 10.777900378310214\n",
            "--- Step: 10200 Ext Loss: 0.6931471824645996 Final Loss: 3.065692112492957 Start Acc: 3.803196853569988 End Acc: 2.432936668011295 Acc: 0.8118192819685357 F1: 3.2336057585579225 EM: 10.74904195240016\n",
            "Epoch: 4 External Loss: 0.6931471824645996 Final Loss: 3.066114670839436 Start Acc: 3.7997741265619878 End Acc: 2.4372472585522242 Acc: 0.8124157528507414 F1: 3.2303817873204492 EM: 10.77095253231617\n",
            "--- Step: 10300 Ext Loss: 0.6931471824645996 Final Loss: 2.935038661956787 Start Acc: 2.5 End Acc: 3.125 Acc: 1.25 F1: 3.5233603428868108 EM: 15.625\n",
            "--- Step: 10400 Ext Loss: 0.6931471824645996 Final Loss: 3.041061530794416 Start Acc: 4.0476190476190474 End Acc: 2.5297619047619047 Acc: 0.8333333333333334 F1: 3.45286152270884 EM: 10.892857142857142\n",
            "--- Step: 10500 Ext Loss: 0.6931471824645996 Final Loss: 3.0413030950034536 Start Acc: 3.856707317073171 End Acc: 2.591463414634146 Acc: 0.7164634146341463 F1: 3.3370472926578754 EM: 10.853658536585366\n",
            "--- Step: 10600 Ext Loss: 0.6931471824645996 Final Loss: 3.0492836201777225 Start Acc: 3.7807377049180326 End Acc: 2.5 Acc: 0.7581967213114754 F1: 3.336523467480936 EM: 10.850409836065573\n",
            "--- Step: 10700 Ext Loss: 0.6931471824645996 Final Loss: 3.0489697503454893 Start Acc: 3.8271604938271606 End Acc: 2.5385802469135803 Acc: 0.8179012345679012 F1: 3.4311087886564215 EM: 10.76388888888889\n",
            "--- Step: 10800 Ext Loss: 0.6931471824645996 Final Loss: 3.0473561702388348 Start Acc: 3.756188118811881 End Acc: 2.4752475247524752 Acc: 0.7920792079207921 F1: 3.3471816876784493 EM: 10.779702970297029\n",
            "--- Step: 10900 Ext Loss: 0.6931471824645996 Final Loss: 3.0482173746282406 Start Acc: 3.8274793388429753 End Acc: 2.489669421487603 Acc: 0.7541322314049587 F1: 3.3014757748054095 EM: 10.676652892561984\n",
            "--- Step: 11000 Ext Loss: 0.6931471824645996 Final Loss: 3.048862636512053 Start Acc: 3.8297872340425534 End Acc: 2.522163120567376 Acc: 0.7801418439716312 F1: 3.325281552909064 EM: 10.65159574468085\n",
            "--- Step: 11100 Ext Loss: 0.6931471824645996 Final Loss: 3.0471884185483 Start Acc: 3.8354037267080745 End Acc: 2.484472049689441 Acc: 0.764751552795031 F1: 3.303065309288302 EM: 10.679347826086957\n",
            "--- Step: 11200 Ext Loss: 0.6931471824645996 Final Loss: 3.048105665333363 Start Acc: 3.87085635359116 End Acc: 2.4447513812154695 Acc: 0.7665745856353591 F1: 3.2913063052418785 EM: 10.618093922651934\n",
            "--- Step: 11300 Ext Loss: 0.6931471824645996 Final Loss: 3.0487373126679986 Start Acc: 3.889925373134328 End Acc: 2.468905472636816 Acc: 0.777363184079602 F1: 3.2899653375673257 EM: 10.631218905472636\n",
            "--- Step: 11400 Ext Loss: 0.6931471824645996 Final Loss: 3.0483634905577786 Start Acc: 3.8687782805429864 End Acc: 2.463235294117647 Acc: 0.7720588235294118 F1: 3.2899332119363467 EM: 10.709841628959277\n",
            "--- Step: 11500 Ext Loss: 0.6931471824645996 Final Loss: 3.0486086873098035 Start Acc: 3.8148340248962658 End Acc: 2.5233402489626555 Acc: 0.7728215767634855 F1: 3.2989140514604 EM: 10.767634854771785\n",
            "--- Step: 11600 Ext Loss: 0.6931471824645996 Final Loss: 3.048961755233706 Start Acc: 3.8697318007662833 End Acc: 2.5670498084291187 Acc: 0.7926245210727969 F1: 3.3294244504034434 EM: 10.790229885057471\n",
            "--- Step: 11700 Ext Loss: 0.6931471824645996 Final Loss: 3.0501750996952803 Start Acc: 3.8745551601423487 End Acc: 2.5645017793594307 Acc: 0.7851423487544484 F1: 3.359792872815603 EM: 10.802935943060499\n",
            "--- Step: 11800 Ext Loss: 0.6931471824645996 Final Loss: 3.0499975773187176 Start Acc: 3.8745847176079735 End Acc: 2.5456810631229234 Acc: 0.7745016611295681 F1: 3.3412694696457286 EM: 10.859634551495017\n",
            "--- Step: 11900 Ext Loss: 0.6931471824645996 Final Loss: 3.051012978152694 Start Acc: 3.860981308411215 End Acc: 2.5311526479750777 Acc: 0.7710280373831776 F1: 3.3294597345036077 EM: 10.850856697819315\n",
            "--- Step: 12000 Ext Loss: 0.6931471824645996 Final Loss: 3.049965708416578 Start Acc: 3.8599706744868034 End Acc: 2.5494868035190614 Acc: 0.7752932551319648 F1: 3.3469531119758673 EM: 10.859604105571847\n",
            "--- Step: 12100 Ext Loss: 0.6931471824645996 Final Loss: 3.0507919645705712 Start Acc: 3.8590720221606647 End Acc: 2.5554016620498614 Acc: 0.775623268698061 F1: 3.3431973674974023 EM: 10.903739612188366\n",
            "--- Step: 12200 Ext Loss: 0.6931471824645996 Final Loss: 3.0495458163614346 Start Acc: 3.9009186351706036 End Acc: 2.5574146981627295 Acc: 0.7890419947506562 F1: 3.346358652258987 EM: 10.976049868766404\n",
            "--- Step: 12300 Ext Loss: 0.6931471824645996 Final Loss: 3.0489845026163684 Start Acc: 3.951059850374065 End Acc: 2.5498753117206983 Acc: 0.7933291770573566 F1: 3.343482970719926 EM: 10.956982543640898\n",
            "--- Step: 12400 Ext Loss: 0.6931471824645996 Final Loss: 3.0485174683097425 Start Acc: 3.96229216152019 End Acc: 2.557897862232779 Acc: 0.791270783847981 F1: 3.3402196018192893 EM: 10.951603325415677\n",
            "--- Step: 12500 Ext Loss: 0.6931471824645996 Final Loss: 3.0475748398406703 Start Acc: 3.9569160997732427 End Acc: 2.552437641723356 Acc: 0.7908163265306123 F1: 3.3462298505942663 EM: 10.922619047619047\n",
            "--- Step: 12600 Ext Loss: 0.6931471824645996 Final Loss: 3.0484149114687376 Start Acc: 3.9560737527114966 End Acc: 2.531182212581345 Acc: 0.7917570498915402 F1: 3.325115078098968 EM: 10.901572668112799\n",
            "--- Step: 12700 Ext Loss: 0.6931471824645996 Final Loss: 3.0477178571625707 Start Acc: 3.9773908523908523 End Acc: 2.5324844074844073 Acc: 0.8043139293139293 F1: 3.33224728777827 EM: 10.880977130977131\n",
            "--- Step: 12800 Ext Loss: 0.6931471824645996 Final Loss: 3.0468567672127973 Start Acc: 3.994510978043912 End Acc: 2.5474051896207586 Acc: 0.8133732534930139 F1: 3.336049348472761 EM: 10.899451097804391\n",
            "Epoch: 5 External Loss: 0.6931471824645996 Final Loss: 3.0462341697621733 Start Acc: 4.000145724798718 End Acc: 2.5453264842677936 Acc: 0.8148444994960351 F1: 3.3350117708826503 EM: 10.868077629441267\n",
            "--- Step: 12900 Ext Loss: 0.6931471824645996 Final Loss: 3.013599388061031 Start Acc: 3.4274193548387095 End Acc: 2.318548387096774 Acc: 0.5040322580645161 F1: 3.08150547930526 EM: 11.088709677419354\n",
            "--- Step: 13000 Ext Loss: 0.6931471824645996 Final Loss: 3.0311378977680934 Start Acc: 3.9122137404580153 End Acc: 2.2662213740458017 Acc: 0.691793893129771 F1: 3.2184116032562207 EM: 10.162213740458014\n",
            "--- Step: 13100 Ext Loss: 0.6931471824645996 Final Loss: 3.0354574934228675 Start Acc: 3.882575757575758 End Acc: 2.6244588744588744 Acc: 0.7846320346320347 F1: 3.3743430637974385 EM: 10.416666666666666\n",
            "--- Step: 13200 Ext Loss: 0.6931471824645996 Final Loss: 3.0283823776821355 Start Acc: 4.135196374622357 End Acc: 2.577416918429003 Acc: 0.7741691842900302 F1: 3.473929823050578 EM: 10.479607250755286\n",
            "--- Step: 13300 Ext Loss: 0.6931471824645996 Final Loss: 3.029864806863382 Start Acc: 4.161832946635731 End Acc: 2.5957076566125292 Acc: 0.7685614849187935 F1: 3.4226518893855324 EM: 10.375580046403712\n",
            "--- Step: 13400 Ext Loss: 0.6931471824645996 Final Loss: 3.0324425894886313 Start Acc: 4.196092278719397 End Acc: 2.6483050847457625 Acc: 0.8180320150659134 F1: 3.4475605257062534 EM: 10.569679849340867\n",
            "--- Step: 13500 Ext Loss: 0.6931471824645996 Final Loss: 3.027310458302687 Start Acc: 4.19473058637084 End Acc: 2.6396592709984152 Acc: 0.822107765451664 F1: 3.4510921345916556 EM: 10.727020602218701\n",
            "--- Step: 13600 Ext Loss: 0.6931471824645996 Final Loss: 3.029549986490962 Start Acc: 4.1894664842681255 End Acc: 2.6291039671682626 Acc: 0.829343365253078 F1: 3.414327597022759 EM: 10.764363885088919\n",
            "--- Step: 13700 Ext Loss: 0.6931471824645996 Final Loss: 3.027092865945056 Start Acc: 4.189229843561973 End Acc: 2.6436522262334536 Acc: 0.8385980746089049 F1: 3.3956327995908917 EM: 10.676143200962695\n",
            "--- Step: 13800 Ext Loss: 0.6931471824645996 Final Loss: 3.027664891631211 Start Acc: 4.178974221267454 End Acc: 2.665145005370569 Acc: 0.8492212674543501 F1: 3.4003669214376404 EM: 10.660580021482277\n",
            "--- Step: 13900 Ext Loss: 0.6931471824645996 Final Loss: 3.0279549601931346 Start Acc: 4.188894277400582 End Acc: 2.6733753637245394 Acc: 0.8456595538312318 F1: 3.4040270023751162 EM: 10.823836081474298\n",
            "--- Step: 14000 Ext Loss: 0.6931471824645996 Final Loss: 3.031072598448061 Start Acc: 4.1942970822281165 End Acc: 2.633178603006189 Acc: 0.8399646330680813 F1: 3.420034887535738 EM: 10.792440318302388\n",
            "--- Step: 14100 Ext Loss: 0.6931471824645996 Final Loss: 3.0313854482869225 Start Acc: 4.206437855402112 End Acc: 2.665515840779854 Acc: 0.8707351746547523 F1: 3.4476341341222727 EM: 10.88038180341186\n",
            "--- Step: 14200 Ext Loss: 0.6931471824645996 Final Loss: 3.028345480258979 Start Acc: 4.230841472577009 End Acc: 2.7023854244928627 Acc: 0.8874906085649887 F1: 3.468050893106429 EM: 10.88232531930879\n",
            "--- Step: 14300 Ext Loss: 0.6931471824645996 Final Loss: 3.029363348798932 Start Acc: 4.20379105520615 End Acc: 2.703529000698812 Acc: 0.886617749825297 F1: 3.4711320327263087 EM: 10.892732354996506\n",
            "--- Step: 14400 Ext Loss: 0.6931471824645996 Final Loss: 3.029551192251862 Start Acc: 4.190480078380144 End Acc: 2.7106466361854995 Acc: 0.9103527106466361 F1: 3.4690989086818593 EM: 10.950767472240365\n",
            "--- Step: 14500 Ext Loss: 0.6931471824645996 Final Loss: 3.029072630851273 Start Acc: 4.188381361128142 End Acc: 2.7437155119558554 Acc: 0.9254291845493562 F1: 3.473297911641246 EM: 10.971030042918455\n",
            "--- Step: 14600 Ext Loss: 0.6931471824645996 Final Loss: 3.0299880361364178 Start Acc: 4.217215482380127 End Acc: 2.7494945118428653 Acc: 0.9189052570768342 F1: 3.4903466764682323 EM: 10.949234546504911\n",
            "--- Step: 14700 Ext Loss: 0.6931471824645996 Final Loss: 3.0279494205633193 Start Acc: 4.224126160567995 End Acc: 2.7426952484980887 Acc: 0.914800655379574 F1: 3.49975043523425 EM: 10.94006007646095\n",
            "--- Step: 14800 Ext Loss: 0.6931471824645996 Final Loss: 3.0315677202284985 Start Acc: 4.20442775763853 End Acc: 2.7204168824443293 Acc: 0.9111211807353703 F1: 3.4821963999766146 EM: 10.978767477990678\n",
            "--- Step: 14900 Ext Loss: 0.6931471824645996 Final Loss: 3.0308840096378376 Start Acc: 4.209748892171344 End Acc: 2.7111029049729196 Acc: 0.9062653865091088 F1: 3.4752468274847215 EM: 10.909034958148695\n",
            "--- Step: 15000 Ext Loss: 0.6931471824645996 Final Loss: 3.0306820722881587 Start Acc: 4.196973251994369 End Acc: 2.699730173627405 Acc: 0.8959995307367433 F1: 3.464617766593375 EM: 10.91183716564993\n",
            "--- Step: 15100 Ext Loss: 0.6931471824645996 Final Loss: 3.0318852208324314 Start Acc: 4.195147915732855 End Acc: 2.6921783953384133 Acc: 0.8894554011653967 F1: 3.4693166947563245 EM: 10.912987449574182\n",
            "--- Step: 15200 Ext Loss: 0.6931471824645996 Final Loss: 3.031476501230005 Start Acc: 4.18007293007293 End Acc: 2.6785714285714284 Acc: 0.8781102531102531 F1: 3.460924798739732 EM: 10.939510939510939\n",
            "--- Step: 15300 Ext Loss: 0.6931471824645996 Final Loss: 3.029560012613388 Start Acc: 4.215086384204032 End Acc: 2.7020773344302755 Acc: 0.9011209378856437 F1: 3.4895795373411795 EM: 10.948426573426573\n",
            "--- Step: 15400 Ext Loss: 0.6931471824645996 Final Loss: 3.0288650933012855 Start Acc: 4.236220861319636 End Acc: 2.703970762544449 Acc: 0.9037929672066377 F1: 3.4899882961391464 EM: 10.94429079415251\n",
            "Epoch: 6 External Loss: 0.6931471824645996 Final Loss: 3.029250697378234 Start Acc: 4.2381628960375 End Acc: 2.7092668828251183 Acc: 0.9022793787266081 F1: 3.4914219184299116 EM: 10.9420251465706\n",
            "--- Step: 15500 Ext Loss: 0.6931471824645996 Final Loss: 2.9804301763835706 Start Acc: 4.605263157894737 End Acc: 2.5767543859649122 Acc: 0.7127192982456141 F1: 3.6951376419638278 EM: 11.293859649122806\n",
            "--- Step: 15600 Ext Loss: 0.6931471824645996 Final Loss: 2.995687887167475 Start Acc: 4.597929936305732 End Acc: 2.8264331210191083 Acc: 0.8160828025477707 F1: 3.5991859885733333 EM: 11.106687898089172\n",
            "--- Step: 15700 Ext Loss: 0.6931471824645996 Final Loss: 3.000369282548066 Start Acc: 4.499027237354086 End Acc: 2.6994163424124515 Acc: 0.8511673151750972 F1: 3.6523928337285576 EM: 11.174610894941635\n",
            "--- Step: 15800 Ext Loss: 0.6931471824645996 Final Loss: 3.0022245428475345 Start Acc: 4.455532212885154 End Acc: 2.8623949579831933 Acc: 0.8665966386554622 F1: 3.6677031971324507 EM: 11.003151260504202\n",
            "--- Step: 15900 Ext Loss: 0.6931471824645996 Final Loss: 3.0144697503396665 Start Acc: 4.533643326039387 End Acc: 2.913019693654267 Acc: 0.8821115973741794 F1: 3.7233615084915184 EM: 11.12554704595186\n",
            "--- Step: 16000 Ext Loss: 0.6931471824645996 Final Loss: 3.0130810318130137 Start Acc: 4.477109515260323 End Acc: 2.8949730700179535 Acc: 0.8864452423698385 F1: 3.6597243214779978 EM: 11.035682226211849\n",
            "--- Step: 16100 Ext Loss: 0.6931471824645996 Final Loss: 3.0088244834447018 Start Acc: 4.466324200913242 End Acc: 2.9823059360730593 Acc: 0.8894596651445966 F1: 3.6079202670659667 EM: 10.863774733637747\n",
            "--- Step: 16200 Ext Loss: 0.6931471824645996 Final Loss: 3.008552698981967 Start Acc: 4.384081902245707 End Acc: 2.9763870541611626 Acc: 0.8751651254953765 F1: 3.601877902340043 EM: 10.877642007926024\n",
            "--- Step: 16300 Ext Loss: 0.6931471824645996 Final Loss: 3.0063280892344313 Start Acc: 4.441365227537923 End Acc: 2.97914235705951 Acc: 0.8933780630105017 F1: 3.6346754019180807 EM: 10.935676779463243\n",
            "--- Step: 16400 Ext Loss: 0.6931471824645996 Final Loss: 3.009781224102311 Start Acc: 4.418103448275862 End Acc: 2.9519331243469176 Acc: 0.8881922675026124 F1: 3.6230329500091822 EM: 10.935867293625915\n",
            "--- Step: 16500 Ext Loss: 0.6931471824645996 Final Loss: 3.008018354480855 Start Acc: 4.381504257332072 End Acc: 2.9623935666982026 Acc: 0.8928571428571429 F1: 3.5863475683589643 EM: 10.915326395458846\n",
            "--- Step: 16600 Ext Loss: 0.6931471824645996 Final Loss: 3.011226567711002 Start Acc: 4.375540190146932 End Acc: 2.9575410544511667 Acc: 0.8940146931719966 F1: 3.575666729547739 EM: 10.911840968020742\n",
            "--- Step: 16700 Ext Loss: 0.6931471824645996 Final Loss: 3.01303854686643 Start Acc: 4.355608591885441 End Acc: 2.946002386634845 Acc: 0.8875298329355609 F1: 3.5792725457318477 EM: 10.874105011933175\n",
            "--- Step: 16800 Ext Loss: 0.6931471824645996 Final Loss: 3.012188900544877 Start Acc: 4.4100036845983785 End Acc: 2.9661016949152543 Acc: 0.9004237288135594 F1: 3.588299563113676 EM: 10.841930729550478\n",
            "--- Step: 16900 Ext Loss: 0.6931471824645996 Final Loss: 3.0111820602875174 Start Acc: 4.399021962937543 End Acc: 2.957704186684969 Acc: 0.8986787920384351 F1: 3.5795775388763027 EM: 10.867793411118738\n",
            "--- Step: 17000 Ext Loss: 0.6931471824645996 Final Loss: 3.0095332002670396 Start Acc: 4.423570969813745 End Acc: 2.9684489402697496 Acc: 0.9172286448298009 F1: 3.6052118129684856 EM: 10.836143224149005\n",
            "--- Step: 17100 Ext Loss: 0.6931471824645996 Final Loss: 3.0092818014723894 Start Acc: 4.479103802051901 End Acc: 2.998642124321062 Acc: 0.9373114061557031 F1: 3.6501936184814854 EM: 10.857347616173808\n",
            "--- Step: 17200 Ext Loss: 0.6931471824645996 Final Loss: 3.0125227742528944 Start Acc: 4.419820717131474 End Acc: 2.980933409220262 Acc: 0.9159789413773477 F1: 3.6091558653243303 EM: 10.86902390438247\n",
            "--- Step: 17300 Ext Loss: 0.6931471824645996 Final Loss: 3.0119725783537588 Start Acc: 4.4308696822832525 End Acc: 2.97859450726979 Acc: 0.9137722132471728 F1: 3.6052206693387023 EM: 10.827275175013462\n",
            "--- Step: 17400 Ext Loss: 0.6931471824645996 Final Loss: 3.0138913954041353 Start Acc: 4.392884517118038 End Acc: 2.9908661216147165 Acc: 0.9149846704138989 F1: 3.6058555858037535 EM: 10.871231476750127\n",
            "--- Step: 17500 Ext Loss: 0.6931471824645996 Final Loss: 3.0151257625064285 Start Acc: 4.379861448711716 End Acc: 2.9639645114244044 Acc: 0.9115216334467672 F1: 3.613512980479542 EM: 10.888125911521634\n",
            "--- Step: 17600 Ext Loss: 0.6931471824645996 Final Loss: 3.01396057093978 Start Acc: 4.379636068613816 End Acc: 2.9525961984237368 Acc: 0.9156235512285582 F1: 3.6211983065255042 EM: 10.89910755679184\n",
            "--- Step: 17700 Ext Loss: 0.6931471824645996 Final Loss: 3.0154649743370423 Start Acc: 4.358661940629154 End Acc: 2.928389455028799 Acc: 0.9082853345148427 F1: 3.6102320374338106 EM: 10.906346920691183\n",
            "--- Step: 17800 Ext Loss: 0.6931471824645996 Final Loss: 3.01317026456332 Start Acc: 4.363332626219771 End Acc: 2.9367310140008485 Acc: 0.910850657615613 F1: 3.6181526164669995 EM: 10.944792108612644\n",
            "--- Step: 17900 Ext Loss: 0.6931471824645996 Final Loss: 3.0139817394858994 Start Acc: 4.357448107448107 End Acc: 2.9316748066748066 Acc: 0.917022792022792 F1: 3.623890407111556 EM: 10.973748473748474\n",
            "--- Step: 18000 Ext Loss: 0.6931471824645996 Final Loss: 3.0149304565620647 Start Acc: 4.355585980954244 End Acc: 2.909428749556862 Acc: 0.9070571983912571 F1: 3.6064584439536573 EM: 10.966220179898318\n",
            "Epoch: 7 External Loss: 0.6931471824645996 Final Loss: 3.0150122408174043 Start Acc: 4.3510996150436565 End Acc: 2.9084241077392012 Acc: 0.9047081253719018 F1: 3.6006494551769133 EM: 10.944784382284382\n",
            "--- Step: 18100 Ext Loss: 0.6931471824645996 Final Loss: 2.9901033941521704 Start Acc: 4.292168674698795 End Acc: 3.1626506024096384 Acc: 0.9036144578313253 F1: 3.635041972681235 EM: 11.521084337349398\n",
            "--- Step: 18200 Ext Loss: 0.6931471824645996 Final Loss: 3.002339211969428 Start Acc: 4.422814207650274 End Acc: 2.800546448087432 Acc: 0.9221311475409836 F1: 3.7050117461932186 EM: 10.94603825136612\n",
            "--- Step: 18300 Ext Loss: 0.6931471824645996 Final Loss: 2.995494791981188 Start Acc: 4.372791519434629 End Acc: 2.859982332155477 Acc: 0.9275618374558304 F1: 3.645455609279451 EM: 11.351590106007068\n",
            "--- Step: 18400 Ext Loss: 0.6931471824645996 Final Loss: 2.997952350437174 Start Acc: 4.422323759791123 End Acc: 2.9046997389033944 Acc: 0.9138381201044387 F1: 3.6541629147099104 EM: 11.259791122715404\n",
            "--- Step: 18500 Ext Loss: 0.6931471824645996 Final Loss: 3.0019464236115323 Start Acc: 4.4189958592132506 End Acc: 2.898550724637681 Acc: 0.9316770186335404 F1: 3.6213883370087903 EM: 11.193064182194616\n",
            "--- Step: 18600 Ext Loss: 0.6931471824645996 Final Loss: 2.9994379611203437 Start Acc: 4.400728987993139 End Acc: 2.867710120068611 Acc: 0.9326758147512865 F1: 3.6342126610449386 EM: 11.283233276157805\n",
            "--- Step: 18700 Ext Loss: 0.6931471824645996 Final Loss: 3.0014591911701296 Start Acc: 4.415263543191801 End Acc: 2.8184480234260616 Acc: 0.9150805270863837 F1: 3.610700761971222 EM: 11.35614934114202\n",
            "--- Step: 18800 Ext Loss: 0.6931471824645996 Final Loss: 3.0053077678875297 Start Acc: 4.418103448275862 End Acc: 2.8216794380587484 Acc: 0.9179438058748404 F1: 3.575105898771614 EM: 11.41044061302682\n",
            "--- Step: 18900 Ext Loss: 0.6931471824645996 Final Loss: 3.0044425257470033 Start Acc: 4.533550396375991 End Acc: 2.905577576443941 Acc: 0.9697055492638732 F1: 3.6922949588048626 EM: 11.417044167610419\n",
            "--- Step: 19000 Ext Loss: 0.6931471824645996 Final Loss: 3.002673974721138 Start Acc: 4.507884028484232 End Acc: 2.9755849440488302 Acc: 0.9727873855544252 F1: 3.7170374452797743 EM: 11.368260427263479\n",
            "--- Step: 19100 Ext Loss: 0.6931471824645996 Final Loss: 3.0030386630628025 Start Acc: 4.492728531855955 End Acc: 2.943213296398892 Acc: 0.9637580794090489 F1: 3.713732761503695 EM: 11.302516158818097\n",
            "--- Step: 19200 Ext Loss: 0.6931471824645996 Final Loss: 3.0043545069577626 Start Acc: 4.532967032967033 End Acc: 2.955938292476754 Acc: 0.9747464074387151 F1: 3.709186875518874 EM: 11.250528317836011\n",
            "--- Step: 19300 Ext Loss: 0.6931471824645996 Final Loss: 3.005783805230216 Start Acc: 4.55962587685113 End Acc: 2.964243959469992 Acc: 0.9888932190179267 F1: 3.69827284060019 EM: 11.184723304754481\n",
            "--- Step: 19400 Ext Loss: 0.6931471824645996 Final Loss: 3.006279971059991 Start Acc: 4.559833694866233 End Acc: 2.9577910339840927 Acc: 1.0055133767172812 F1: 3.7033247532470974 EM: 11.243673174258857\n",
            "--- Step: 19500 Ext Loss: 0.6931471824645996 Final Loss: 3.004402669152937 Start Acc: 4.557906271072151 End Acc: 2.975387727579231 Acc: 1.0009271746459878 F1: 3.7314860999410424 EM: 11.248314227916385\n",
            "--- Step: 19600 Ext Loss: 0.6931471824645996 Final Loss: 3.0047794243842083 Start Acc: 4.550300063171194 End Acc: 2.9631238155401136 Acc: 1.000868603916614 F1: 3.724508542654248 EM: 11.244472520530637\n",
            "--- Step: 19700 Ext Loss: 0.6931471824645996 Final Loss: 3.0047903746561286 Start Acc: 4.5547385620915035 End Acc: 2.983882947118241 Acc: 1.0212418300653594 F1: 3.748641888858456 EM: 11.272653000594177\n",
            "--- Step: 19800 Ext Loss: 0.6931471824645996 Final Loss: 3.004263746156762 Start Acc: 4.523625911385306 End Acc: 2.9812815479528885 Acc: 1.0095344924284912 F1: 3.743100465956988 EM: 11.273135165451487\n",
            "--- Step: 19900 Ext Loss: 0.6931471824645996 Final Loss: 3.004746659031208 Start Acc: 4.505775358470526 End Acc: 2.965679766330324 Acc: 1.0023898035050451 F1: 3.744729876683771 EM: 11.235395645246946\n",
            "--- Step: 20000 Ext Loss: 0.6931471824645996 Final Loss: 3.0049660182997124 Start Acc: 4.480269793242562 End Acc: 2.9516515380736257 Acc: 0.9991174987392839 F1: 3.7334557728917623 EM: 11.195158850226928\n",
            "--- Step: 20100 Ext Loss: 0.6931471824645996 Final Loss: 3.0056047416683347 Start Acc: 4.446711473835814 End Acc: 2.9509721555448873 Acc: 0.993158905424868 F1: 3.725486634744253 EM: 11.18428948631781\n",
            "--- Step: 20200 Ext Loss: 0.6931471824645996 Final Loss: 3.0064401214842507 Start Acc: 4.421953733394411 End Acc: 2.956081081081081 Acc: 0.9977668346312414 F1: 3.7200581399472448 EM: 11.201614750343564\n",
            "--- Step: 20300 Ext Loss: 0.6931471824645996 Final Loss: 3.0052827120976024 Start Acc: 4.426741130091984 End Acc: 2.9415790626368814 Acc: 0.9882829610162067 F1: 3.7117885286448797 EM: 11.18457074025405\n",
            "--- Step: 20400 Ext Loss: 0.6931471824645996 Final Loss: 3.0049411382086877 Start Acc: 4.4167016365925305 End Acc: 2.9309169114561477 Acc: 0.9940201426772975 F1: 3.7008448472012843 EM: 11.19256189676878\n",
            "--- Step: 20500 Ext Loss: 0.6931471824645996 Final Loss: 3.0037582386322574 Start Acc: 4.449003221908981 End Acc: 2.9336991542488926 Acc: 1.0018123238018526 F1: 3.7107060385871016 EM: 11.193616592831253\n",
            "Epoch: 8 External Loss: 0.6931471824645996 Final Loss: 3.0036656329496574 Start Acc: 4.453106974145992 End Acc: 2.9387834408053726 Acc: 1.0042867378289433 F1: 3.709933560674969 EM: 11.225895316804408\n",
            "--- Step: 20600 Ext Loss: 0.6931471824645996 Final Loss: 2.766549481285943 Start Acc: 6.944444444444445 End Acc: 6.25 Acc: 1.7361111111111112 F1: 5.482001904303884 EM: 8.680555555555555\n",
            "--- Step: 20700 Ext Loss: 0.6931471824645996 Final Loss: 2.9959088399869587 Start Acc: 4.386467889908257 End Acc: 3.153669724770642 Acc: 0.8314220183486238 F1: 3.6734611964158517 EM: 9.747706422018348\n",
            "--- Step: 20800 Ext Loss: 0.6931471824645996 Final Loss: 2.9932097521695225 Start Acc: 4.366028708133971 End Acc: 2.8857655502392343 Acc: 0.8373205741626795 F1: 3.568760064930386 EM: 10.062799043062201\n",
            "--- Step: 20900 Ext Loss: 0.6931471824645996 Final Loss: 2.98628763859326 Start Acc: 4.3790453074433655 End Acc: 3.1957928802588995 Acc: 0.9101941747572816 F1: 3.6620179017377024 EM: 10.325647249190938\n",
            "--- Step: 21000 Ext Loss: 0.6931471824645996 Final Loss: 2.992569362621727 Start Acc: 4.400977995110025 End Acc: 3.23960880195599 Acc: 0.9550733496332519 F1: 3.7416580614550727 EM: 10.444682151589243\n",
            "--- Step: 21100 Ext Loss: 0.6931471824645996 Final Loss: 2.9909924959621166 Start Acc: 4.340618860510806 End Acc: 3.229371316306483 Acc: 0.9393418467583498 F1: 3.7509525303643887 EM: 10.590618860510805\n",
            "--- Step: 21200 Ext Loss: 0.6931471824645996 Final Loss: 2.9872548349189447 Start Acc: 4.382183908045977 End Acc: 3.222495894909688 Acc: 0.9852216748768473 F1: 3.7532579306350526 EM: 10.729679802955665\n",
            "--- Step: 21300 Ext Loss: 0.6931471824645996 Final Loss: 2.990582711942105 Start Acc: 4.394393511988716 End Acc: 3.177891396332863 Acc: 0.974083215796897 F1: 3.74095082720766 EM: 10.803067700987306\n",
            "--- Step: 21400 Ext Loss: 0.6931471824645996 Final Loss: 2.9917343116661232 Start Acc: 4.395859085290482 End Acc: 3.0902348578491967 Acc: 0.9463844252163165 F1: 3.714644621227464 EM: 10.811959208899877\n",
            "--- Step: 21500 Ext Loss: 0.6931471824645996 Final Loss: 2.992257559653556 Start Acc: 4.4107535753575355 End Acc: 3.0115511551155114 Acc: 0.9282178217821783 F1: 3.6694554911407633 EM: 10.839521452145215\n",
            "--- Step: 21600 Ext Loss: 0.6931471824645996 Final Loss: 2.992436919387197 Start Acc: 4.410307234886026 End Acc: 2.9980178394449952 Acc: 0.9260406342913776 F1: 3.6700338567720014 EM: 10.855426164519326\n",
            "--- Step: 21700 Ext Loss: 0.6931471824645996 Final Loss: 2.993697876968934 Start Acc: 4.404305680793508 End Acc: 3.0207394048692517 Acc: 0.9383453561767358 F1: 3.6603026384525172 EM: 10.809287646528404\n",
            "--- Step: 21800 Ext Loss: 0.6931471824645996 Final Loss: 2.990525749126084 Start Acc: 4.445822994210091 End Acc: 3.026778329197684 Acc: 0.9408602150537635 F1: 3.6716637664752305 EM: 10.713916459884201\n",
            "--- Step: 21900 Ext Loss: 0.6931471824645996 Final Loss: 2.992296935402773 Start Acc: 4.466673032849504 End Acc: 3.0056340718105425 Acc: 0.9477654698242933 F1: 3.6555256899882758 EM: 10.72860962566845\n",
            "--- Step: 22000 Ext Loss: 0.6931471824645996 Final Loss: 2.9925643314783925 Start Acc: 4.440205819730306 End Acc: 3.0163236337828248 Acc: 0.9536905606813343 F1: 3.6636200628305824 EM: 10.829932576295246\n",
            "--- Step: 22100 Ext Loss: 0.6931471824645996 Final Loss: 2.9923223363713287 Start Acc: 4.469019218025182 End Acc: 3.0380218687872764 Acc: 0.973326706428098 F1: 3.685445946402957 EM: 10.878479125248509\n",
            "--- Step: 22200 Ext Loss: 0.6931471824645996 Final Loss: 2.9924896001963974 Start Acc: 4.461233685518956 End Acc: 3.0376009944064637 Acc: 0.971100062150404 F1: 3.6871448231580533 EM: 10.901569297700435\n",
            "--- Step: 22300 Ext Loss: 0.6931471824645996 Final Loss: 2.994011395712775 Start Acc: 4.456187829139848 End Acc: 3.0335722644821534 Acc: 0.9636483323581042 F1: 3.687257147331223 EM: 10.8780719719134\n",
            "--- Step: 22400 Ext Loss: 0.6931471824645996 Final Loss: 2.993685767557029 Start Acc: 4.463792150359315 End Acc: 3.0161691542288556 Acc: 0.9639303482587065 F1: 3.6986093269877354 EM: 10.927998894416804\n",
            "--- Step: 22500 Ext Loss: 0.6931471824645996 Final Loss: 2.99360636680647 Start Acc: 4.478784704033526 End Acc: 3.03496595075956 Acc: 0.9707307490832897 F1: 3.699660189472392 EM: 10.900667888947092\n",
            "--- Step: 22600 Ext Loss: 0.6931471824645996 Final Loss: 2.9935219623723275 Start Acc: 4.461174713787954 End Acc: 3.005226480836237 Acc: 0.9644101543056247 F1: 3.6876929983730795 EM: 10.90872324539572\n",
            "--- Step: 22700 Ext Loss: 0.6931471824645996 Final Loss: 2.994252560996738 Start Acc: 4.498577524893315 End Acc: 3.0094238975817924 Acc: 0.9720246562351825 F1: 3.6937784780683405 EM: 10.898233760075865\n",
            "--- Step: 22800 Ext Loss: 0.6931471824645996 Final Loss: 2.9937764601018864 Start Acc: 4.487324581258488 End Acc: 3.004753282028067 Acc: 0.9761204164780444 F1: 3.703342761908414 EM: 10.898596650067905\n",
            "--- Step: 22900 Ext Loss: 0.6931471824645996 Final Loss: 2.9929349627934685 Start Acc: 4.474339540926808 End Acc: 2.999133824166306 Acc: 0.9744478129060199 F1: 3.6994207976876905 EM: 10.946297098310957\n",
            "--- Step: 23000 Ext Loss: 0.6931471824645996 Final Loss: 2.992904978008278 Start Acc: 4.4611353258613535 End Acc: 2.993980904939809 Acc: 0.9781029472810294 F1: 3.7182568610661537 EM: 10.966687422166874\n",
            "--- Step: 23100 Ext Loss: 0.6931471824645996 Final Loss: 2.993826228434843 Start Acc: 4.460193304105221 End Acc: 3.0104125149461938 Acc: 0.9764846552411319 F1: 3.7073595390188734 EM: 10.950577919489836\n",
            "Epoch: 9 External Loss: 0.6931471824645996 Final Loss: 2.9938306873309677 Start Acc: 4.474965693953635 End Acc: 3.0201464534227114 Acc: 0.9763561514080659 F1: 3.7149360374464284 EM: 10.940921452285089\n",
            "--- Step: 23200 Ext Loss: 0.6931471824645996 Final Loss: 2.951617751802717 Start Acc: 5.267857142857143 End Acc: 3.9285714285714284 Acc: 1.25 F1: 4.249668173511846 EM: 11.964285714285714\n",
            "--- Step: 23300 Ext Loss: 0.6931471824645996 Final Loss: 2.982494513193766 Start Acc: 4.236111111111111 End Acc: 3.1944444444444446 Acc: 0.9027777777777778 F1: 3.8321376334672297 EM: 11.597222222222221\n",
            "--- Step: 23400 Ext Loss: 0.6931471824645996 Final Loss: 2.9809315153893006 Start Acc: 4.5212765957446805 End Acc: 3.377659574468085 Acc: 1.050531914893617 F1: 3.9388423358799067 EM: 11.529255319148936\n",
            "--- Step: 23500 Ext Loss: 0.6931471824645996 Final Loss: 2.9865807241468287 Start Acc: 4.626865671641791 End Acc: 3.3955223880597014 Acc: 1.0634328358208955 F1: 3.8754185916344293 EM: 11.800373134328359\n",
            "--- Step: 23600 Ext Loss: 0.6931471824645996 Final Loss: 2.9832929293314616 Start Acc: 4.813218390804598 End Acc: 3.3477011494252875 Acc: 1.0919540229885059 F1: 3.952280174581939 EM: 11.688218390804598\n",
            "--- Step: 23700 Ext Loss: 0.6931471824645996 Final Loss: 2.9742031993152938 Start Acc: 4.929906542056075 End Acc: 3.34696261682243 Acc: 1.1098130841121496 F1: 4.011460366056044 EM: 11.536214953271028\n",
            "--- Step: 23800 Ext Loss: 0.6931471824645996 Final Loss: 2.9803567300631304 Start Acc: 4.832677165354331 End Acc: 3.302165354330709 Acc: 1.062992125984252 F1: 3.912812040649704 EM: 11.584645669291339\n",
            "--- Step: 23900 Ext Loss: 0.6931471824645996 Final Loss: 2.9803338949372167 Start Acc: 4.855442176870748 End Acc: 3.320578231292517 Acc: 1.0459183673469388 F1: 3.8788361903111586 EM: 11.415816326530612\n",
            "--- Step: 24000 Ext Loss: 0.6931471824645996 Final Loss: 2.978342933140829 Start Acc: 4.854041916167665 End Acc: 3.282185628742515 Acc: 1.029191616766467 F1: 3.8752153168071013 EM: 11.362275449101796\n",
            "--- Step: 24100 Ext Loss: 0.6931471824645996 Final Loss: 2.9796642841502305 Start Acc: 4.81951871657754 End Acc: 3.342245989304813 Acc: 1.0427807486631016 F1: 3.8589445451824798 EM: 11.330213903743315\n",
            "--- Step: 24200 Ext Loss: 0.6931471824645996 Final Loss: 2.9806020734390777 Start Acc: 4.833937198067633 End Acc: 3.3121980676328504 Acc: 1.0295893719806763 F1: 3.8646201596840073 EM: 11.295289855072463\n",
            "--- Step: 24300 Ext Loss: 0.6931471824645996 Final Loss: 2.9806983349081704 Start Acc: 4.840308370044053 End Acc: 3.2709251101321586 Acc: 1.0297356828193833 F1: 3.8463941733996427 EM: 11.277533039647578\n",
            "--- Step: 24400 Ext Loss: 0.6931471824645996 Final Loss: 2.979991768439289 Start Acc: 4.863360323886639 End Acc: 3.279352226720648 Acc: 1.0526315789473684 F1: 3.858633730991742 EM: 11.272773279352228\n",
            "--- Step: 24500 Ext Loss: 0.6931471824645996 Final Loss: 2.980985203396515 Start Acc: 4.88061797752809 End Acc: 3.326310861423221 Acc: 1.0697565543071161 F1: 3.8628185360464107 EM: 11.266385767790263\n",
            "--- Step: 24600 Ext Loss: 0.6931471824645996 Final Loss: 2.980142553508905 Start Acc: 4.862804878048781 End Acc: 3.312282229965157 Acc: 1.056184668989547 F1: 3.8641228642744285 EM: 11.243466898954704\n",
            "--- Step: 24700 Ext Loss: 0.6931471824645996 Final Loss: 2.9812895066575043 Start Acc: 4.869706840390879 End Acc: 3.2797231270358305 Acc: 1.0688110749185669 F1: 3.877293593214544 EM: 11.19299674267101\n",
            "--- Step: 24800 Ext Loss: 0.6931471824645996 Final Loss: 2.9828280497034756 Start Acc: 4.806957186544342 End Acc: 3.2626146788990824 Acc: 1.0301987767584098 F1: 3.834140554155937 EM: 11.186926605504587\n",
            "--- Step: 24900 Ext Loss: 0.6931471824645996 Final Loss: 2.981322816606901 Start Acc: 4.796469740634006 End Acc: 3.2636887608069163 Acc: 1.0176512968299711 F1: 3.823342998741835 EM: 11.213976945244957\n",
            "--- Step: 25000 Ext Loss: 0.6931471824645996 Final Loss: 2.9809481723431994 Start Acc: 4.795640326975477 End Acc: 3.2510217983651226 Acc: 1.0047683923705721 F1: 3.774205249156091 EM: 11.166553133514986\n",
            "--- Step: 25100 Ext Loss: 0.6931471824645996 Final Loss: 2.9827116175215376 Start Acc: 4.820736434108527 End Acc: 3.228359173126615 Acc: 0.9964470284237726 F1: 3.7617157089137194 EM: 11.170865633074936\n",
            "--- Step: 25200 Ext Loss: 0.6931471824645996 Final Loss: 2.9832981728218697 Start Acc: 4.778869778869779 End Acc: 3.20485257985258 Acc: 0.9889434889434889 F1: 3.7617081133784147 EM: 11.188574938574938\n",
            "--- Step: 25300 Ext Loss: 0.6931471824645996 Final Loss: 2.9839734841286436 Start Acc: 4.774590163934426 End Acc: 3.2142857142857144 Acc: 0.9909250585480094 F1: 3.761594498690609 EM: 11.2134074941452\n",
            "--- Step: 25400 Ext Loss: 0.6931471824645996 Final Loss: 2.982907180018073 Start Acc: 4.774888143176733 End Acc: 3.1865212527964206 Acc: 0.9927293064876958 F1: 3.757225200526398 EM: 11.222035794183444\n",
            "--- Step: 25500 Ext Loss: 0.6931471824645996 Final Loss: 2.984051124748377 Start Acc: 4.791220556745182 End Acc: 3.1718415417558887 Acc: 0.9823340471092077 F1: 3.7444499091095214 EM: 11.208511777301927\n",
            "--- Step: 25600 Ext Loss: 0.6931471824645996 Final Loss: 2.984055390837746 Start Acc: 4.811344969199179 End Acc: 3.191735112936345 Acc: 0.9933264887063655 F1: 3.7560778039146565 EM: 11.239733059548255\n",
            "--- Step: 25700 Ext Loss: 0.6931471824645996 Final Loss: 2.9837521954636133 Start Acc: 4.838510848126233 End Acc: 3.187869822485207 Acc: 1.0022189349112427 F1: 3.7661996245468083 EM: 11.273422090729783\n",
            "Epoch: 10 External Loss: 0.6931471824645996 Final Loss: 2.984944151646422 Start Acc: 4.827133957521221 End Acc: 3.181658105334742 Acc: 1.0006436178610028 F1: 3.7593922988157664 EM: 11.296531751077204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-fcdb515fc695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpt4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt4_glove3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrained_model_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpt4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexpt4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-ad072ef4ca9f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iterator)\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mevaluation_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'beta2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'delta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mevaluation_parameters\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_itos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-ad072ef4ca9f>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(self, model, dataset_iterator, evaluation_parameters, vocab_itos)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#[B, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mquestion_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m       \u001b[0mtrue_start_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_start_index_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m       \u001b[0mtrue_end_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_end_index_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0munanswerable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_answer_absent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'word_start_index_1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8SIvkUmYT36",
        "colab_type": "text"
      },
      "source": [
        "## Expt 5 - Updated memory module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJMc8lOgcspE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /content/drive/Shared\\ drives/CIS\\ 700-1\\ Final\\ Project/runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTJjDPBfYXMM",
        "colab_type": "code",
        "outputId": "2005a968-bc76-4cae-8cf6-4ba80718176c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "hyperparameters5 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 2,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': False,\\\n",
        "                   'num_epochs': 10,\\\n",
        "                   'learning_rate': 1e-4,\\\n",
        "                   'loss_function': retrospective_parallel_loss,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt5 = Experiment(hyperparameters5, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt5_glove')\n",
        "trained_model_glove = expt5.train(train_iterator, expt_name='expt5_glove', load_model=True)\n",
        "expt5.evaluate(val_iterator, expt_name='expt5_glove', load_model=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a4455d0c2ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpt5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt5_glove'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# trained_model_glove = expt5.train(train_iterator, expt_name='expt5_glove', load_model=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexpt5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expt5_glove'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-94340eead14a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iterator, expt_name, load_model)\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mevaluation_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'beta2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'delta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mevaluation_parameters\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab_itos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mexpt_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-94340eead14a>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(self, model, dataset_iterator, evaluation_parameters, vocab_itos, expt_name)\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0munanswerable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_answer_absent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mpred_start_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_end_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ext_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_int_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'start_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_start_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_end_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unanswerable_ext'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_ext_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unanswerable_int'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_int_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0e9450fc8117>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, document, doc_lengths, question, question_lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDPrime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQPrime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPrime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mMPrime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-b6a974972d76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, D, Q, question_lengths)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpasses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_passes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-869231b89e3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, D, Q, prev_mem)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Q = Q.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# prev_mem = prev_mem.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttnGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Now considering prev_mem, C and question, we will update the memory state as follows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-869231b89e3c>\u001b[0m in \u001b[0;36mgateMatrix\u001b[0;34m(self, D, Q, prev_mem)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0membedding_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprev_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# z.size() = (batch_size, num_sentences, 4*embedding_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membedding_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 15.18 GiB already allocated; 5.88 MiB free; 15.19 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdAX5roUoUE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expt5 = Experiment(hyperparameters5, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt5_glove')\n",
        "expt5.evaluate(val_iterator, expt_name='expt5_glove', load_model=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGo_ERl22zg",
        "colab_type": "text"
      },
      "source": [
        "## Expt 6 - Without unanswerable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9DtmK5223Ep",
        "colab_type": "code",
        "outputId": "391915c3-c423-4c36-8819-2f5ddebc06ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hyperparameters6 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 2,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': False,\\\n",
        "                   'num_epochs': 10,\\\n",
        "                   'learning_rate': 1e-4,\\\n",
        "                   'loss_function': retrospective_loss_span,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt6 = Experiment(hyperparameters6, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt6_glove')\n",
        "trained_model_glove = expt6.train(train_iterator, expt_name='expt6_glove_5', load_model=True)\n",
        "expt6.evaluate(val_iterator, expt_name='expt6_glove', load_model=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "--- Step: 0 Loss: 9.596662521362305 Start Acc: 12.5 End Acc: 9.375 Acc: 3.125 F1: 10.155237234566538 EM: 3.125\n",
            "--- Step: 100 Loss: 9.748134258950111 Start Acc: 8.818069306930694 End Acc: 5.816831683168317 Acc: 2.258663366336634 F1: 8.33831979339612 EM: 4.672029702970297\n",
            "--- Step: 200 Loss: 9.700473332286474 Start Acc: 8.61318407960199 End Acc: 5.892412935323383 Acc: 2.347636815920398 F1: 8.113869690568356 EM: 4.353233830845771\n",
            "--- Step: 300 Loss: 9.673300700330259 Start Acc: 8.731312292358805 End Acc: 5.928156146179402 Acc: 2.3151993355481726 F1: 8.124684863249463 EM: 4.45390365448505\n",
            "--- Step: 400 Loss: 9.657380678409947 Start Acc: 8.829488778054863 End Acc: 5.953865336658354 Acc: 2.3924563591022445 F1: 8.205733174057672 EM: 4.51215710723192\n",
            "--- Step: 500 Loss: 9.66882743188245 Start Acc: 8.813622754491018 End Acc: 5.906936127744511 Acc: 2.4451097804391217 F1: 8.270117556207767 EM: 4.572105788423154\n",
            "--- Step: 600 Loss: 9.67645449804982 Start Acc: 8.782237936772047 End Acc: 5.93801996672213 Acc: 2.381447587354409 F1: 8.111714105281566 EM: 4.47171381031614\n",
            "--- Step: 700 Loss: 9.67525524963836 Start Acc: 8.733059914407988 End Acc: 5.888908701854493 Acc: 2.3760699001426535 F1: 8.074711459795164 EM: 4.426711840228245\n",
            "--- Step: 800 Loss: 9.674381173356494 Start Acc: 8.805399500624219 End Acc: 5.914481897627965 Acc: 2.395443196004994 F1: 8.086289998673728 EM: 4.416354556803995\n",
            "--- Step: 900 Loss: 9.67408972707361 Start Acc: 8.809655937846836 End Acc: 5.927441731409545 Acc: 2.3688956714761376 F1: 8.056201536142915 EM: 4.394422863485016\n",
            "--- Step: 1000 Loss: 9.674254937605424 Start Acc: 8.834915084915085 End Acc: 5.953421578421579 Acc: 2.357017982017982 F1: 8.009210088228762 EM: 4.358141858141858\n",
            "--- Step: 1100 Loss: 9.685365764798087 Start Acc: 8.841394187102633 End Acc: 5.9519754768392374 Acc: 2.358651226158038 F1: 7.996836360137573 EM: 4.348319709355132\n",
            "--- Step: 1200 Loss: 9.689039558693333 Start Acc: 8.779142381348876 End Acc: 5.942964196502914 Acc: 2.3313905079100747 F1: 7.98495895579959 EM: 4.353143213988343\n",
            "--- Step: 1300 Loss: 9.695603703462922 Start Acc: 8.728862413528056 End Acc: 5.971368178324366 Acc: 2.3491544965411224 F1: 7.95533304192253 EM: 4.3452152190622595\n",
            "--- Step: 1400 Loss: 9.686451576676733 Start Acc: 8.779443254817988 End Acc: 6.015792291220556 Acc: 2.3643825838686654 F1: 8.007549298896137 EM: 4.38749107780157\n",
            "--- Step: 1500 Loss: 9.683079797374972 Start Acc: 8.844103930712858 End Acc: 6.014740173217855 Acc: 2.4046469020652896 F1: 8.054126382244332 EM: 4.447035309793471\n",
            "--- Step: 1600 Loss: 9.683177387766507 Start Acc: 8.830418488444723 End Acc: 5.965021861336664 Acc: 2.3793722673329167 F1: 8.035008489074755 EM: 4.4347282948157405\n",
            "--- Step: 1700 Loss: 9.690074981205047 Start Acc: 8.862433862433862 End Acc: 5.908289241622575 Acc: 2.3552322163433272 F1: 7.9977966519447845 EM: 4.403659611992945\n",
            "--- Step: 1800 Loss: 9.68886943739828 Start Acc: 8.859661299278178 End Acc: 5.90123542476402 Acc: 2.345918933925597 F1: 7.965487784242633 EM: 4.3899222654081065\n",
            "--- Step: 1900 Loss: 9.690006715131396 Start Acc: 8.822659126775381 End Acc: 5.866977906365071 Acc: 2.326078379800105 F1: 7.949906420819708 EM: 4.380917937927407\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 5 Loss: 9.691087981934285 Start Acc: 8.841742934808833 End Acc: 5.86125363915652 Acc: 2.332277106689614 F1: 7.953227241663876 EM: 4.381409523264902\n",
            "--- Step: 2000 Loss: 9.38369241254083 Start Acc: 9.644396551724139 End Acc: 7.219827586206897 Acc: 2.9633620689655173 F1: 8.805503568024804 EM: 5.334051724137931\n",
            "--- Step: 2100 Loss: 9.47377087496504 Start Acc: 10.205696202531646 End Acc: 6.625791139240507 Acc: 2.9667721518987342 F1: 9.272820770431556 EM: 5.597310126582278\n",
            "--- Step: 2200 Loss: 9.538850730703782 Start Acc: 9.811046511627907 End Acc: 6.25 Acc: 2.616279069767442 F1: 8.521975669388379 EM: 4.953972868217054\n",
            "--- Step: 2300 Loss: 9.55156680858335 Start Acc: 9.645600558659218 End Acc: 6.302374301675978 Acc: 2.618715083798883 F1: 8.536289718832347 EM: 4.862081005586592\n",
            "--- Step: 2400 Loss: 9.549611336279124 Start Acc: 9.497816593886462 End Acc: 6.290938864628821 Acc: 2.599617903930131 F1: 8.481958284540704 EM: 4.762554585152839\n",
            "--- Step: 2500 Loss: 9.574974365986376 Start Acc: 9.453405017921147 End Acc: 6.13239247311828 Acc: 2.536962365591398 F1: 8.453972000344816 EM: 4.715501792114695\n",
            "--- Step: 2600 Loss: 9.574166692861308 Start Acc: 9.436740121580547 End Acc: 6.136018237082067 Acc: 2.578837386018237 F1: 8.490228714637293 EM: 4.758738601823708\n",
            "--- Step: 2700 Loss: 9.575124975244728 Start Acc: 9.440963060686016 End Acc: 6.171668865435357 Acc: 2.6055408970976255 F1: 8.514072381446848 EM: 4.8070580474934035\n",
            "--- Step: 2800 Loss: 9.576610243959582 Start Acc: 9.345862470862471 End Acc: 6.21722027972028 Acc: 2.5713869463869465 F1: 8.516196396943984 EM: 4.785839160839161\n",
            "--- Step: 2900 Loss: 9.58353534322193 Start Acc: 9.355427974947808 End Acc: 6.184759916492693 Acc: 2.603079331941545 F1: 8.516688250301133 EM: 4.837552192066806\n",
            "--- Step: 3000 Loss: 9.574424036058451 Start Acc: 9.448842155009451 End Acc: 6.25 Acc: 2.637641776937618 F1: 8.596638101940519 EM: 4.882443289224953\n",
            "--- Step: 3100 Loss: 9.58003595867717 Start Acc: 9.401986183074266 End Acc: 6.309369602763385 Acc: 2.6311528497409324 F1: 8.612882778727815 EM: 4.881800518134715\n",
            "--- Step: 3200 Loss: 9.583573501705176 Start Acc: 9.432134340222575 End Acc: 6.289745627980922 Acc: 2.608306836248013 F1: 8.615494167385481 EM: 4.843998410174881\n",
            "--- Step: 3300 Loss: 9.591415701979917 Start Acc: 9.41412002945508 End Acc: 6.25230117820324 Acc: 2.618740795287187 F1: 8.59544842366468 EM: 4.832474226804123\n",
            "--- Step: 3400 Loss: 9.578295503951232 Start Acc: 9.471450617283951 End Acc: 6.286436899862826 Acc: 2.642746913580247 F1: 8.615637307198599 EM: 4.841820987654321\n",
            "--- Step: 3500 Loss: 9.580087685921685 Start Acc: 9.517410141206675 End Acc: 6.254011553273427 Acc: 2.647625160462131 F1: 8.626349168914178 EM: 4.8419448010269575\n",
            "--- Step: 3600 Loss: 9.568630897811872 Start Acc: 9.552171290711701 End Acc: 6.261308805790109 Acc: 2.646260554885404 F1: 8.62879183592506 EM: 4.851477683956574\n",
            "--- Step: 3700 Loss: 9.572366661857284 Start Acc: 9.543870875995449 End Acc: 6.258887940841865 Acc: 2.63971843003413 F1: 8.593202723458976 EM: 4.849260523321957\n",
            "--- Step: 3800 Loss: 9.577362190150085 Start Acc: 9.494416038751346 End Acc: 6.251681916038751 Acc: 2.613697524219591 F1: 8.56311153082513 EM: 4.823735199138859\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 6 Loss: 9.579718955693266 Start Acc: 9.499605925592318 End Acc: 6.266587315629473 Acc: 2.621801161313152 F1: 8.562087504008161 EM: 4.831446217189913\n",
            "--- Step: 3900 Loss: 9.198715845743815 Start Acc: 9.791666666666666 End Acc: 5.625 Acc: 2.9166666666666665 F1: 8.318277674327332 EM: 4.166666666666667\n",
            "--- Step: 4000 Loss: 9.42216448991195 Start Acc: 9.103260869565217 End Acc: 7.010869565217392 Acc: 2.391304347826087 F1: 8.052774576253913 EM: 4.375\n",
            "--- Step: 4100 Loss: 9.425524460992147 Start Acc: 9.418604651162791 End Acc: 6.9476744186046515 Acc: 2.5 F1: 8.272218897299876 EM: 4.549418604651163\n",
            "--- Step: 4200 Loss: 9.449022235567607 Start Acc: 9.593253968253968 End Acc: 6.726190476190476 Acc: 2.5198412698412698 F1: 8.296285155314312 EM: 4.593253968253968\n",
            "--- Step: 4300 Loss: 9.421716079941715 Start Acc: 9.593373493975903 End Acc: 6.596385542168675 Acc: 2.5150602409638556 F1: 8.375478227210438 EM: 4.631024096385542\n",
            "--- Step: 4400 Loss: 9.431692546548195 Start Acc: 9.629854368932039 End Acc: 6.601941747572815 Acc: 2.5849514563106797 F1: 8.406081646421319 EM: 4.654126213592233\n",
            "--- Step: 4500 Loss: 9.43732381138375 Start Acc: 9.690040650406504 End Acc: 6.544715447154472 Acc: 2.627032520325203 F1: 8.524449098427946 EM: 4.735772357723577\n",
            "--- Step: 4600 Loss: 9.44601660508376 Start Acc: 9.746503496503497 End Acc: 6.534090909090909 Acc: 2.6267482517482517 F1: 8.529339319571454 EM: 4.750874125874126\n",
            "--- Step: 4700 Loss: 9.452326532375594 Start Acc: 9.674079754601227 End Acc: 6.583588957055214 Acc: 2.638036809815951 F1: 8.57677403574962 EM: 4.792944785276074\n",
            "--- Step: 4800 Loss: 9.46829727412573 Start Acc: 9.69603825136612 End Acc: 6.577868852459017 Acc: 2.6331967213114753 F1: 8.572058169358769 EM: 4.791666666666667\n",
            "--- Step: 4900 Loss: 9.47051273120448 Start Acc: 9.750615763546797 End Acc: 6.594827586206897 Acc: 2.6416256157635467 F1: 8.590968512404693 EM: 4.775246305418719\n",
            "--- Step: 5000 Loss: 9.476628986495493 Start Acc: 9.784192825112108 End Acc: 6.558295964125561 Acc: 2.6569506726457397 F1: 8.580808041193803 EM: 4.778587443946188\n",
            "--- Step: 5100 Loss: 9.473162499573005 Start Acc: 9.768518518518519 End Acc: 6.54320987654321 Acc: 2.685185185185185 F1: 8.632798130844767 EM: 4.840534979423868\n",
            "--- Step: 5200 Loss: 9.47167801367466 Start Acc: 9.866920152091256 End Acc: 6.58745247148289 Acc: 2.7566539923954374 F1: 8.717483036682456 EM: 4.912072243346008\n",
            "--- Step: 5300 Loss: 9.464478023987356 Start Acc: 9.942579505300353 End Acc: 6.614399293286219 Acc: 2.7782685512367493 F1: 8.751486969915582 EM: 4.91386925795053\n",
            "--- Step: 5400 Loss: 9.472892438303125 Start Acc: 9.92986798679868 End Acc: 6.613036303630363 Acc: 2.7681518151815183 F1: 8.758207574538458 EM: 4.9216171617161715\n",
            "--- Step: 5500 Loss: 9.462837027027142 Start Acc: 10.019349845201239 End Acc: 6.654411764705882 Acc: 2.801857585139319 F1: 8.82576356874397 EM: 4.970975232198143\n",
            "--- Step: 5600 Loss: 9.468960062199361 Start Acc: 10.00911078717201 End Acc: 6.612609329446064 Acc: 2.7824344023323615 F1: 8.809491244500643 EM: 4.961734693877551\n",
            "--- Step: 5700 Loss: 9.467825655503706 Start Acc: 9.96728650137741 End Acc: 6.608126721763085 Acc: 2.7737603305785123 F1: 8.800129441044376 EM: 4.962121212121212\n",
            "--- Step: 5800 Loss: 9.464586467095517 Start Acc: 9.95757180156658 End Acc: 6.630221932114883 Acc: 2.7806788511749345 F1: 8.809675377541115 EM: 4.98041775456919\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 7 Loss: 9.468362914982126 Start Acc: 9.933892007527625 End Acc: 6.644577053610204 Acc: 2.779430924385968 F1: 8.796375972300513 EM: 4.973277768246889\n",
            "--- Step: 5900 Loss: 9.322584052880606 Start Acc: 10.416666666666666 End Acc: 6.770833333333333 Acc: 2.8211805555555554 F1: 9.592439214994194 EM: 5.46875\n",
            "--- Step: 6000 Loss: 9.346674447835879 Start Acc: 10.592296511627907 End Acc: 6.9585755813953485 Acc: 2.9978197674418605 F1: 9.760468544599625 EM: 5.650436046511628\n",
            "--- Step: 6100 Loss: 9.352384490125319 Start Acc: 10.650275735294118 End Acc: 7.2380514705882355 Acc: 3.1135110294117645 F1: 9.856122427038688 EM: 5.8708639705882355\n",
            "--- Step: 6200 Loss: 9.36695804262674 Start Acc: 10.63508064516129 End Acc: 7.048051075268817 Acc: 3.0493951612903225 F1: 9.647698598109345 EM: 5.687163978494624\n",
            "--- Step: 6300 Loss: 9.338532204345121 Start Acc: 10.805084745762711 End Acc: 7.117319915254237 Acc: 3.1183792372881354 F1: 9.69531456935007 EM: 5.713718220338983\n",
            "--- Step: 6400 Loss: 9.335911315637869 Start Acc: 10.680725524475525 End Acc: 6.9438374125874125 Acc: 2.9938811188811187 F1: 9.517950387052277 EM: 5.567089160839161\n",
            "--- Step: 6500 Loss: 9.344835062112127 Start Acc: 10.709635416666666 End Acc: 6.914992559523809 Acc: 3.0459449404761907 F1: 9.579802029308867 EM: 5.645461309523809\n",
            "--- Step: 6600 Loss: 9.341569947455213 Start Acc: 10.654145077720207 End Acc: 6.897668393782383 Acc: 3.0642810880829017 F1: 9.58681080976743 EM: 5.654954663212435\n",
            "--- Step: 6700 Loss: 9.338182402313302 Start Acc: 10.622133027522937 End Acc: 6.837729357798165 Acc: 3.0246559633027523 F1: 9.545592529334012 EM: 5.60493119266055\n",
            "--- Step: 6800 Loss: 9.339141528302259 Start Acc: 10.535622427983538 End Acc: 6.838348765432099 Acc: 3.015689300411523 F1: 9.478220342494659 EM: 5.574845679012346\n",
            "--- Step: 6900 Loss: 9.35072472780498 Start Acc: 10.511893656716419 End Acc: 6.859258395522388 Acc: 2.98798973880597 F1: 9.391735586739854 EM: 5.486240671641791\n",
            "--- Step: 7000 Loss: 9.348461142579037 Start Acc: 10.502879692832764 End Acc: 6.884598976109215 Acc: 2.9756825938566553 F1: 9.359612955771732 EM: 5.47141638225256\n",
            "--- Step: 7100 Loss: 9.346437772864816 Start Acc: 10.460888364779874 End Acc: 6.910868710691824 Acc: 2.957940251572327 F1: 9.3726723040954 EM: 5.46875\n",
            "--- Step: 7200 Loss: 9.348428260133149 Start Acc: 10.399963556851311 End Acc: 6.935586734693878 Acc: 2.9473396501457727 F1: 9.32816059603887 EM: 5.425473760932944\n",
            "--- Step: 7300 Loss: 9.34794230921113 Start Acc: 10.425866168478262 End Acc: 6.9802989130434785 Acc: 2.987007472826087 F1: 9.350738095768973 EM: 5.451766304347826\n",
            "--- Step: 7400 Loss: 9.352297969750168 Start Acc: 10.434557888040713 End Acc: 6.9517334605597965 Acc: 2.9818702290076335 F1: 9.366190373930985 EM: 5.448870865139949\n",
            "--- Step: 7500 Loss: 9.356844791670165 Start Acc: 10.438471889952153 End Acc: 6.958358253588517 Acc: 2.9680023923444976 F1: 9.355763310342313 EM: 5.455666866028708\n",
            "--- Step: 7600 Loss: 9.352559292020432 Start Acc: 10.443707674943566 End Acc: 6.97834367945824 Acc: 2.9909706546275396 F1: 9.396171257632641 EM: 5.496966704288939\n",
            "--- Step: 7700 Loss: 9.360886212852266 Start Acc: 10.416666666666666 End Acc: 6.907719017094017 Acc: 2.96474358974359 F1: 9.342014999419769 EM: 5.463741987179487\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 8 Loss: 9.36445567881515 Start Acc: 10.379437358253849 End Acc: 6.88745556609995 Acc: 2.9531453571600905 F1: 9.323188259166368 EM: 5.432249671184309\n",
            "--- Step: 7800 Loss: 9.150392811873864 Start Acc: 12.176724137931034 End Acc: 8.297413793103448 Acc: 3.3405172413793105 F1: 9.80699062060677 EM: 5.711206896551724\n",
            "--- Step: 7900 Loss: 9.23071591429008 Start Acc: 11.676356589147288 End Acc: 7.340116279069767 Acc: 3.125 F1: 9.787824621186077 EM: 5.838178294573644\n",
            "--- Step: 8000 Loss: 9.206790620062549 Start Acc: 11.285480349344978 End Acc: 7.218886462882096 Acc: 3.125 F1: 9.979801146810411 EM: 5.93613537117904\n",
            "--- Step: 8100 Loss: 9.252525754250291 Start Acc: 10.989741641337385 End Acc: 6.886398176291793 Acc: 2.992021276595745 F1: 9.827231154613441 EM: 5.851063829787234\n",
            "--- Step: 8200 Loss: 9.223809749096423 Start Acc: 11.101398601398602 End Acc: 7.233391608391608 Acc: 3.2124125874125875 F1: 10.217306747554305 EM: 6.148018648018648\n",
            "--- Step: 8300 Loss: 9.233709969908167 Start Acc: 10.934546313799622 End Acc: 7.195179584120983 Acc: 3.1427221172022684 F1: 9.952860580641055 EM: 5.948724007561436\n",
            "--- Step: 8400 Loss: 9.247570163684355 Start Acc: 10.925079491255962 End Acc: 7.218799682034976 Acc: 3.1746820349761524 F1: 9.898577709245888 EM: 5.927066772655008\n",
            "--- Step: 8500 Loss: 9.233470484389526 Start Acc: 10.973936899862826 End Acc: 7.265946502057613 Acc: 3.2621742112482854 F1: 9.924441287526477 EM: 5.958504801097393\n",
            "--- Step: 8600 Loss: 9.23456809949242 Start Acc: 11.060012062726177 End Acc: 7.256483715319662 Acc: 3.2984016887816647 F1: 9.877274400139301 EM: 5.940892641737032\n",
            "--- Step: 8700 Loss: 9.231971267478459 Start Acc: 11.016550053821312 End Acc: 7.333153928955866 Acc: 3.310010764262648 F1: 9.906295634698372 EM: 5.9607104413347685\n",
            "--- Step: 8800 Loss: 9.209205453666932 Start Acc: 11.11212342079689 End Acc: 7.370626822157434 Acc: 3.3375850340136055 F1: 9.988064615153021 EM: 6.013119533527697\n",
            "--- Step: 8900 Loss: 9.224124033753062 Start Acc: 10.963795394154118 End Acc: 7.371014171833481 Acc: 3.280004428697963 F1: 9.910969117452993 EM: 5.945527015057573\n",
            "--- Step: 9000 Loss: 9.226793696183902 Start Acc: 10.956570382424735 End Acc: 7.391680227827502 Acc: 3.3106183889340928 F1: 9.934118602531912 EM: 5.955044751830757\n",
            "--- Step: 9100 Loss: 9.233070494089922 Start Acc: 10.966892400300978 End Acc: 7.371613995485327 Acc: 3.2895974416854776 F1: 9.875040060536442 EM: 5.91375094055681\n",
            "--- Step: 9200 Loss: 9.24080872619127 Start Acc: 10.940780265920225 End Acc: 7.378411476557033 Acc: 3.275892232330301 F1: 9.851509147922323 EM: 5.8826102169349195\n",
            "--- Step: 9300 Loss: 9.250661537177894 Start Acc: 10.864944408109876 End Acc: 7.378188358404186 Acc: 3.2394538914323086 F1: 9.778679614282 EM: 5.824885546108567\n",
            "--- Step: 9400 Loss: 9.246373668925903 Start Acc: 10.842541436464089 End Acc: 7.349217311233886 Acc: 3.2170810313075506 F1: 9.79026768515918 EM: 5.804941682013506\n",
            "--- Step: 9500 Loss: 9.248139041086509 Start Acc: 10.838996529786003 End Acc: 7.31456043956044 Acc: 3.2244071717755927 F1: 9.76910732231058 EM: 5.789112203585888\n",
            "--- Step: 9600 Loss: 9.252728409295267 Start Acc: 10.8529250956807 End Acc: 7.2905276107162384 Acc: 3.2155549480590486 F1: 9.738013751648788 EM: 5.761344997266265\n",
            "--- Step: 9700 Loss: 9.259069391332678 Start Acc: 10.8112048993082 End Acc: 7.222591254475641 Acc: 3.1916789527404696 F1: 9.688584475162136 EM: 5.7137765681700365\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 9 Loss: 9.261246658144572 Start Acc: 10.805681105338502 End Acc: 7.213974361036496 Acc: 3.192806935709575 F1: 9.679023169465053 EM: 5.704773546062789\n",
            "--- Step: 9800 Loss: 9.11818133398544 Start Acc: 11.228197674418604 End Acc: 8.030523255813954 Acc: 3.45203488372093 F1: 10.381695658038025 EM: 6.504360465116279\n",
            "--- Step: 9900 Loss: 9.140506421366046 Start Acc: 10.618279569892474 End Acc: 8.182123655913978 Acc: 3.5618279569892475 F1: 9.88725785439129 EM: 6.199596774193548\n",
            "--- Step: 10000 Loss: 9.126793021088714 Start Acc: 10.850087412587413 End Acc: 8.107517482517483 Acc: 3.5729895104895104 F1: 10.009143812429746 EM: 6.25\n",
            "--- Step: 10100 Loss: 9.131714776389957 Start Acc: 11.212759067357513 End Acc: 8.071567357512953 Acc: 3.562176165803109 F1: 9.964026260209026 EM: 6.063795336787565\n",
            "--- Step: 10200 Loss: 9.123396722377572 Start Acc: 11.439043209876543 End Acc: 8.031121399176955 Acc: 3.568672839506173 F1: 10.130004219640776 EM: 6.147119341563786\n",
            "--- Step: 10300 Loss: 9.115250241634381 Start Acc: 11.310793515358363 End Acc: 8.041808873720136 Acc: 3.5409556313993176 F1: 10.131842907484195 EM: 6.127346416382252\n",
            "--- Step: 10400 Loss: 9.142747712899922 Start Acc: 11.18804664723032 End Acc: 7.971938775510204 Acc: 3.421100583090379 F1: 10.078877794932444 EM: 6.017674927113703\n",
            "--- Step: 10500 Loss: 9.13093743797477 Start Acc: 11.227735368956743 End Acc: 7.848282442748092 Acc: 3.4072837150127224 F1: 10.117206965167844 EM: 6.03132951653944\n",
            "--- Step: 10600 Loss: 9.143365078532131 Start Acc: 11.131489841986456 End Acc: 7.847770880361174 Acc: 3.4071670428893905 F1: 10.05694679259511 EM: 5.999576749435666\n",
            "--- Step: 10700 Loss: 9.124297496996835 Start Acc: 11.222743407707911 End Acc: 7.872718052738337 Acc: 3.438767748478702 F1: 10.132786270916347 EM: 6.072515212981744\n",
            "--- Step: 10800 Loss: 9.13413127658556 Start Acc: 11.225253222836097 End Acc: 7.766459484346225 Acc: 3.383977900552486 F1: 10.099740246025718 EM: 6.00828729281768\n",
            "--- Step: 10900 Loss: 9.134423872270633 Start Acc: 11.258958684654301 End Acc: 7.783516020236088 Acc: 3.383220910623946 F1: 10.149667104936029 EM: 6.026032883642496\n",
            "--- Step: 11000 Loss: 9.142104104740623 Start Acc: 11.226671850699844 End Acc: 7.780909797822706 Acc: 3.4190318818040435 F1: 10.134857005852844 EM: 6.024008553654744\n",
            "--- Step: 11100 Loss: 9.145827171950458 Start Acc: 11.187770562770563 End Acc: 7.756132756132756 Acc: 3.391053391053391 F1: 10.088067325644515 EM: 5.999729437229437\n",
            "--- Step: 11200 Loss: 9.153195628567948 Start Acc: 11.212987886944818 End Acc: 7.755720053835801 Acc: 3.3857671601615076 F1: 10.126389600980508 EM: 6.0060565275908475\n",
            "--- Step: 11300 Loss: 9.159985330937639 Start Acc: 11.173943883984867 End Acc: 7.751418663303909 Acc: 3.3673549810844894 F1: 10.078325829953183 EM: 5.968237704918033\n",
            "--- Step: 11400 Loss: 9.156772470700501 Start Acc: 11.198843416370106 End Acc: 7.745774021352313 Acc: 3.3529804270462633 F1: 10.125065544161503 EM: 5.994217081850533\n",
            "--- Step: 11500 Loss: 9.15511917186878 Start Acc: 11.231452967525197 End Acc: 7.746010638297872 Acc: 3.3594624860022395 F1: 10.12971541167706 EM: 6.008538633818589\n",
            "--- Step: 11600 Loss: 9.162346730429446 Start Acc: 11.184384941675503 End Acc: 7.711426299045599 Acc: 3.328804347826087 F1: 10.085358036648461 EM: 5.983231707317073\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 10 Loss: 9.158590144983469 Start Acc: 11.20136397999067 End Acc: 7.735117659358865 Acc: 3.345611297871998 F1: 10.085540345952252 EM: 5.991057642820381\n",
            "Loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a8fd808f64a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpt6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt6_glove'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrained_model_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpt6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expt6_glove_5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexpt6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expt6_glove'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-7697f1fe63e6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iterator, expt_name, load_model)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpt_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mevaluation_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'beta2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'delta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shared drives/CIS 700-1 Final Project/models/expt6_glove.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilpvLM_fpcnB",
        "colab_type": "text"
      },
      "source": [
        "## Expt 7 - Skip Memory + Verification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjSa4Sa3pcvq",
        "colab_type": "code",
        "outputId": "406c4c2f-1b7b-47d5-d82b-89a83ad6920d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hyperparameters7 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 3,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': True,\\\n",
        "                   'num_epochs': 10,\\\n",
        "                   'learning_rate': 2e-3,\\\n",
        "                   'loss_function': retrospective_loss_span,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt7 = Experiment(hyperparameters7, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt7_glove')\n",
        "trained_model_glove = expt7.train(train_iterator, expt_name='expt7_1_glove', load_model=False)\n",
        "expt7.evaluate(val_iterator, expt_name='expt7_glove', load_model=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Step: 0 Loss: 13.97938346862793 Start Acc: 0.0 End Acc: 0.0 Acc: 0.0 F1: 0.457353366416592 EM: 0.0\n",
            "--- Step: 100 Loss: 12.043832977219383 Start Acc: 1.7017326732673268 End Acc: 1.4232673267326732 Acc: 0.40222772277227725 F1: 2.7393515052951787 EM: 0.9900990099009901\n",
            "--- Step: 200 Loss: 11.777081128969714 Start Acc: 2.347636815920398 End Acc: 1.6480099502487562 Acc: 0.5130597014925373 F1: 2.9907315830741528 EM: 1.0883084577114428\n",
            "--- Step: 300 Loss: 11.650936963154232 Start Acc: 2.8654485049833887 End Acc: 1.7753322259136213 Acc: 0.550249169435216 F1: 3.079303979584413 EM: 1.058970099667774\n",
            "--- Step: 400 Loss: 11.55719116857819 Start Acc: 2.984725685785536 End Acc: 1.9560473815461346 Acc: 0.5610972568578554 F1: 3.104485228801713 EM: 1.0832294264339153\n",
            "--- Step: 500 Loss: 11.48647401432791 Start Acc: 3.2559880239520957 End Acc: 2.089570858283433 Acc: 0.592564870259481 F1: 3.217511413795408 EM: 1.1477045908183632\n",
            "--- Step: 600 Loss: 11.430453905051639 Start Acc: 3.3953826955074877 End Acc: 2.105865224625624 Acc: 0.5875623960066556 F1: 3.2982495639566225 EM: 1.1595257903494176\n",
            "--- Step: 700 Loss: 11.392471767866322 Start Acc: 3.4682596291012837 End Acc: 2.1799215406562054 Acc: 0.6151925820256776 F1: 3.3327255146866275 EM: 1.1991797432239657\n",
            "--- Step: 800 Loss: 11.358201160264224 Start Acc: 3.5580524344569286 End Acc: 2.2198813982521846 Acc: 0.6359238451935081 F1: 3.348868830348322 EM: 1.2016229712858926\n",
            "--- Step: 900 Loss: 11.32437252760198 Start Acc: 3.648723640399556 End Acc: 2.316870144284129 Acc: 0.6485849056603774 F1: 3.3906068742875033 EM: 1.234739178690344\n",
            "--- Step: 1000 Loss: 11.300544747344025 Start Acc: 3.7306443556443556 End Acc: 2.350774225774226 Acc: 0.6774475524475524 F1: 3.400801415997964 EM: 1.2487512487512487\n",
            "--- Step: 1100 Loss: 11.274784790614213 Start Acc: 3.786330608537693 End Acc: 2.3387829246139873 Acc: 0.6783605812897366 F1: 3.400268357939337 EM: 1.2602179836512262\n",
            "--- Step: 1200 Loss: 11.255230166731428 Start Acc: 3.806723563696919 End Acc: 2.3834304746044963 Acc: 0.6817235636969192 F1: 3.4280046148720267 EM: 1.2749791840133222\n",
            "--- Step: 1300 Loss: 11.229360444100429 Start Acc: 3.903247501921599 End Acc: 2.4452344350499615 Acc: 0.7085895465026902 F1: 3.4887579158562874 EM: 1.3066871637202153\n",
            "--- Step: 1400 Loss: 11.2079749189046 Start Acc: 3.981531049250535 End Acc: 2.4848322626695216 Acc: 0.7160064239828694 F1: 3.530833620005945 EM: 1.3249464668094217\n",
            "--- Step: 1500 Loss: 11.192515648658238 Start Acc: 4.009826782145237 End Acc: 2.5191538974017322 Acc: 0.7307628247834776 F1: 3.540402666073519 EM: 1.342854763491006\n",
            "--- Step: 1600 Loss: 11.17944932058407 Start Acc: 4.0580106183635225 End Acc: 2.5335727670206123 Acc: 0.747579637726421 F1: 3.5697968947911956 EM: 1.3546221111805121\n",
            "--- Step: 1700 Loss: 11.166442275958367 Start Acc: 4.074808935920047 End Acc: 2.5646678424456204 Acc: 0.7660934744268078 F1: 3.6061012764797464 EM: 1.3778659611992945\n",
            "--- Step: 1800 Loss: 11.153363036420993 Start Acc: 4.103622987229317 End Acc: 2.614866740699611 Acc: 0.7912270960577457 F1: 3.671274328136772 EM: 1.4245558023320377\n",
            "--- Step: 1900 Loss: 11.14418493252814 Start Acc: 4.137624934245134 End Acc: 2.620331404523935 Acc: 0.7923461336138874 F1: 3.6765442557499566 EM: 1.4252367175170964\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 1 Loss: 11.141425353377613 Start Acc: 4.141802448086729 End Acc: 2.6475366328352448 Acc: 0.7994080841549919 F1: 3.68133143171831 EM: 1.4268933112216695\n",
            "--- Step: 2000 Loss: 10.89567388337234 Start Acc: 4.9030172413793105 End Acc: 2.5323275862068964 Acc: 0.646551724137931 F1: 3.648815603699342 EM: 1.3469827586206897\n",
            "--- Step: 2100 Loss: 10.80145916154113 Start Acc: 5.379746835443038 End Acc: 3.6985759493670884 Acc: 1.4042721518987342 F1: 4.500803864361365 EM: 2.0371835443037973\n",
            "--- Step: 2200 Loss: 10.710762585780417 Start Acc: 5.171996124031008 End Acc: 3.694282945736434 Acc: 1.3202519379844961 F1: 4.452410481416543 EM: 1.937984496124031\n",
            "--- Step: 2300 Loss: 10.768484930752376 Start Acc: 5.097765363128492 End Acc: 3.482891061452514 Acc: 1.160963687150838 F1: 4.2852523904923325 EM: 1.833100558659218\n",
            "--- Step: 2400 Loss: 10.765348794678934 Start Acc: 5.0968886462882095 End Acc: 3.459334061135371 Acc: 1.105349344978166 F1: 4.232373351073951 EM: 1.8217794759825328\n",
            "--- Step: 2500 Loss: 10.769375619922487 Start Acc: 5.068324372759856 End Acc: 3.3490143369175627 Acc: 1.0864695340501793 F1: 4.163847747155452 EM: 1.80331541218638\n",
            "--- Step: 2600 Loss: 10.764994318362065 Start Acc: 5.005699088145897 End Acc: 3.314969604863222 Acc: 1.059080547112462 F1: 4.162100601130421 EM: 1.7952127659574468\n",
            "--- Step: 2700 Loss: 10.774964527593125 Start Acc: 5.021437994722955 End Acc: 3.3393799472295513 Acc: 1.0718997361477574 F1: 4.146726569602399 EM: 1.8057387862796834\n",
            "--- Step: 2800 Loss: 10.76672844786744 Start Acc: 5.077214452214452 End Acc: 3.3253205128205128 Acc: 1.078088578088578 F1: 4.180974849068532 EM: 1.8283799533799534\n",
            "--- Step: 2900 Loss: 10.770455430096526 Start Acc: 5.144180584551148 End Acc: 3.229384133611691 Acc: 1.0666753653444676 F1: 4.174612176350307 EM: 1.8234603340292275\n",
            "--- Step: 3000 Loss: 10.768235078606128 Start Acc: 5.109877126654064 End Acc: 3.263823251417769 Acc: 1.0810491493383743 F1: 4.215049409101318 EM: 1.851961247637051\n",
            "--- Step: 3100 Loss: 10.762467271511625 Start Acc: 5.197538860103627 End Acc: 3.3678756476683938 Acc: 1.1091321243523315 F1: 4.261193319907248 EM: 1.8809369602763386\n",
            "--- Step: 3200 Loss: 10.771924306933187 Start Acc: 5.144574721780604 End Acc: 3.3436009538950717 Acc: 1.1004570747217806 F1: 4.243598371650192 EM: 1.8556240063593006\n",
            "--- Step: 3300 Loss: 10.775495881008997 Start Acc: 5.182253313696613 End Acc: 3.3021907216494846 Acc: 1.0976620029455082 F1: 4.241291855916249 EM: 1.843243740795287\n",
            "--- Step: 3400 Loss: 10.775899853071886 Start Acc: 5.231910150891633 End Acc: 3.311471193415638 Acc: 1.1209705075445817 F1: 4.290989814251463 EM: 1.8882887517146776\n",
            "--- Step: 3500 Loss: 10.772991658175252 Start Acc: 5.237082798459563 End Acc: 3.3235718870346598 Acc: 1.1132060333761233 F1: 4.3124832314078585 EM: 1.881418485237484\n",
            "--- Step: 3600 Loss: 10.777659226671778 Start Acc: 5.235977080820265 End Acc: 3.3323281061519903 Acc: 1.1082629674306392 F1: 4.300601444234103 EM: 1.8772617611580218\n",
            "--- Step: 3700 Loss: 10.769563057316855 Start Acc: 5.243885096700796 End Acc: 3.359641638225256 Acc: 1.118102957906712 F1: 4.3510385632585935 EM: 1.9091296928327646\n",
            "--- Step: 3800 Loss: 10.770754194875579 Start Acc: 5.240850376749193 End Acc: 3.3318756727664156 Acc: 1.0915635091496232 F1: 4.327301490003564 EM: 1.8854278794402584\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 2 Loss: 10.767412267882248 Start Acc: 5.2291261198951275 End Acc: 3.319875826349906 Acc: 1.0873236718083994 F1: 4.354737080974789 EM: 1.8933112216694308\n",
            "--- Step: 3900 Loss: 10.65929037729899 Start Acc: 6.666666666666667 End Acc: 4.166666666666667 Acc: 1.875 F1: 4.671301788467503 EM: 2.2916666666666665\n",
            "--- Step: 4000 Loss: 10.554172209034796 Start Acc: 5.706521739130435 End Acc: 3.6684782608695654 Acc: 1.141304347826087 F1: 4.847541820610702 EM: 2.2282608695652173\n",
            "--- Step: 4100 Loss: 10.4894948227461 Start Acc: 6.031976744186046 End Acc: 3.8517441860465116 Acc: 1.177325581395349 F1: 4.992560542406451 EM: 2.354651162790698\n",
            "--- Step: 4200 Loss: 10.550580416785346 Start Acc: 5.892857142857143 End Acc: 3.8095238095238093 Acc: 1.1607142857142858 F1: 4.7687016574828744 EM: 2.2718253968253967\n",
            "--- Step: 4300 Loss: 10.561258795175208 Start Acc: 6.031626506024097 End Acc: 3.8328313253012047 Acc: 1.2048192771084338 F1: 4.850457985541947 EM: 2.3042168674698793\n",
            "--- Step: 4400 Loss: 10.57395128601963 Start Acc: 5.976941747572815 End Acc: 3.804611650485437 Acc: 1.2014563106796117 F1: 4.861132872131492 EM: 2.275485436893204\n",
            "--- Step: 4500 Loss: 10.562270479279805 Start Acc: 5.940040650406504 End Acc: 3.744918699186992 Acc: 1.1788617886178863 F1: 4.813258077656351 EM: 2.2154471544715446\n",
            "--- Step: 4600 Loss: 10.578436894850297 Start Acc: 6.000874125874126 End Acc: 3.8111888111888113 Acc: 1.2237762237762237 F1: 4.837005342751478 EM: 2.2115384615384617\n",
            "--- Step: 4700 Loss: 10.592916000108778 Start Acc: 5.843558282208589 End Acc: 3.811349693251534 Acc: 1.2001533742331287 F1: 4.723880529021698 EM: 2.166411042944785\n",
            "--- Step: 4800 Loss: 10.59685073029148 Start Acc: 5.8640710382513666 End Acc: 3.790983606557377 Acc: 1.215846994535519 F1: 4.7459709435467765 EM: 2.175546448087432\n",
            "--- Step: 4900 Loss: 10.593540062458057 Start Acc: 5.914408866995074 End Acc: 3.7746305418719213 Acc: 1.2222906403940887 F1: 4.736220115858752 EM: 2.164408866995074\n",
            "--- Step: 5000 Loss: 10.592421586844953 Start Acc: 5.930493273542601 End Acc: 3.7696188340807173 Acc: 1.2303811659192825 F1: 4.758382453311232 EM: 2.18609865470852\n",
            "--- Step: 5100 Loss: 10.59617382489114 Start Acc: 5.907921810699588 End Acc: 3.757716049382716 Acc: 1.2268518518518519 F1: 4.726916126092653 EM: 2.1707818930041154\n",
            "--- Step: 5200 Loss: 10.607235614065889 Start Acc: 5.895912547528517 End Acc: 3.7452471482889735 Acc: 1.2309885931558935 F1: 4.748517572680563 EM: 2.1958174904942966\n",
            "--- Step: 5300 Loss: 10.604191915559262 Start Acc: 5.9143109540636045 End Acc: 3.752208480565371 Acc: 1.245583038869258 F1: 4.768185881932047 EM: 2.2261484098939928\n",
            "--- Step: 5400 Loss: 10.608911073640629 Start Acc: 5.9055280528052805 End Acc: 3.7334983498349836 Acc: 1.2355610561056105 F1: 4.7441734730434675 EM: 2.2091584158415842\n",
            "--- Step: 5500 Loss: 10.608080934813886 Start Acc: 5.901702786377709 End Acc: 3.7422600619195046 Acc: 1.2345201238390093 F1: 4.744713555384516 EM: 2.196207430340557\n",
            "--- Step: 5600 Loss: 10.608643845983567 Start Acc: 5.934766763848397 End Acc: 3.75 Acc: 1.2481778425655976 F1: 4.758870607833603 EM: 2.2157434402332363\n",
            "--- Step: 5700 Loss: 10.608643548350688 Start Acc: 5.924586776859504 End Acc: 3.741391184573003 Acc: 1.2448347107438016 F1: 4.7510130552800796 EM: 2.1986914600550964\n",
            "--- Step: 5800 Loss: 10.617411370190254 Start Acc: 5.927376580987352 End Acc: 3.7421460628314973 Acc: 1.2435740514075888 F1: 4.745788955346373 EM: 2.2046344647519582\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 3 Loss: 10.615481440455289 Start Acc: 5.940068520692928 End Acc: 3.7316433707033827 Acc: 1.2449534348812148 F1: 4.746620248452193 EM: 2.2066392177045806\n",
            "--- Step: 5900 Loss: 10.531506405936348 Start Acc: 6.684027777777778 End Acc: 4.557291666666667 Acc: 1.6927083333333333 F1: 5.161692127186619 EM: 2.6909722222222223\n",
            "--- Step: 6000 Loss: 10.453788502271784 Start Acc: 6.595203488372093 End Acc: 4.505813953488372 Acc: 1.4353197674418605 F1: 5.075678314453303 EM: 2.34375\n",
            "--- Step: 6100 Loss: 10.467807489282945 Start Acc: 6.675091911764706 End Acc: 4.273897058823529 Acc: 1.3786764705882353 F1: 5.250556912875862 EM: 2.4471507352941178\n",
            "--- Step: 6200 Loss: 10.501477665798639 Start Acc: 6.451612903225806 End Acc: 4.158266129032258 Acc: 1.3860887096774193 F1: 5.144070056203709 EM: 2.40255376344086\n",
            "--- Step: 6300 Loss: 10.504220226053464 Start Acc: 6.4022775423728815 End Acc: 4.204184322033898 Acc: 1.3837394067796611 F1: 5.03798506992126 EM: 2.3503707627118646\n",
            "--- Step: 6400 Loss: 10.497312053933843 Start Acc: 6.424825174825175 End Acc: 4.261363636363637 Acc: 1.3603583916083917 F1: 5.020534607040114 EM: 2.3109702797202796\n",
            "--- Step: 6500 Loss: 10.520673847624234 Start Acc: 6.273251488095238 End Acc: 4.245721726190476 Acc: 1.329985119047619 F1: 4.949778466409624 EM: 2.24609375\n",
            "--- Step: 6600 Loss: 10.518992336920507 Start Acc: 6.24595207253886 End Acc: 4.274611398963731 Acc: 1.323672279792746 F1: 4.9525856953480645 EM: 2.254695595854922\n",
            "--- Step: 6700 Loss: 10.532205599710482 Start Acc: 6.24283256880734 End Acc: 4.275372706422019 Acc: 1.3725630733944953 F1: 4.970972339376483 EM: 2.289994266055046\n",
            "--- Step: 6800 Loss: 10.509886296688284 Start Acc: 6.311085390946502 End Acc: 4.279192386831276 Acc: 1.3953189300411524 F1: 5.03946143421614 EM: 2.346965020576132\n",
            "--- Step: 6900 Loss: 10.514037629116826 Start Acc: 6.282066231343284 End Acc: 4.244402985074627 Acc: 1.3701026119402986 F1: 4.980599105708388 EM: 2.3116837686567164\n",
            "--- Step: 7000 Loss: 10.510653156876158 Start Acc: 6.234001706484642 End Acc: 4.202218430034129 Acc: 1.3225255972696246 F1: 4.9366281705147745 EM: 2.2690912969283277\n",
            "--- Step: 7100 Loss: 10.520513987016377 Start Acc: 6.1812106918239 End Acc: 4.18877751572327 Acc: 1.2897995283018868 F1: 4.879513075136759 EM: 2.233195754716981\n",
            "--- Step: 7200 Loss: 10.510533583407499 Start Acc: 6.23405612244898 End Acc: 4.220572157434402 Acc: 1.3119533527696794 F1: 4.919014463940309 EM: 2.2754190962099123\n",
            "--- Step: 7300 Loss: 10.50891473468231 Start Acc: 6.209663722826087 End Acc: 4.233186141304348 Acc: 1.3119904891304348 F1: 4.9212441217773355 EM: 2.277938179347826\n",
            "--- Step: 7400 Loss: 10.513270940792772 Start Acc: 6.198314249363868 End Acc: 4.232267811704834 Acc: 1.3100349872773538 F1: 4.9313540507158695 EM: 2.2880884223918576\n",
            "--- Step: 7500 Loss: 10.512176365658426 Start Acc: 6.21261961722488 End Acc: 4.240804425837321 Acc: 1.3064443779904307 F1: 4.918752605170468 EM: 2.267120215311005\n",
            "--- Step: 7600 Loss: 10.513617505069242 Start Acc: 6.241182279909706 End Acc: 4.26954006772009 Acc: 1.3314757336343115 F1: 4.949148656075616 EM: 2.299661399548533\n",
            "--- Step: 7700 Loss: 10.515511870638937 Start Acc: 6.234975961538462 End Acc: 4.263488247863248 Acc: 1.3354700854700854 F1: 4.948265305299769 EM: 2.3036858974358974\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 4 Loss: 10.51577402326905 Start Acc: 6.2569365138086885 End Acc: 4.27047980569719 Acc: 1.354329188850107 F1: 4.96542108158364 EM: 2.322439526505404\n",
            "--- Step: 7800 Loss: 10.349924942542767 Start Acc: 7.112068965517241 End Acc: 4.202586206896552 Acc: 1.4008620689655173 F1: 6.340680740206563 EM: 3.3405172413793105\n",
            "--- Step: 7900 Loss: 10.376151576522709 Start Acc: 7.194767441860465 End Acc: 4.505813953488372 Acc: 1.5261627906976745 F1: 5.444513992413011 EM: 2.688953488372093\n",
            "--- Step: 8000 Loss: 10.382264235134208 Start Acc: 6.932314410480349 End Acc: 4.503275109170306 Acc: 1.4601528384279476 F1: 5.267262910575575 EM: 2.5245633187772927\n",
            "--- Step: 8100 Loss: 10.426050493420076 Start Acc: 6.696428571428571 End Acc: 4.378799392097265 Acc: 1.3677811550151975 F1: 5.258833139168212 EM: 2.526595744680851\n",
            "--- Step: 8200 Loss: 10.396498647880998 Start Acc: 6.75990675990676 End Acc: 4.348776223776224 Acc: 1.3767482517482517 F1: 5.165742712669913 EM: 2.4694055944055946\n",
            "--- Step: 8300 Loss: 10.38507653634805 Start Acc: 6.823015122873346 End Acc: 4.5073251417769375 Acc: 1.482750472589792 F1: 5.279785877277024 EM: 2.5874291115311907\n",
            "--- Step: 8400 Loss: 10.405753442706667 Start Acc: 6.791534181240063 End Acc: 4.381955484896661 Acc: 1.4705882352941178 F1: 5.251794472038714 EM: 2.573529411764706\n",
            "--- Step: 8500 Loss: 10.415189996831865 Start Acc: 6.76440329218107 End Acc: 4.3681412894375855 Acc: 1.478909465020576 F1: 5.308092762842691 EM: 2.5848765432098766\n",
            "--- Step: 8600 Loss: 10.413433989512187 Start Acc: 6.826749095295537 End Acc: 4.350120627261761 Acc: 1.5266887816646562 F1: 5.337055920364773 EM: 2.5972557297949335\n",
            "--- Step: 8700 Loss: 10.41335503995996 Start Acc: 6.791576964477933 End Acc: 4.376345532831001 Acc: 1.5204520990312163 F1: 5.346592543843306 EM: 2.593514531754575\n",
            "--- Step: 8800 Loss: 10.421675846574374 Start Acc: 6.915087463556851 End Acc: 4.385325558794946 Acc: 1.5427599611273082 F1: 5.324380056766218 EM: 2.575315840621963\n",
            "--- Step: 8900 Loss: 10.42366840229085 Start Acc: 6.878321523472099 End Acc: 4.467449069973428 Acc: 1.5555801594331267 F1: 5.350449439930765 EM: 2.5990921169176264\n",
            "--- Step: 9000 Loss: 10.4265998680481 Start Acc: 6.8348250610252235 End Acc: 4.4522986167615946 Acc: 1.5485150528885272 F1: 5.3607896304150175 EM: 2.6088283157038243\n",
            "--- Step: 9100 Loss: 10.42744690282978 Start Acc: 6.835496613995486 End Acc: 4.469996237772762 Acc: 1.5472159518434914 F1: 5.379123453241574 EM: 2.6123965387509407\n",
            "--- Step: 9200 Loss: 10.421370277711777 Start Acc: 6.916987403778866 End Acc: 4.513645906228132 Acc: 1.561406578026592 F1: 5.4091256561785395 EM: 2.6417074877536737\n",
            "--- Step: 9300 Loss: 10.435103975299763 Start Acc: 6.857014388489208 End Acc: 4.504578155657292 Acc: 1.5757848266841072 F1: 5.386520710970708 EM: 2.644702419882276\n",
            "--- Step: 9400 Loss: 10.431888007180978 Start Acc: 6.848526703499079 End Acc: 4.487031921424187 Acc: 1.5557857581338244 F1: 5.365761508088846 EM: 2.6243093922651934\n",
            "--- Step: 9500 Loss: 10.431594790441581 Start Acc: 6.850057836899942 End Acc: 4.509470792365529 Acc: 1.5706333140543667 F1: 5.361357467719992 EM: 2.6370011567379987\n",
            "--- Step: 9600 Loss: 10.4403809747649 Start Acc: 6.789912520503007 End Acc: 4.488449972662657 Acc: 1.554811372334609 F1: 5.3100376817443955 EM: 2.6072990705303445\n",
            "--- Step: 9700 Loss: 10.435688946270584 Start Acc: 6.806214863179042 End Acc: 4.5315360562513165 Acc: 1.5747776355653484 F1: 5.321881162845134 EM: 2.611456713322965\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 5 Loss: 10.43866244358803 Start Acc: 6.789339080921973 End Acc: 4.52300912000772 Acc: 1.5650383619372377 F1: 5.311429799850353 EM: 2.6038986103962944\n",
            "--- Step: 9800 Loss: 10.205050640327986 Start Acc: 8.212209302325581 End Acc: 4.905523255813954 Acc: 1.8531976744186047 F1: 5.6843908833686605 EM: 2.79796511627907\n",
            "--- Step: 9900 Loss: 10.221353823138822 Start Acc: 8.030913978494624 End Acc: 4.922715053763441 Acc: 1.7641129032258065 F1: 5.598831602890416 EM: 2.872983870967742\n",
            "--- Step: 10000 Loss: 10.279620567401805 Start Acc: 7.7250874125874125 End Acc: 4.709353146853147 Acc: 1.7591783216783217 F1: 5.513374436408585 EM: 2.8299825174825175\n",
            "--- Step: 10100 Loss: 10.296490529658263 Start Acc: 7.302461139896373 End Acc: 4.69559585492228 Acc: 1.6110751295336787 F1: 5.488690202679776 EM: 2.7525906735751295\n",
            "--- Step: 10200 Loss: 10.289625837969682 Start Acc: 7.516718106995885 End Acc: 4.8032407407407405 Acc: 1.7296810699588476 F1: 5.652538635199015 EM: 2.880658436213992\n",
            "--- Step: 10300 Loss: 10.297596447296923 Start Acc: 7.449872013651877 End Acc: 4.788822525597269 Acc: 1.727815699658703 F1: 5.624087652304787 EM: 2.8370307167235493\n",
            "--- Step: 10400 Loss: 10.301637436836176 Start Acc: 7.48451166180758 End Acc: 4.833272594752186 Acc: 1.6809402332361516 F1: 5.613696391206237 EM: 2.828899416909621\n",
            "--- Step: 10500 Loss: 10.318069474387714 Start Acc: 7.391062340966921 End Acc: 4.814726463104326 Acc: 1.6499681933842238 F1: 5.588362036647636 EM: 2.7989821882951653\n",
            "--- Step: 10600 Loss: 10.334380649282485 Start Acc: 7.3151805869074495 End Acc: 4.775677200902934 Acc: 1.6718397291196387 F1: 5.591047474417654 EM: 2.818143340857788\n",
            "--- Step: 10700 Loss: 10.335987952853314 Start Acc: 7.302231237322515 End Acc: 4.776242393509127 Acc: 1.6956135902636917 F1: 5.611848055307452 EM: 2.8587728194726165\n",
            "--- Step: 10800 Loss: 10.329474217325284 Start Acc: 7.314686924493555 End Acc: 4.7997237569060776 Acc: 1.6862338858195212 F1: 5.619561358159627 EM: 2.848756906077348\n",
            "--- Step: 10900 Loss: 10.334697860891461 Start Acc: 7.288153456998313 End Acc: 4.824515177065767 Acc: 1.6968802698145025 F1: 5.629548492565857 EM: 2.8483347386172007\n",
            "--- Step: 11000 Loss: 10.341158162379525 Start Acc: 7.263316485225506 End Acc: 4.79442068429238 Acc: 1.6937208398133747 F1: 5.596319988469814 EM: 2.8358281493001556\n",
            "--- Step: 11100 Loss: 10.343400205884661 Start Acc: 7.228535353535354 End Acc: 4.791215728715729 Acc: 1.6662157287157287 F1: 5.552133861189918 EM: 2.786796536796537\n",
            "--- Step: 11200 Loss: 10.354275676154833 Start Acc: 7.204744279946164 End Acc: 4.771618438761776 Acc: 1.6613391655450875 F1: 5.51524162989031 EM: 2.769599596231494\n",
            "--- Step: 11300 Loss: 10.356250881697761 Start Acc: 7.164249684741488 End Acc: 4.766314627994956 Acc: 1.6275220680958387 F1: 5.5077819398181775 EM: 2.7388083228247164\n",
            "--- Step: 11400 Loss: 10.356373769674153 Start Acc: 7.148947212336892 End Acc: 4.759786476868327 Acc: 1.6088374851720046 F1: 5.486300078398576 EM: 2.715376631079478\n",
            "--- Step: 11500 Loss: 10.35768521673068 Start Acc: 7.130109182530795 End Acc: 4.722494400895856 Acc: 1.5869960806270997 F1: 5.4846871722267805 EM: 2.7103163493840987\n",
            "--- Step: 11600 Loss: 10.361664243113577 Start Acc: 7.118239660657476 End Acc: 4.725609756097561 Acc: 1.5939819724284199 F1: 5.48143830235349 EM: 2.705792682926829\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 6 Loss: 10.366277629422331 Start Acc: 7.120683276768911 End Acc: 4.688681217931189 Acc: 1.5779060976982837 F1: 5.463358004851302 EM: 2.690748841996912\n",
            "--- Step: 11700 Loss: 9.91293286168298 Start Acc: 8.212209302325581 End Acc: 6.468023255813954 Acc: 2.761627906976744 F1: 6.351158161232834 EM: 3.3430232558139537\n",
            "--- Step: 11800 Loss: 10.08935363476093 Start Acc: 8.020104895104895 End Acc: 6.031468531468532 Acc: 1.8356643356643356 F1: 5.918300668577886 EM: 2.819055944055944\n",
            "--- Step: 11900 Loss: 10.213087446895646 Start Acc: 7.87037037037037 End Acc: 5.73559670781893 Acc: 1.8518518518518519 F1: 5.754609016917825 EM: 2.7777777777777777\n",
            "--- Step: 12000 Loss: 10.229138606491311 Start Acc: 7.807944606413995 End Acc: 5.502915451895044 Acc: 1.776603498542274 F1: 5.6701112151947255 EM: 2.7332361516034984\n",
            "--- Step: 12100 Loss: 10.235303780833432 Start Acc: 7.717268623024831 End Acc: 5.283577878103838 Acc: 1.7071106094808126 F1: 5.714806535845748 EM: 2.744074492099323\n",
            "--- Step: 12200 Loss: 10.261953573420122 Start Acc: 7.556399631675875 End Acc: 5.127762430939226 Acc: 1.6862338858195212 F1: 5.63996754411135 EM: 2.6933701657458564\n",
            "--- Step: 12300 Loss: 10.283619525458542 Start Acc: 7.3435069984447905 End Acc: 5.078732503888025 Acc: 1.608670295489891 F1: 5.5821733400174 EM: 2.634136858475894\n",
            "--- Step: 12400 Loss: 10.260161312560214 Start Acc: 7.381393001345895 End Acc: 5.093371467025572 Acc: 1.6150740242261103 F1: 5.579639085699591 EM: 2.632907133243607\n",
            "--- Step: 12500 Loss: 10.290818518732507 Start Acc: 7.313908659549229 End Acc: 5.111951364175564 Acc: 1.6162514827995256 F1: 5.498479048377155 EM: 2.5911921708185055\n",
            "--- Step: 12600 Loss: 10.285314033039048 Start Acc: 7.340270413573701 End Acc: 5.133218451749735 Acc: 1.6304347826086956 F1: 5.54040613797202 EM: 2.621288441145281\n",
            "--- Step: 12700 Loss: 10.293841789004215 Start Acc: 7.334611697027804 End Acc: 5.084491850431448 Acc: 1.6239213806327901 F1: 5.5439556685170235 EM: 2.636625119846596\n",
            "--- Step: 12800 Loss: 10.284148006823015 Start Acc: 7.354549431321085 End Acc: 5.079833770778652 Acc: 1.6404199475065617 F1: 5.565406417936946 EM: 2.660214348206474\n",
            "--- Step: 12900 Loss: 10.296173912042201 Start Acc: 7.2983708769107 End Acc: 5.023129525341915 Acc: 1.6140386162510056 F1: 5.535922481522143 EM: 2.642296862429606\n",
            "--- Step: 13000 Loss: 10.293960376009512 Start Acc: 7.2831347728965 End Acc: 5.040022338049144 Acc: 1.617181682799702 F1: 5.543451056083238 EM: 2.6503164556962027\n",
            "--- Step: 13100 Loss: 10.298574428465823 Start Acc: 7.276507276507276 End Acc: 5.009095634095634 Acc: 1.609060984060984 F1: 5.531090246802992 EM: 2.655058905058905\n",
            "--- Step: 13200 Loss: 10.302600360581106 Start Acc: 7.2423849643551526 End Acc: 4.9882534024627345 Acc: 1.6080686973428386 F1: 5.543094647521903 EM: 2.6652624756966947\n",
            "--- Step: 13300 Loss: 10.312278127060818 Start Acc: 7.189592209373098 End Acc: 4.981360316494218 Acc: 1.605295191722459 F1: 5.529792641521363 EM: 2.664713937918442\n",
            "--- Step: 13400 Loss: 10.307378704904895 Start Acc: 7.184093516924842 End Acc: 5.000358577165806 Acc: 1.615390131956397 F1: 5.564593568371034 EM: 2.6875358577165804\n",
            "--- Step: 13500 Loss: 10.302862982095355 Start Acc: 7.241928920238741 End Acc: 4.990165491047206 Acc: 1.6125203472599023 F1: 5.57264623843769 EM: 2.6892295170916984\n",
            "--- Step: 13600 Loss: 10.311209121808156 Start Acc: 7.241318299528719 End Acc: 4.9669460037638125 Acc: 1.618117771951553 F1: 5.55867926884146 EM: 2.690748841996912\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 7 Loss: 10.311209121808156 Start Acc: 7.241318299528719 End Acc: 4.9669460037638125 Acc: 1.618117771951553 F1: 5.55867926884146 EM: 2.690748841996912\n",
            "--- Step: 13700 Loss: 10.186136837005614 Start Acc: 7.96875 End Acc: 5.4375 Acc: 1.78125 F1: 5.590791077962324 EM: 2.78125\n",
            "--- Step: 13800 Loss: 10.196696741580963 Start Acc: 7.640625 End Acc: 5.375 Acc: 1.734375 F1: 5.697171573822114 EM: 2.890625\n",
            "--- Step: 13900 Loss: 10.222243895530701 Start Acc: 7.53125 End Acc: 5.364583333333333 Acc: 1.71875 F1: 5.680795603276853 EM: 2.875\n",
            "--- Step: 14000 Loss: 10.215381503105164 Start Acc: 7.734375 End Acc: 5.3046875 Acc: 1.7578125 F1: 5.767733900092801 EM: 2.90625\n",
            "--- Step: 14100 Loss: 10.218923290252686 Start Acc: 7.7125 End Acc: 5.29375 Acc: 1.7375 F1: 5.724328639800196 EM: 2.84375\n",
            "--- Step: 14200 Loss: 10.225128731727601 Start Acc: 7.786458333333333 End Acc: 5.229166666666667 Acc: 1.6822916666666667 F1: 5.622426944199676 EM: 2.7552083333333335\n",
            "--- Step: 14300 Loss: 10.230286886351449 Start Acc: 7.78125 End Acc: 5.227678571428571 Acc: 1.6785714285714286 F1: 5.658491767872958 EM: 2.7723214285714284\n",
            "--- Step: 14400 Loss: 10.23550215780735 Start Acc: 7.640625 End Acc: 5.12890625 Acc: 1.60546875 F1: 5.599496502282908 EM: 2.7109375\n",
            "--- Step: 14500 Loss: 10.238745138380263 Start Acc: 7.645833333333333 End Acc: 5.232638888888889 Acc: 1.6215277777777777 F1: 5.601472162523355 EM: 2.7152777777777777\n",
            "--- Step: 14600 Loss: 10.237651777744293 Start Acc: 7.534375 End Acc: 5.178125 Acc: 1.5875 F1: 5.57376452311443 EM: 2.70625\n",
            "--- Step: 14700 Loss: 10.236428393884138 Start Acc: 7.548295454545454 End Acc: 5.119318181818182 Acc: 1.5909090909090908 F1: 5.5732294886667395 EM: 2.7073863636363638\n",
            "--- Step: 14800 Loss: 10.240847801764806 Start Acc: 7.541666666666667 End Acc: 5.065104166666667 Acc: 1.5989583333333333 F1: 5.539079254127737 EM: 2.6901041666666665\n",
            "--- Step: 14900 Loss: 10.24686074403616 Start Acc: 7.569711538461538 End Acc: 5.060096153846154 Acc: 1.6033653846153846 F1: 5.561717601495526 EM: 2.735576923076923\n",
            "--- Step: 15000 Loss: 10.248471930367606 Start Acc: 7.560267857142857 End Acc: 5.058035714285714 Acc: 1.609375 F1: 5.579156617030158 EM: 2.7410714285714284\n",
            "--- Step: 15100 Loss: 10.250761081695556 Start Acc: 7.5375 End Acc: 5.05625 Acc: 1.6270833333333334 F1: 5.56875712674872 EM: 2.722916666666667\n",
            "--- Step: 15200 Loss: 10.254775481820106 Start Acc: 7.5390625 End Acc: 5.080078125 Acc: 1.638671875 F1: 5.576755062923942 EM: 2.7421875\n",
            "--- Step: 15300 Loss: 10.256268665089326 Start Acc: 7.538602941176471 End Acc: 5.082720588235294 Acc: 1.6452205882352942 F1: 5.5965747298545345 EM: 2.7536764705882355\n",
            "--- Step: 15400 Loss: 10.261312282085418 Start Acc: 7.529513888888889 End Acc: 5.046875 Acc: 1.6319444444444444 F1: 5.548352181182943 EM: 2.732638888888889\n",
            "--- Step: 15500 Loss: 10.261799011732403 Start Acc: 7.498355263157895 End Acc: 5.036184210526316 Acc: 1.643092105263158 F1: 5.557715752050994 EM: 2.7483552631578947\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 8 Loss: 10.262623081855166 Start Acc: 7.485805278988596 End Acc: 5.028067748628782 Acc: 1.639027842563253 F1: 5.542093115184506 EM: 2.7389989706639217\n",
            "--- Step: 15600 Loss: 9.956737593600625 Start Acc: 8.168859649122806 End Acc: 7.072368421052632 Acc: 2.412280701754386 F1: 6.723817521244548 EM: 3.7828947368421053\n",
            "--- Step: 15700 Loss: 10.048657875911445 Start Acc: 8.061305732484076 End Acc: 6.110668789808917 Acc: 1.8312101910828025 F1: 6.151885176229796 EM: 3.2444267515923566\n",
            "--- Step: 15800 Loss: 10.105852782030514 Start Acc: 7.928015564202335 End Acc: 5.848735408560311 Acc: 1.7509727626459144 F1: 5.950150574583901 EM: 3.0885214007782102\n",
            "--- Step: 15900 Loss: 10.139761073916565 Start Acc: 7.860644257703081 End Acc: 5.899859943977591 Acc: 1.8557422969187676 F1: 6.0965823817921105 EM: 3.203781512605042\n",
            "--- Step: 16000 Loss: 10.14337582348212 Start Acc: 7.863785557986871 End Acc: 5.785010940919038 Acc: 1.887308533916849 F1: 6.130935700227499 EM: 3.1933807439824946\n",
            "--- Step: 16100 Loss: 10.159822077896694 Start Acc: 7.865798922800718 End Acc: 5.784335727109515 Acc: 1.90754039497307 F1: 6.103511621677881 EM: 3.1923249551166966\n",
            "--- Step: 16200 Loss: 10.16181579645003 Start Acc: 7.838660578386606 End Acc: 5.688736681887367 Acc: 1.893074581430746 F1: 6.079324025059394 EM: 3.172564687975647\n",
            "--- Step: 16300 Loss: 10.161854180823545 Start Acc: 7.826948480845442 End Acc: 5.6349075297225895 Acc: 1.8700462351387055 F1: 6.034289721505854 EM: 3.1332562747688244\n",
            "--- Step: 16400 Loss: 10.172677327203917 Start Acc: 7.879959159859976 End Acc: 5.513418903150525 Acc: 1.866977829638273 F1: 6.004204581806252 EM: 3.1359393232205366\n",
            "--- Step: 16500 Loss: 10.18762419938792 Start Acc: 7.804336468129572 End Acc: 5.492424242424242 Acc: 1.8547544409613375 F1: 5.961919049876115 EM: 3.108672936259143\n",
            "--- Step: 16600 Loss: 10.184138835322328 Start Acc: 7.7903263954588455 End Acc: 5.490184484389783 Acc: 1.8507568590350048 F1: 5.960276151714619 EM: 3.1043046357615895\n",
            "--- Step: 16700 Loss: 10.193648506055748 Start Acc: 7.797644770959378 End Acc: 5.447817631806396 Acc: 1.8582541054451167 F1: 5.966166107475307 EM: 3.133102852203976\n",
            "--- Step: 16800 Loss: 10.184937739239482 Start Acc: 7.865950676213206 End Acc: 5.4619132856006365 Acc: 1.8645584725536992 F1: 5.98894302417373 EM: 3.1473747016706444\n",
            "--- Step: 16900 Loss: 10.19388212697332 Start Acc: 7.8781319086219606 End Acc: 5.44629697862933 Acc: 1.8883566691230655 F1: 6.013333957362579 EM: 3.1595431098010316\n",
            "--- Step: 17000 Loss: 10.196472021350324 Start Acc: 7.890785861358957 End Acc: 5.454272477693891 Acc: 1.8767158544955387 F1: 5.989892390924952 EM: 3.137868908716541\n",
            "--- Step: 17100 Loss: 10.211655831444164 Start Acc: 7.825545921644188 End Acc: 5.4331245985870265 Acc: 1.8605491329479769 F1: 5.97192247957022 EM: 3.1169717405266537\n",
            "--- Step: 17200 Loss: 10.209974954843377 Start Acc: 7.804013277006638 End Acc: 5.431502715751358 Acc: 1.8576493663246831 F1: 5.972872408541518 EM: 3.1136843693421845\n",
            "--- Step: 17300 Loss: 10.218394829321802 Start Acc: 7.792046101309049 End Acc: 5.417615253272624 Acc: 1.8586368810472396 F1: 5.975718670438157 EM: 3.116107000569152\n",
            "--- Step: 17400 Loss: 10.210818968348176 Start Acc: 7.81334141087776 End Acc: 5.440562735595046 Acc: 1.8712977921378569 F1: 5.993520225348212 EM: 3.128365643511039\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 9 Loss: 10.217167215499446 Start Acc: 7.778546267552396 End Acc: 5.423750623280951 Acc: 1.860996284441299 F1: 5.957072477032343 EM: 3.0992665980442613\n",
            "--- Step: 17500 Loss: 10.668100902012416 Start Acc: 6.026785714285714 End Acc: 5.580357142857143 Acc: 1.1160714285714286 F1: 5.146970995573291 EM: 2.6785714285714284\n",
            "--- Step: 17600 Loss: 10.012846156170493 Start Acc: 8.991228070175438 End Acc: 6.551535087719298 Acc: 2.192982456140351 F1: 6.609326363339807 EM: 3.7280701754385963\n",
            "--- Step: 17700 Loss: 10.051106163274461 Start Acc: 8.498831775700934 End Acc: 6.25 Acc: 2.044392523364486 F1: 6.454548301403048 EM: 3.5776869158878504\n",
            "--- Step: 17800 Loss: 10.069122729028107 Start Acc: 8.280254777070065 End Acc: 5.9414808917197455 Acc: 1.910828025477707 F1: 6.207883035682328 EM: 3.314092356687898\n",
            "--- Step: 17900 Loss: 10.059211333592733 Start Acc: 8.257850241545894 End Acc: 5.932971014492754 Acc: 1.955012077294686 F1: 6.1892983506688095 EM: 3.328804347826087\n",
            "--- Step: 18000 Loss: 10.07826437374961 Start Acc: 8.298881322957198 End Acc: 5.897373540856031 Acc: 1.9880836575875487 F1: 6.233734516350278 EM: 3.3681906614785992\n",
            "--- Step: 18100 Loss: 10.103601555094269 Start Acc: 8.224755700325733 End Acc: 5.776669381107492 Acc: 1.9493078175895766 F1: 6.119566414520044 EM: 3.2471498371335503\n",
            "--- Step: 18200 Loss: 10.128493001147145 Start Acc: 8.25017507002801 End Acc: 5.632878151260504 Acc: 1.908263305322129 F1: 6.041583151539656 EM: 3.1862745098039214\n",
            "--- Step: 18300 Loss: 10.118732153051315 Start Acc: 8.146498771498772 End Acc: 5.58968058968059 Acc: 1.8811425061425062 F1: 6.007168010537581 EM: 3.159551597051597\n",
            "--- Step: 18400 Loss: 10.132237375434691 Start Acc: 8.099699124726477 End Acc: 5.579868708971554 Acc: 1.8702133479212253 F1: 5.980838908532786 EM: 3.111323851203501\n",
            "--- Step: 18500 Loss: 10.13656208548085 Start Acc: 8.049802761341223 End Acc: 5.525764299802761 Acc: 1.8521942800788955 F1: 5.943934822216531 EM: 3.0664447731755424\n",
            "--- Step: 18600 Loss: 10.12863912102992 Start Acc: 8.008864452423698 End Acc: 5.557114003590664 Acc: 1.854241472172352 F1: 5.95335625176789 EM: 3.0520646319569122\n",
            "--- Step: 18700 Loss: 10.129049692358178 Start Acc: 8.031301482701812 End Acc: 5.601317957166392 Acc: 1.8713962108731466 F1: 5.97731534232636 EM: 3.0863879736408566\n",
            "--- Step: 18800 Loss: 10.128796349558838 Start Acc: 8.031297564687975 End Acc: 5.619767884322679 Acc: 1.8835616438356164 F1: 6.003781553825248 EM: 3.1012176560121767\n",
            "--- Step: 18900 Loss: 10.147372161819371 Start Acc: 7.940682461103253 End Acc: 5.560466760961811 Acc: 1.852015558698727 F1: 5.963253811148008 EM: 3.0741690240452617\n",
            "--- Step: 19000 Loss: 10.153401734176999 Start Acc: 7.932215984147953 End Acc: 5.5399603698811095 Acc: 1.8267007926023777 F1: 5.950379946571744 EM: 3.0527575957727873\n",
            "--- Step: 19100 Loss: 10.157749577230385 Start Acc: 7.918990086741016 End Acc: 5.5665272614622054 Acc: 1.8200123915737298 F1: 5.937804195671124 EM: 3.0339993804213137\n",
            "--- Step: 19200 Loss: 10.163294101679478 Start Acc: 7.9237164527421236 End Acc: 5.517065344224037 Acc: 1.8049883313885648 F1: 5.915999407999672 EM: 3.0137835472578764\n",
            "--- Step: 19300 Loss: 10.161596888086892 Start Acc: 7.950316979051819 End Acc: 5.500620176405733 Acc: 1.8036797133406837 F1: 5.924570394233117 EM: 3.0285281146637266\n",
            "--- Step: 19400 Loss: 10.171680156464612 Start Acc: 7.890869905956113 End Acc: 5.466300940438871 Acc: 1.7927115987460815 F1: 5.882254282551662 EM: 2.994383490073145\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 10 Loss: 10.172950243986701 Start Acc: 7.897572823342073 End Acc: 5.467179231474482 Acc: 1.8030914735165913 F1: 5.884650614315231 EM: 3.002766340710242\n",
            "Loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c0dd6e33a6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpt7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt7_glove'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrained_model_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpt7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expt7_1_glove'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexpt7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expt7_glove'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-7697f1fe63e6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iterator, expt_name, load_model)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Shared drives/CIS 700-1 Final Project/models/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpt_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mevaluation_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'beta2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0;34m'delta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shared drives/CIS 700-1 Final Project/models/expt7_glove.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNLVwsKfXACo",
        "colab_type": "text"
      },
      "source": [
        "## Expt 8 - Reweighted loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6h9jtCIXASC",
        "colab_type": "code",
        "outputId": "5d2a090e-4183-4d90-f99b-cb7aba3dd16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hyperparameters8 = {'vocab_size': len(FIELD.vocab),\\\n",
        "                   'embedding_length': 300,\\\n",
        "                   'word_embeddings': FIELD.vocab.vectors,\\\n",
        "                   'vocab_itos':  FIELD.vocab.itos,\\\n",
        "                   'hidden_size': 300,\\\n",
        "                   'num_passes': 2,\\\n",
        "                   'sketchy_mode': 'word',\\\n",
        "                   'intensive_mode': 'similarity',\\\n",
        "                   'pool_mode': 'max',\\\n",
        "                   'skip_memory': False,\\\n",
        "                   'num_epochs': 10,\\\n",
        "                   'learning_rate': 1e-4,\\\n",
        "                   'loss_function': retrospective_parallel_loss,\\\n",
        "                   'bert_encoder': None,\\\n",
        "                   'bert_tokenizer': None,\\\n",
        "                   'beta1': 0.5,\\\n",
        "                   'beta2': 0.5,\\\n",
        "                   'lambda1': 0.5,\\\n",
        "                   'lambda2': 0.5,\\\n",
        "                   'delta': 0}\n",
        "\n",
        "expt8 = Experiment(hyperparameters8, log_dir='/content/drive/Shared drives/CIS 700-1 Final Project/runs/expt8_glove_saket_1')\n",
        "trained_model_glove = expt8.train(train_iterator, expt_name='expt8_glove_saket_6', load_model=True)\n",
        "expt8.evaluate(val_iterator, expt_name='expt8_glove_saket_9', load_model=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "--- Step: 0 Ext Loss: 0.6931471824645996 Final Loss: 1.7458192110061646 Start Acc: 0.0 End Acc: 0.0 Acc: 0.0 F1: 2.1151714564281243 EM: 9.375\n",
            "--- Step: 100 Ext Loss: 1.1392034917774767 Final Loss: 2.0794503712418058 Start Acc: 8.756188118811881 End Acc: 5.631188118811881 Acc: 2.4133663366336635 F1: 6.705342052717596 EM: 12.623762376237623\n",
            "--- Step: 200 Ext Loss: 1.1774140471842751 Final Loss: 2.1195582495399967 Start Acc: 8.255597014925373 End Acc: 5.301616915422885 Acc: 2.2388059701492535 F1: 6.734809265754014 EM: 12.873134328358208\n",
            "--- Step: 300 Ext Loss: 1.1722455191057781 Final Loss: 2.1208479638115514 Start Acc: 7.8903654485049834 End Acc: 5.4194352159468435 Acc: 2.1802325581395348 F1: 6.605015166811358 EM: 12.925664451827242\n",
            "--- Step: 400 Ext Loss: 1.1778647168318828 Final Loss: 2.1283392380003323 Start Acc: 7.761845386533666 End Acc: 5.3849750623441395 Acc: 2.158665835411471 F1: 6.498738810828334 EM: 13.037718204488778\n",
            "--- Step: 500 Ext Loss: 1.179770901769459 Final Loss: 2.12990041454871 Start Acc: 7.759481037924152 End Acc: 5.276946107784431 Acc: 2.095808383233533 F1: 6.402842717527786 EM: 12.849301397205588\n",
            "--- Step: 600 Ext Loss: 1.1749880042131648 Final Loss: 2.125143182852899 Start Acc: 7.783901830282862 End Acc: 5.2880615640599 Acc: 2.168261231281198 F1: 6.555102912789357 EM: 12.90557404326123\n",
            "--- Step: 700 Ext Loss: 1.1791704898213864 Final Loss: 2.129727026705395 Start Acc: 7.712196861626248 End Acc: 5.246968616262482 Acc: 2.1665477888730384 F1: 6.519710339634448 EM: 12.945791726105563\n",
            "--- Step: 800 Ext Loss: 1.180524353677414 Final Loss: 2.1309934022572454 Start Acc: 7.674001248439451 End Acc: 5.223938826466917 Acc: 2.1730649188514355 F1: 6.536067596278363 EM: 12.917446941323346\n",
            "--- Step: 900 Ext Loss: 1.1761448248377386 Final Loss: 2.127979773527774 Start Acc: 7.557574916759156 End Acc: 5.101970033296338 Acc: 2.1157047724750275 F1: 6.46742182714788 EM: 12.843368479467259\n",
            "--- Step: 1000 Ext Loss: 1.1767944603652267 Final Loss: 2.1261778575676185 Start Acc: 7.617382617382617 End Acc: 5.113636363636363 Acc: 2.1197552447552446 F1: 6.557637566313393 EM: 12.949550449550449\n",
            "--- Step: 1100 Ext Loss: 1.179293495216335 Final Loss: 2.1286703711959256 Start Acc: 7.572661217075386 End Acc: 5.072093551316985 Acc: 2.077656675749319 F1: 6.503723234186374 EM: 12.97683923705722\n",
            "--- Step: 1200 Ext Loss: 1.1788514776094867 Final Loss: 2.1280261054821157 Start Acc: 7.561407160699417 End Acc: 5.07910074937552 Acc: 2.086802664446295 F1: 6.535945607475586 EM: 12.976165695253956\n",
            "--- Step: 1300 Ext Loss: 1.176546041916005 Final Loss: 2.1252046134451734 Start Acc: 7.556687163720215 End Acc: 5.087432744043044 Acc: 2.084934665641814 F1: 6.512813872467786 EM: 12.999615680245965\n",
            "--- Step: 1400 Ext Loss: 1.175466439623564 Final Loss: 2.1242954784593437 Start Acc: 7.590560314061385 End Acc: 5.072269807280514 Acc: 2.0855638829407566 F1: 6.5289725983304505 EM: 13.001873661670235\n",
            "--- Step: 1500 Ext Loss: 1.1754544841536356 Final Loss: 2.124610697881608 Start Acc: 7.597018654230513 End Acc: 5.050799467021985 Acc: 2.0902731512325117 F1: 6.518994306833581 EM: 13.010076615589607\n",
            "--- Step: 1600 Ext Loss: 1.1737123205838986 Final Loss: 2.123013106530194 Start Acc: 7.631948782011243 End Acc: 5.063241723922548 Acc: 2.0982979387882574 F1: 6.521041793472106 EM: 13.007495315427857\n",
            "--- Step: 1700 Ext Loss: 1.1749507532899062 Final Loss: 2.124360821234486 Start Acc: 7.626028806584362 End Acc: 5.030129335684891 Acc: 2.092519106407995 F1: 6.514378788704779 EM: 13.062169312169312\n",
            "--- Step: 1800 Ext Loss: 1.1761239651902922 Final Loss: 2.125966789985881 Start Acc: 7.570446973903387 End Acc: 4.998958911715714 Acc: 2.0700305385896725 F1: 6.457357514831424 EM: 13.041365907828984\n",
            "--- Step: 1900 Ext Loss: 1.1748263752504375 Final Loss: 2.1247965182711237 Start Acc: 7.574960547080484 End Acc: 5.010520778537612 Acc: 2.059771173066807 F1: 6.447919328047648 EM: 13.016175697001579\n",
            "--- Step: 2000 Ext Loss: 1.173571923147256 Final Loss: 2.1235022877288543 Start Acc: 7.602448775612194 End Acc: 5.038105947026486 Acc: 2.0833333333333335 F1: 6.4640751518385375 EM: 13.01224387806097\n",
            "--- Step: 2100 Ext Loss: 1.1736534907669183 Final Loss: 2.1233478001558685 Start Acc: 7.602034745359353 End Acc: 5.040754402665398 Acc: 2.0778795811518322 F1: 6.461384921103171 EM: 12.975963826749167\n",
            "--- Step: 2200 Ext Loss: 1.1736489297692638 Final Loss: 2.123393846229768 Start Acc: 7.632894139027715 End Acc: 5.047421626533394 Acc: 2.082860063607451 F1: 6.496143197423608 EM: 12.977055883689232\n",
            "--- Step: 2300 Ext Loss: 1.17272225513193 Final Loss: 2.1221963043888463 Start Acc: 7.639341590612777 End Acc: 5.052151238591916 Acc: 2.094198174706649 F1: 6.512532215433645 EM: 12.960397653194264\n",
            "--- Step: 2400 Ext Loss: 1.1733882357209289 Final Loss: 2.1229857517649164 Start Acc: 7.61271345272803 End Acc: 5.0590899625156185 Acc: 2.0837671803415243 F1: 6.512332613304587 EM: 12.95423781757601\n",
            "--- Step: 2500 Ext Loss: 1.1755597944595202 Final Loss: 2.1250614316308085 Start Acc: 7.616953218712515 End Acc: 5.05922630947621 Acc: 2.0891643342662936 F1: 6.528976974220369 EM: 12.986055577768893\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 7 External Loss: 1.1757961295330903 Final Loss: 2.1253508898113345 Start Acc: 7.606834493059856 End Acc: 5.055436142178829 Acc: 2.0996514748564 F1: 6.536189738919503 EM: 13.018184467048103\n",
            "--- Step: 2600 Ext Loss: 1.2803570341180872 Final Loss: 2.2084766405600087 Start Acc: 8.449074074074074 End Acc: 6.018518518518518 Acc: 2.662037037037037 F1: 8.01515295252338 EM: 13.541666666666666\n",
            "--- Step: 2700 Ext Loss: 1.2095591777891626 Final Loss: 2.151353109539963 Start Acc: 7.5049212598425195 End Acc: 5.413385826771654 Acc: 2.1899606299212597 F1: 6.853729649801938 EM: 14.000984251968504\n",
            "--- Step: 2800 Ext Loss: 1.193124485961141 Final Loss: 2.1327638531571442 Start Acc: 7.778083700440528 End Acc: 5.245044052863436 Acc: 2.188876651982379 F1: 6.878563463555176 EM: 13.711453744493392\n",
            "--- Step: 2900 Ext Loss: 1.19415951978176 Final Loss: 2.129198567947481 Start Acc: 7.94151376146789 End Acc: 5.323012232415902 Acc: 2.217125382262997 F1: 6.973496168445578 EM: 13.818807339449542\n",
            "--- Step: 3000 Ext Loss: 1.1856814773635507 Final Loss: 2.120697558065767 Start Acc: 7.999121779859485 End Acc: 5.48887587822014 Acc: 2.276053864168618 F1: 7.122548932297618 EM: 13.861241217798595\n",
            "--- Step: 3100 Ext Loss: 1.1858456854123545 Final Loss: 2.1237626881029392 Start Acc: 7.60792220113852 End Acc: 5.402039848197344 Acc: 2.1821631878557874 F1: 6.934749928933303 EM: 13.846062618595825\n",
            "--- Step: 3200 Ext Loss: 1.1874087315046806 Final Loss: 2.126726516125875 Start Acc: 7.655502392344498 End Acc: 5.377791068580542 Acc: 2.143141945773525 F1: 6.976926387805044 EM: 13.800837320574162\n",
            "--- Step: 3300 Ext Loss: 1.185562124114581 Final Loss: 2.1234357348826434 Start Acc: 7.745873452544704 End Acc: 5.416093535075653 Acc: 2.18363136176066 F1: 6.985746348738268 EM: 13.802441540577716\n",
            "--- Step: 3400 Ext Loss: 1.1878812784439419 Final Loss: 2.128143523883935 Start Acc: 7.723700120918984 End Acc: 5.350665054413543 Acc: 2.1500906892382106 F1: 6.889328345409497 EM: 13.81121523579202\n",
            "--- Step: 3500 Ext Loss: 1.1856343935222688 Final Loss: 2.1235538742946063 Start Acc: 7.817556634304207 End Acc: 5.410598705501618 Acc: 2.1878371089536137 F1: 6.948493569436883 EM: 13.774271844660195\n",
            "--- Step: 3600 Ext Loss: 1.1835720962767708 Final Loss: 2.122135117316316 Start Acc: 7.859664070107108 End Acc: 5.385832521908471 Acc: 2.190847127555988 F1: 6.94152803413826 EM: 13.729308666017527\n",
            "--- Step: 3700 Ext Loss: 1.1846047384927203 Final Loss: 2.122293823465586 Start Acc: 7.841614906832298 End Acc: 5.426464063886424 Acc: 2.179458740017746 F1: 6.954041972836021 EM: 13.68955190771961\n",
            "--- Step: 3800 Ext Loss: 1.185822202504507 Final Loss: 2.1244295198157634 Start Acc: 7.846882640586797 End Acc: 5.399348003259984 Acc: 2.18520782396088 F1: 6.947099730635263 EM: 13.6690097799511\n",
            "--- Step: 3900 Ext Loss: 1.1883251271754525 Final Loss: 2.1265451123863572 Start Acc: 7.825452147701583 End Acc: 5.390448379804069 Acc: 2.1641861341371516 F1: 6.882352943609507 EM: 13.613884702336096\n",
            "--- Step: 4000 Ext Loss: 1.1872593061691743 Final Loss: 2.1252749343810584 Start Acc: 7.85958304134548 End Acc: 5.336807988787666 Acc: 2.1614400840925017 F1: 6.868216908867415 EM: 13.527067274001402\n",
            "--- Step: 4100 Ext Loss: 1.18508514596031 Final Loss: 2.123680755447233 Start Acc: 7.83194171578258 End Acc: 5.310658153241651 Acc: 2.159053700065488 F1: 6.835797275047404 EM: 13.5396201702685\n",
            "--- Step: 4200 Ext Loss: 1.1897544769985806 Final Loss: 2.1285823402627524 Start Acc: 7.828826060233559 End Acc: 5.316533497234174 Acc: 2.153119237861094 F1: 6.821418456095868 EM: 13.485325752919485\n",
            "--- Step: 4300 Ext Loss: 1.188465191438561 Final Loss: 2.1264630387681165 Start Acc: 7.8604516502605675 End Acc: 5.343442385639838 Acc: 2.17501447596989 F1: 6.834470074006833 EM: 13.422843080486393\n",
            "--- Step: 4400 Ext Loss: 1.1871746569039983 Final Loss: 2.1255061925925647 Start Acc: 7.885194307608101 End Acc: 5.348590585659551 Acc: 2.1722769567597155 F1: 6.835790828437677 EM: 13.37404214559387\n",
            "--- Step: 4500 Ext Loss: 1.1875018410937552 Final Loss: 2.1254221793587 Start Acc: 7.889530358069538 End Acc: 5.336987545407369 Acc: 2.1811754021795537 F1: 6.85023673402234 EM: 13.36436170212766\n",
            "--- Step: 4600 Ext Loss: 1.1880318706824853 Final Loss: 2.1264296067896065 Start Acc: 7.894980266403552 End Acc: 5.340404538727183 Acc: 2.184570794277257 F1: 6.857341221969236 EM: 13.386470152935372\n",
            "--- Step: 4700 Ext Loss: 1.1867398044902728 Final Loss: 2.125489282843424 Start Acc: 7.919017395392571 End Acc: 5.3229313587212035 Acc: 2.180300893276916 F1: 6.87131960360961 EM: 13.380054066760696\n",
            "--- Step: 4800 Ext Loss: 1.1869257394647619 Final Loss: 2.125132398376242 Start Acc: 7.981589582397844 End Acc: 5.3224629546475075 Acc: 2.177817691962281 F1: 6.862710044145513 EM: 13.419117647058824\n",
            "--- Step: 4900 Ext Loss: 1.1869281750151566 Final Loss: 2.1258467570056103 Start Acc: 7.928663515255694 End Acc: 5.322034808766652 Acc: 2.1728620541469703 F1: 6.839863900567873 EM: 13.418564675547916\n",
            "--- Step: 5000 Ext Loss: 1.1877334928335748 Final Loss: 2.126007791207954 Start Acc: 7.972805933250927 End Acc: 5.320354346930367 Acc: 2.1850535640708695 F1: 6.873418028305016 EM: 13.474711578079933\n",
            "--- Step: 5100 Ext Loss: 1.188166507723589 Final Loss: 2.1264997291904595 Start Acc: 7.942965967550455 End Acc: 5.289127423822714 Acc: 2.176493866244559 F1: 6.861429357939264 EM: 13.458399287692917\n",
            "Saving model...\n",
            "Saving outputs...\n",
            "Epoch: 8 External Loss: 1.1889896823994948 Final Loss: 2.1274168187551195 Start Acc: 7.949287770046268 End Acc: 5.284952700159083 Acc: 2.174942620860505 F1: 6.849904001811658 EM: 13.428758741258742\n",
            "--- Step: 5200 Ext Loss: 1.1108050346374512 Final Loss: 2.0383923233680004 Start Acc: 7.724056603773585 End Acc: 6.07311320754717 Acc: 2.240566037735849 F1: 7.8380095223769315 EM: 13.797169811320755\n",
            "--- Step: 5300 Ext Loss: 1.157760837498833 Final Loss: 2.08616812322654 Start Acc: 7.863562091503268 End Acc: 5.821078431372549 Acc: 2.144607843137255 F1: 7.150226288741618 EM: 13.562091503267974\n",
            "--- Step: 5400 Ext Loss: 1.1715366760261445 Final Loss: 2.0997349085072754 Start Acc: 7.942193675889328 End Acc: 5.916501976284585 Acc: 2.359189723320158 F1: 7.19160516781328 EM: 13.599308300395258\n",
            "--- Step: 5500 Ext Loss: 1.1642538292212121 Final Loss: 2.0906624017288595 Start Acc: 8.117917847025495 End Acc: 5.7719546742209635 Acc: 2.26628895184136 F1: 7.063218144705947 EM: 13.473796033994335\n",
            "--- Step: 5600 Ext Loss: 1.1664984828852134 Final Loss: 2.09495033886259 Start Acc: 8.174668874172186 End Acc: 5.691225165562914 Acc: 2.2626931567328916 F1: 7.055594366398473 EM: 13.396799116997792\n",
            "--- Step: 5700 Ext Loss: 1.1635435461782417 Final Loss: 2.0890669038024128 Start Acc: 8.374773960216999 End Acc: 5.713155515370706 Acc: 2.2716998191681737 F1: 7.104661865987637 EM: 13.257233273056057\n",
            "--- Step: 5800 Ext Loss: 1.1723732088462864 Final Loss: 2.098748438938836 Start Acc: 8.331738131699847 End Acc: 5.5991577335375196 Acc: 2.2396630934150075 F1: 7.086100396084316 EM: 13.351837672281777\n",
            "--- Step: 5900 Ext Loss: 1.1744285071513567 Final Loss: 2.1015188947300194 Start Acc: 8.387284196547144 End Acc: 5.635790172642762 Acc: 2.3032868525896415 F1: 7.103070668268528 EM: 13.371513944223107\n",
            "--- Step: 6000 Ext Loss: 1.175392312573101 Final Loss: 2.102918366334922 Start Acc: 8.448124267291911 End Acc: 5.6308616647127785 Acc: 2.341002344665885 F1: 7.20876728406298 EM: 13.463511137162953\n",
            "--- Step: 6100 Ext Loss: 1.1792899626624798 Final Loss: 2.1059574889486257 Start Acc: 8.486358866736621 End Acc: 5.6204092339979015 Acc: 2.36096537250787 F1: 7.242774061969793 EM: 13.56571353620147\n",
            "--- Step: 6200 Ext Loss: 1.178786287280569 Final Loss: 2.104742965127668 Start Acc: 8.449074074074074 End Acc: 5.686134852801519 Acc: 2.371201329534663 F1: 7.267510199517994 EM: 13.526828110161443\n",
            "--- Step: 6300 Ext Loss: 1.1808120976917038 Final Loss: 2.1058994579191115 Start Acc: 8.486014744145708 End Acc: 5.680832610581093 Acc: 2.3877927146574156 F1: 7.259454481519191 EM: 13.513660017346053\n",
            "--- Step: 6400 Ext Loss: 1.1827217841852407 Final Loss: 2.1069390019320147 Start Acc: 8.442238627294493 End Acc: 5.651436552274541 Acc: 2.3668196328810853 F1: 7.266559565766196 EM: 13.532521947326417\n",
            "--- Step: 6500 Ext Loss: 1.1765053703973491 Final Loss: 2.102430601694455 Start Acc: 8.32409460458241 End Acc: 5.559405025868441 Acc: 2.307372505543237 F1: 7.182012253754714 EM: 13.465447154471544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C671iB8flxog",
        "colab_type": "code",
        "outputId": "9b175f14-a628-4923-bd98-fcc96e9cb3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for i in range(5, 10):\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klYAo6lwblCP",
        "colab_type": "text"
      },
      "source": [
        "# Run as Python script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzFuXwkI9hZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf retroqa\n",
        "!git clone https://github.com/karvesaket/retroqa.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDLOzq3bkIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmuub_fmcINM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 /content/retroqa/run.py"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}